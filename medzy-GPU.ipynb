{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Preprocessing libraries\n",
    "from PIL import Image, ImageEnhance\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# TF libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "\n",
    "# Fine tuning libraries\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU set Memory Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = \"./Dataset/Training\"\n",
    "training_labels = pd.read_csv(\n",
    "    os.path.join(training_path,\"training_labels.csv\"),\n",
    "    delimiter = \",\"\n",
    ")\n",
    "\n",
    "validation_path = \"./Dataset/Validation\"\n",
    "validation_labels = pd.read_csv(\n",
    "    os.path.join(validation_path,\"validation_labels.csv\"),\n",
    "    delimiter = \",\"\n",
    ")\n",
    "\n",
    "testing_path = \"./Dataset/Testing\"\n",
    "testing_labels = pd.read_csv(\n",
    "    os.path.join(testing_path,\"testing_labels.csv\"),\n",
    "    delimiter = \",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "\n",
    "training_labels_enc = to_categorical(\n",
    "    encoder.fit_transform(training_labels[\"MEDICINE_NAME\"]),\n",
    "    num_classes=78\n",
    ")\n",
    "\n",
    "validation_labels_enc = to_categorical(\n",
    "    encoder.fit_transform(validation_labels[\"MEDICINE_NAME\"]),\n",
    "    num_classes=78\n",
    ")\n",
    "\n",
    "testing_labels_enc = to_categorical(\n",
    "    encoder.fit_transform(testing_labels[\"MEDICINE_NAME\"]),\n",
    "    num_classes=78\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (224, 224)\n",
    "padding_horizontal = 200\n",
    "padding_vertical = 75\n",
    "padding_color = (255, 0, 0)\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path, labels_enc):\n",
    "    images = []\n",
    "    files = glob.glob(path + \"/*png\")\n",
    "    for file in files:\n",
    "        image = Image.open(file).convert(\"RGB\")\n",
    "\n",
    "        # top = 75 // 2 - image.height // 2\n",
    "        # bottom = 75 // 2 - image.height // 2\n",
    "        # left = 200 // 2 - image.width // 2\n",
    "        # right = 200 // 2 - image.width // 2\n",
    "\n",
    "        # new_width = image.width + left + right\n",
    "        # new_height = image.height + top + bottom\n",
    "\n",
    "        # padded_image = Image.new(image.mode, (new_width, new_height), padding_color)\n",
    "        # padded_image.paste(image, (left, top))\n",
    "\n",
    "        # padded_image = padded_image.resize(image_size)\n",
    "\n",
    "        images.append(np.asarray(image.resize((100, 300))).astype(\"float32\")/255.0)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    np_labels_enc = np.array(labels_enc)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, np_labels_enc))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = get_images(training_path + \"/training_words\", training_labels_enc)\n",
    "validation_dataset = get_images(validation_path + \"/validation_words\", validation_labels_enc)\n",
    "testing_dataset = get_images(testing_path + \"/testing_words\", testing_labels_enc)\n",
    "\n",
    "# Shuffling Training Dataset\n",
    "BUFFER_SIZE = 3120\n",
    "training_dataset = training_dataset.shuffle(BUFFER_SIZE, seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(training_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hp):\n",
    "    input_shape = (300, 100, 3)\n",
    "    model = Sequential()\n",
    "    metrics = [\n",
    "        'accuracy', \n",
    "        Precision(name = 'precision'),\n",
    "        Recall(name = 'recall'),\n",
    "        # f1_score\n",
    "    ]\n",
    "    \n",
    "    #input layer. The shape of the input layer must be huge to scale down the image. Ideally (5x5) or (7x7)\n",
    "    model.add(layers.Conv2D(filters = hp.Choice(\"input_filter\", [32, 64, 128]),\n",
    "                            kernel_size = hp.Choice(\"input_kernel_size\", [3, 5, 7]),\n",
    "                            activation = hp.Choice(\"input_activation\", [\"relu\", \"leaky_relu\"]),\n",
    "                            input_shape = input_shape))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    \n",
    "\n",
    "    #Convolutional layer\n",
    "    for i in range(hp.Choice(\"n_conv_layers\", [1, 4])):\n",
    "        model.add(layers.Conv2D(filters = hp.Choice(f\"conv_filter_{i}\", [32, 64, 128]),\n",
    "                            kernel_size = hp.Choice(f\"conv_kernel_size_{i}\", [3, 5, 7]),\n",
    "                            activation = hp.Choice(f\"conv_activation_{i}\", [\"relu\", \"leaky_relu\"])))\n",
    "        if i < 1:\n",
    "            model.add(layers.MaxPooling2D(pool_size=2))\n",
    "        \n",
    "        # Add Dropout layer after each convolutional layer\n",
    "        # model.add(layers.Dropout(rate=hp.Float(f\"dropout_conv_{i}\", 0.2, 0.5, step=0.1, default=0.25)))\n",
    "        \n",
    "    #Flatten the resulted image so that the dense layer could extract the patterns and categorize it\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    #Building the dense layers\n",
    "    for i in range(hp.Int(\"n_dense_layer\", 1, 5, step = 1, default = 1)):\n",
    "        model.add(layers.Dense(hp.Choice(f\"n_neurons_dense_{i}\", [256, 512, 1024]), \n",
    "                            activation = hp.Choice(f\"dense_activation{i}\", [\"relu\", \"leaky_relu\"])))\n",
    "        \n",
    "    # Add Dropout layer after dense layer\n",
    "    # model.add(layers.Dropout(rate=hp.Float(f\"dropout_dense_{i}\", 0.2, 0.5, step=0.1, default=0.25)))\n",
    "\n",
    "    #Output layer\n",
    "    #there are 78 classes in the output layer so we will take 78 neurons to classify it\n",
    "    model.add(layers.Dense(78, activation = \"softmax\"))\n",
    "\n",
    "    # GPU Optimized optimizers\n",
    "    optimizer_name = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
    "\n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate', [0.0001, 0.001, 0.01]))\n",
    "    # if optimizer_name == 'sgd':\n",
    "    #     optimizer = tf.keras.optimizers.SGD(learning_rate=hp.Choice('learning_rate', [0.00001, 0.0001, 0.001]), momentum=0.9)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp.Choice('learning_rate', [0.0001, 0.001]), rho=0.9)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tuner = keras_tuner.Hyperband(\n",
    "    hypermodel = create_model,\n",
    "    objective = keras_tuner.Objective(\"accuracy\", \"max\"),\n",
    "    max_epochs = 500,\n",
    "    factor = 5,\n",
    "    hyperband_iterations = 1,\n",
    "    seed = 42,\n",
    "    # distribution_strategy = tf.distribute.MirroredStrategy(), # this is to speed up the process by distributing the load for computation (only for gpu)\n",
    "    project_name = \"medzy-train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - ETA: 0s - loss: 7.0086 - accuracy: 9.6154e-04 - precision: 0.0000e+00 - recall: 0.0000e+00WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 780 batches). You may need to use the repeat() function when building your dataset.\n"
     ]
    }
   ],
   "source": [
    "validation_steps = 780 #the number of validation for number of batches\n",
    "accumulation_steps = 4\n",
    "\n",
    "tuner.search(\n",
    "    training_dataset,\n",
    "    epochs = 500,\n",
    "    validation_data = validation_dataset, #data used to evaluate the model after each epoch\n",
    "    validation_steps = validation_steps, #number of batches used for validation\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(\"accuracy\", mode=\"max\"),\n",
    "        keras.callbacks.EarlyStopping(\"val_recall\", mode=\"max\")\n",
    "    ]\n",
    "    # callbacks=[CustomTunerCallback(img_height, img_width, train_images_full, train_labels_full, val_images, val_labels, accumulation_steps, batch_size, num_classes)],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
