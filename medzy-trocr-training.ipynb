{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medzy\n",
    "## Overview\n",
    "This project aims to develop a machine learning model capable of interpreting doctors’ handwriting on prescriptions. By accurately detecting and translating challenging handwriting, the model will empower patients to read their prescriptions independently, making it easier for them to purchase their medications without confusion if they run out of medicine.\n",
    "\n",
    "This model is using Hugging Face's [TrOCR](https://huggingface.co/docs/transformers/en/model_doc/trocr) for classifying the handwritings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is referenced from [TrOCR tutorial notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb) with some changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DirectML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "\n",
    "device = torch_directml.device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA (fallback to CPU if none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"./Dataset/Training/training_labels.csv\", delimiter = \",\")\n",
    "val_df = pd.read_csv(\"./Dataset/Validation/validation_labels.csv\", delimiter = \",\")\n",
    "test_df = pd.read_csv(\"./Dataset/Testing/testing_labels.csv\", delimiter = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IMAGE</th>\n",
       "      <th>MEDICINE_NAME</th>\n",
       "      <th>GENERIC_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>Aceta</td>\n",
       "      <td>Paracetamol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>Aceta</td>\n",
       "      <td>Paracetamol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>Aceta</td>\n",
       "      <td>Paracetamol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>Aceta</td>\n",
       "      <td>Paracetamol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>Aceta</td>\n",
       "      <td>Paracetamol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>3115.png</td>\n",
       "      <td>Zithrin</td>\n",
       "      <td>Azithromycin Dihydrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>3116.png</td>\n",
       "      <td>Zithrin</td>\n",
       "      <td>Azithromycin Dihydrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>3117.png</td>\n",
       "      <td>Zithrin</td>\n",
       "      <td>Azithromycin Dihydrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3118</th>\n",
       "      <td>3118.png</td>\n",
       "      <td>Zithrin</td>\n",
       "      <td>Azithromycin Dihydrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119</th>\n",
       "      <td>3119.png</td>\n",
       "      <td>Zithrin</td>\n",
       "      <td>Azithromycin Dihydrate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3120 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         IMAGE MEDICINE_NAME            GENERIC_NAME\n",
       "0        0.png         Aceta             Paracetamol\n",
       "1        1.png         Aceta             Paracetamol\n",
       "2        2.png         Aceta             Paracetamol\n",
       "3        3.png         Aceta             Paracetamol\n",
       "4        4.png         Aceta             Paracetamol\n",
       "...        ...           ...                     ...\n",
       "3115  3115.png       Zithrin  Azithromycin Dihydrate\n",
       "3116  3116.png       Zithrin  Azithromycin Dihydrate\n",
       "3117  3117.png       Zithrin  Azithromycin Dihydrate\n",
       "3118  3118.png       Zithrin  Azithromycin Dihydrate\n",
       "3119  3119.png       Zithrin  Azithromycin Dihydrate\n",
       "\n",
       "[3120 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element of the dataset should return 2 things:\n",
    "\n",
    "- `pixel_values`, which serve as input to the model.\n",
    "- `labels`, which are the input_ids of the corresponding text in the image.\n",
    "\n",
    "We use `TrOCRProcessor` to prepare the data for the model. `TrOCRProcessor` is actually just a wrapper around a `ViTFeatureExtractor` (which can be used to resize + normalize images) and a `RobertaTokenizer` (which can be used to encode and decode text into/from input_ids)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = self.df['IMAGE'][idx]\n",
    "        text = self.df['MEDICINE_NAME'][idx]\n",
    "\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # add labels (input_ids) by encoding the text\n",
    "        labels = self.processor.tokenizer(\n",
    "            text, \n",
    "            padding=\"max_length\", \n",
    "            max_length=self.max_target_length\n",
    "        ).input_ids\n",
    "\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
    "\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize the training and evaluation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "\n",
    "train_dataset = IAMDataset(\n",
    "    root_dir='./Dataset/Training/training_words/',\n",
    "    df=train_df,\n",
    "    processor=processor\n",
    ")\n",
    "eval_dataset = IAMDataset(\n",
    "    root_dir='./Dataset/Validation/validation_words/',\n",
    "    df=test_df,\n",
    "    processor=processor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 3120\n",
      "Number of validation examples: 780\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify an example from the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 384, 384])\n",
      "labels torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the original image and decode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABcAO4DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKTPX2oAWim714560oOaAFooooAKKKKACiiigAooooAKKKTdwfagBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAppOD+lOrwP4gfFbVNB+KC2UBmbTLKIJPaJIFE7SR5DbtuRjep/4D2zQB6P4i+J/hnwvqiafqNzcfaXAYLHAxAB6ZNdhbzRzxCSJtyHo2MV5B4Z+GrTfDDUk1OKGXX9RR8XU4EjIykqg3ZPoOnrWt8CJriT4emO4d2MN5Kih23EKQrdfqW/OgD06iiigAooooAKKKKACiiigArJsdf03UdWu9Ntbnfd2ozNH5bDb8xXqRg8g1qmvF/hVbfYvif4zt/MZgsjYLksx/fOOpJP60Ae0L0yMYNLSDp0xS0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFBoAaW5Hv2r5mNhD45+PazwgPYCaCSXzB1VIQ2MHB5KEV7p451+38P8AgvVL+SV0ZbZxEyKSd5+Vfp8zCvGfgvq+kaPpd3dy6ddXuqvcEZg2gxoqccu6g53P0oA+iNpxtwPf3ryT4OWt14d1/wAT+Hb8ATxfZ58o+5fmQ/r0/KurPxAQJn/hHdXPoA1t/wDHq5Dw3q91b/GPxDKNHv3a9srd1gDw7lChU3MTJjr6En2oA9jB9OaaGJHTrWMNQ1u55g0MQKOovrxF3fTyxJ+uK841DxN4p+IM97oHhjytLlspSL2+F0SuzLJtUGMNnq2Rjp1BoA6zxP8AFXwx4UlMF7cTyXQ3AQQwMTxkdTgYyMda5GX9ofR4j82h6kMrvHzR8jsetec+KfhN4k8HTwarFdx3cKy+YbmFsSRFTu3bXIzwM8HrXYHwt4p+KGm6aL3ytL061gQfaZWSeS7bA+bC4KgqScE4BP40Ae1aNrFtrmlW2oWbM0NxGsi7lwRuUMAfwIq4ZQqO7HCrnP4d6zkXT/DOixxhFt7S2jC/u1OAqr1wPYV5t8Z9WllufD3hm0vJ7Z9TvAk0kbEL5ZIjwwH3h8/TvigDf0b4xeFdf1ddNsZLsyuQFd4CFbLBR79+4rvwc18/fELSfh/o9pDpWkaeIteLYgMCyKQ2GVSWLAffA5ya99hysKB/vKgDH3oAkNeOfDOQS/FjxuwbKmRiuB28+SvTtS8Q6Zpd5DZXdwY7iaNpEXy2OVBAJyBjqRXzl8IfGeieEvEWt3Os3LwJdoBGVjaQsd5POAfWgD6hHHA7UFioyeg6156fjb4DXltXlAI4/wBEl5/8drk/GHxutbq0+xeD/tEt2ytuumURiPggYDgknOD0FAHtvmDJ59vxpcnHNeNaF4m8c3VlFolqi3GrwRLdXN1cXCAKhGwJjY2W3Kx4446811vgb4i23i66vtOkspLHUrEkTQswccHa2GAA4bigDuqKaGAH606gAooooAKKKKACiiigAooooAKRs4460tVtRhnuNMuoLacwXEkTJFKByjEEBvwNAHifjq8n8bfFfSfCllP/AMS+AoblJM7JDxK3GMthVXg969j07SNO0e3NtplhbWkRJbZbxhBn14HWuN+Gnw+l8HWVzPq7213qs8u43EaliigYChjz0J6Adcc16JQAzA4wMc9q8r8TOuhfGnR9SuJH8nUrJLKNV5IcTqefYhv0r1Yj8q4/xd4Mn8UavpF4mqtaR6bJ5wiWItubcpzkMP7vvQB1uPTk9DXjnw/1qw8DXmtaH4gSOwvJLs3EToDKJUJKdVU4wVPB9a9kQFQQeT144zWbrHh3RteRF1bTLW8CZ2GeMNtz6UAXllifI3ZAHTFU9U1zTtHthPe3HlRE7QRGzc4JxwPY1lQ/D/wtaBxb6LaQiVDHJsUjzFPUHnpV2x8J+H9NkD2Wi2EEgwQyQKGGPfGew/KgDEvrk+PfDlzZ6XBJDa3EbRtc3DBV2shHCgktwwODtHvXlvxI8CXHh6XQdVvdVudS0m2nWGaNmYPDCMNhCXJPyq3QjoK+hQvv3zTJoI7mF4Z4lkjkUqysMhlPB/nQBzvhrw74Wgsbe80bSbSJXVJI5DD+8xgEZY5YnpznrXSY6DkenOa8xuPhhrGjXEs3gnxD/ZMch3PaMjGNiCSOSWx2HToKsjwr8QdRneW78Zx2KcKsNrAJAVyf4sIVPOOBQBp/EDVPCWk2lvL4lsbe6kbctrHLaCYs2PuqSMLnA6kDpXlPwHu9HtG1ZtQSFZLm5ght90Jcknf8vTjtXtel+DtI0uRbhLOOa9wN93ODJKx68M5JHPPB61x3hP4P2nh/xPd6jcTRXNsZRNZwAP8AuWDlg3LdQNoHXv8AiAdp4h0lZ9Bv4rKwtJLl7aRIlcKo3lTtOSDjn+deM+EtcjsdMh8P+FvD5g12eJUub+V4wI5NoRpMncThyGwBzX0GuNvAxS0Acl4K8Hp4U0uWOWVJ9RuXElzdEFmdtozyxPG7cfxqLxf4FtNfT7fZBbPW4EJt7yNmRgeSM7SMjfg856e9dlTe2fyoA4zwJ40PiNb3TLyJoNW0x/JuUyGVyvylgwAGCwbj2rtBjNYl34YsZ9cttXjj8i8hUxtJESpdNwba2CMjIzznqa2lAHbGaAHUUUUAFFFFABRRRQAUUUUAFFFFACbR6cjvS0UUAGKTApaKADFGBRRQAm0elG0elLRQAmBRgUtFACbQOgFLgUUUAJgelG1TjI6dKWigAAxRRRQAUYFFFACFQeooAA6ClooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA/9k=",
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=238x92>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_dataset.root_dir + train_df['IMAGE'][0]).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aceta\n"
     ]
    }
   ],
   "source": [
    "labels = encoding['labels']\n",
    "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
    "label_str = processor.decode(labels, skip_special_tokens=True)\n",
    "print(label_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create corresponding dataloaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"encoder_stride\": 16,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 768,\n",
      "  \"image_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"model_type\": \"vit\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_channels\": 3,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"patch_size\": 16,\n",
      "  \"qkv_bias\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\"\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"cross_attention_hidden_size\": 768,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"layernorm_embedding\": false,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"trocr\",\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"use_learned_position_embeddings\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TrOCRForCausalLM(\n",
       "    (model): TrOCRDecoderWrapper(\n",
       "      (decoder): TrOCRDecoder(\n",
       "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
       "        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x TrOCRDecoderLayer(\n",
       "            (self_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): TrOCRAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, we need to set a couple of attributes, namely:\n",
    "\n",
    "the attributes required for creating the `decoder_input_ids` from the `labels` (the model will automatically create the `decoder_input_ids` by shifting the labels one position to the right and prepending the `decoder_start_token_id`, as well as replacing ids which are -100 by the pad_token_id)\n",
    "the vocabulary size of the model (for the language modeling head on top of the decoder)\n",
    "beam-search related parameters which are used when generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on the Character Error Rate (CER), which is available in HuggingFace Datasets (see [here](https://huggingface.co/metrics/cer))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "cer_metric = evaluate.load(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cer(pred_ids, label_ids):\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CondaEnvs\\pyt_cuda\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ceb87232a84ab5a4f1fc3b96a266d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 0.8641235761153392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a952a812c774fa1b2d8aec854b65267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CondaEnvs\\pyt_cuda\\lib\\site-packages\\transformers\\generation\\utils.py:1532: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.07196705308230161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b082233a0d40e8b1cd2966bcd15230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 1: 0.4874115059008965\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99769aa32364f9e96a9818f94178da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.07936962257787127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c82ba1c48a64e5da9800afc22324808",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 2: 0.44741287162670723\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713e63c611b441a180ca732bd0ebf399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.09149286527082255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c65dccad3e684784be1d3b4a83a320cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 3: 0.40935823909747293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f239777874400ea1c72ec5e8acb0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.06401191069271106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0810c92c4026431c9870e254db914001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 4: 0.4304484924444785\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42166c8d2f144f19a1a468b452b92a79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.04398249384706843\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914ffc31e6b84adcb41af18245f92e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 5: 0.3988894797288455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09174d21271f45b88b92715b97798bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.017918574576737845\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff974c53c1c46bca28627f251cc4131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 6: 0.3306926605411065\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6f4611e2ef410eb6d817dfa48b978d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.01383156579585151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db62391269bf45d399e67698094d705e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 7: 0.3170634035498668\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bf33f4c11046adbea7c5d37e265f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.017757584064982023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e075e9265ede40abaea3a02994b02881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 8: 0.2542660194807328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12a60f182204b668500a445ac16130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 0.026046499302568967\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70f1db78c1740e598634e435d93c7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 9: 0.6999252750036808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa2de91f80e42c582c4732019d3459d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation CER: 1.0438726731254901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CondaEnvs\\pyt_cuda\\lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 64, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    # train\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # get the inputs\n",
    "        for k,v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
    "    \n",
    "    # evaluate\n",
    "    model.eval()\n",
    "    valid_cer = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader):\n",
    "            # run batch generation\n",
    "            outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
    "\n",
    "            # compute metrics\n",
    "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
    "            valid_cer += cer \n",
    "\n",
    "    print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
    "\n",
    "model.save_pretrained(\"./model-output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
