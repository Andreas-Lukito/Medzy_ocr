{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHOPyyQO_MpB"
      },
      "source": [
        "# Medzy\n",
        "## Overview\n",
        "This project aims to develop a machine learning model capable of interpreting doctors’ handwriting on prescriptions. By accurately detecting and translating challenging handwriting, the model will empower patients to read their prescriptions independently, making it easier for them to purchase their medications without confusion if they run out of medicine.\n",
        "\n",
        "This model is using Hugging Face's [TrOCR](https://huggingface.co/docs/transformers/en/model_doc/trocr) for classifying the handwritings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7jSH4Kr_MpF"
      },
      "source": [
        "The code below is referenced from [TrOCR tutorial notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TrOCR/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_native_PyTorch.ipynb) with some changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLZFfCLU_MpF"
      },
      "source": [
        "## Get Pytorch Device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENopP-Gc_MpH"
      },
      "source": [
        "### DirectML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eoe5sJmw_MpH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch_directml\n",
        "\n",
        "device = torch_directml.device()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_pwHMkE_MpL"
      },
      "source": [
        "### CUDA (fallback to CPU if none)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxvXoOFF_MpM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFY8t9iE_MpM"
      },
      "source": [
        "## Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb1dC9vu_MpN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"./Dataset/Training/training_labels.csv\", delimiter = \",\")\n",
        "val_df = pd.read_csv(\"./Dataset/Validation/validation_labels.csv\", delimiter = \",\")\n",
        "test_df = pd.read_csv(\"./Dataset/Testing/testing_labels.csv\", delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oahsnKt4_MpN",
        "outputId": "d6b78d1b-9ce0-48d7-92d5-08821e89221f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IMAGE</th>\n",
              "      <th>MEDICINE_NAME</th>\n",
              "      <th>GENERIC_NAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3115</th>\n",
              "      <td>3115.png</td>\n",
              "      <td>Zithrin</td>\n",
              "      <td>Azithromycin Dihydrate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3116</th>\n",
              "      <td>3116.png</td>\n",
              "      <td>Zithrin</td>\n",
              "      <td>Azithromycin Dihydrate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3117</th>\n",
              "      <td>3117.png</td>\n",
              "      <td>Zithrin</td>\n",
              "      <td>Azithromycin Dihydrate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3118</th>\n",
              "      <td>3118.png</td>\n",
              "      <td>Zithrin</td>\n",
              "      <td>Azithromycin Dihydrate</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3119</th>\n",
              "      <td>3119.png</td>\n",
              "      <td>Zithrin</td>\n",
              "      <td>Azithromycin Dihydrate</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3120 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         IMAGE MEDICINE_NAME            GENERIC_NAME\n",
              "0        0.png         Aceta             Paracetamol\n",
              "1        1.png         Aceta             Paracetamol\n",
              "2        2.png         Aceta             Paracetamol\n",
              "3        3.png         Aceta             Paracetamol\n",
              "4        4.png         Aceta             Paracetamol\n",
              "...        ...           ...                     ...\n",
              "3115  3115.png       Zithrin  Azithromycin Dihydrate\n",
              "3116  3116.png       Zithrin  Azithromycin Dihydrate\n",
              "3117  3117.png       Zithrin  Azithromycin Dihydrate\n",
              "3118  3118.png       Zithrin  Azithromycin Dihydrate\n",
              "3119  3119.png       Zithrin  Azithromycin Dihydrate\n",
              "\n",
              "[3120 rows x 3 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yfsvEoo_MpO"
      },
      "source": [
        "Each element of the dataset should return 2 things:\n",
        "\n",
        "- `pixel_values`, which serve as input to the model.\n",
        "- `labels`, which are the input_ids of the corresponding text in the image.\n",
        "\n",
        "We use `TrOCRProcessor` to prepare the data for the model. `TrOCRProcessor` is actually just a wrapper around a `ViTFeatureExtractor` (which can be used to resize + normalize images) and a `RobertaTokenizer` (which can be used to encode and decode text into/from input_ids)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGZuYVlq_MpP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class IAMDataset(Dataset):\n",
        "    def __init__(self, root_dir, df, processor, max_target_length=128):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # get file name + text\n",
        "        file_name = self.df['IMAGE'][idx]\n",
        "        text = self.df['MEDICINE_NAME'][idx]\n",
        "\n",
        "        # prepare image (i.e. resize + normalize)\n",
        "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "        # add labels (input_ids) by encoding the text\n",
        "        labels = self.processor.tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_target_length\n",
        "        ).input_ids\n",
        "\n",
        "        # important: make sure that PAD tokens are ignored by the loss function\n",
        "        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n",
        "\n",
        "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
        "        return encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ota4W9gx_MpP"
      },
      "source": [
        "Let's initialize the training and evaluation datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWKsN2H2_MpQ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrOCRProcessor\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
        "\n",
        "train_dataset = IAMDataset(\n",
        "    root_dir='./Dataset/Training/training_words/',\n",
        "    df=train_df,\n",
        "    processor=processor\n",
        ")\n",
        "eval_dataset = IAMDataset(\n",
        "    root_dir='./Dataset/Validation/validation_words/',\n",
        "    df=test_df,\n",
        "    processor=processor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PO6rd332_MpQ",
        "outputId": "0c6c787f-8b1f-4c45-ce67-e97027a7e4aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 3120\n",
            "Number of validation examples: 780\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of training examples:\", len(train_dataset))\n",
        "print(\"Number of validation examples:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty4etDA8_MpQ"
      },
      "source": [
        "Let's verify an example from the training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDIuMJ_3_MpQ",
        "outputId": "7cd6ccb9-86cf-4189-92a6-a1c579272fe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pixel_values torch.Size([3, 384, 384])\n",
            "labels torch.Size([128])\n"
          ]
        }
      ],
      "source": [
        "encoding = train_dataset[0]\n",
        "for k,v in encoding.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIzs057Q_MpQ"
      },
      "source": [
        "We can also check the original image and decode the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fblPIHHQ_MpR",
        "outputId": "f1be49b8-5b69-4511-9c10-09244f4bcc95"
      },
      "outputs": [
        {
          "data": {
            "image/jpeg": "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",
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=238x92>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image = Image.open(train_dataset.root_dir + train_df['IMAGE'][0]).convert(\"RGB\")\n",
        "image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4USMyXq_MpR",
        "outputId": "f00eb67e-37df-42f8-9660-7ef92770b97f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aceta\n"
          ]
        }
      ],
      "source": [
        "labels = encoding['labels']\n",
        "labels[labels == -100] = processor.tokenizer.pad_token_id\n",
        "label_str = processor.decode(labels, skip_special_tokens=True)\n",
        "print(label_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUzUyCOS_MpS"
      },
      "source": [
        "Let's create corresponding dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1trD1YYm_MpS"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtPP3PUs_MpS"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tz_UgRF_MpS",
        "outputId": "80170515-d6a6-4f55-f2f8-b8d26252cae4",
        "colab": {
          "referenced_widgets": [
            "ea91871c65bb4a588dbbd80d90623d96",
            "7ee86541170346048843bf6f7ca35335"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea91871c65bb4a588dbbd80d90623d96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   3%|3         | 52.4M/1.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Config of the encoder: <class 'transformers.models.vit.modeling_vit.ViTModel'> is overwritten by shared encoder config: ViTConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"image_size\": 384,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.49.0\"\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.trocr.modeling_trocr.TrOCRForCausalLM'> is overwritten by shared decoder config: TrOCRConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"relu\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"cross_attention_hidden_size\": 768,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 12,\n",
            "  \"decoder_start_token_id\": 2,\n",
            "  \"dropout\": 0.1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"layernorm_embedding\": false,\n",
            "  \"max_position_embeddings\": 1024,\n",
            "  \"model_type\": \"trocr\",\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": false,\n",
            "  \"use_learned_position_embeddings\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-stage1 and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ee86541170346048843bf6f7ca35335",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "VisionEncoderDecoderModel(\n",
              "  (encoder): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (pooler): ViTPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (decoder): TrOCRForCausalLM(\n",
              "    (model): TrOCRDecoderWrapper(\n",
              "      (decoder): TrOCRDecoder(\n",
              "        (embed_tokens): TrOCRScaledWordEmbedding(50265, 1024, padding_idx=1)\n",
              "        (embed_positions): TrOCRSinusoidalPositionalEmbedding()\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x TrOCRDecoderLayer(\n",
              "            (self_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (activation_fn): ReLU()\n",
              "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (encoder_attn): TrOCRAttention(\n",
              "              (k_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
              "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            )\n",
              "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (output_projection): Linear(in_features=1024, out_features=50265, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import VisionEncoderDecoderModel\n",
        "\n",
        "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rnsBygd_MpS"
      },
      "source": [
        "Importantly, we need to set a couple of attributes, namely:\n",
        "\n",
        "the attributes required for creating the `decoder_input_ids` from the `labels` (the model will automatically create the `decoder_input_ids` by shifting the labels one position to the right and prepending the `decoder_start_token_id`, as well as replacing ids which are -100 by the pad_token_id)\n",
        "the vocabulary size of the model (for the language modeling head on top of the decoder)\n",
        "beam-search related parameters which are used when generating text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QM5A9vuU_MpT"
      },
      "outputs": [],
      "source": [
        "# set special tokens used for creating the decoder_input_ids from the labels\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "# make sure vocab size is set correctly\n",
        "model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "# set beam search parameters\n",
        "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
        "model.config.max_length = 64\n",
        "model.config.early_stopping = True\n",
        "model.config.no_repeat_ngram_size = 3\n",
        "model.config.length_penalty = 2.0\n",
        "model.config.num_beams = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-hmmdM3_MpT"
      },
      "source": [
        "We will evaluate the model on the Character Error Rate (CER), which is available in HuggingFace Datasets (see [here](https://huggingface.co/metrics/cer))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24Dsfd5X_MpT",
        "outputId": "e8d7c254-3a1c-4679-f6a2-0a7e65bac470",
        "colab": {
          "referenced_widgets": [
            "2b3ba689f8a64401aa7eb644fc00ed02"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b3ba689f8a64401aa7eb644fc00ed02",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "cer_metric = evaluate.load(\"cer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rr_S5qJ_MpT"
      },
      "outputs": [],
      "source": [
        "def compute_cer(pred_ids, label_ids):\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbHFlnal_MpT",
        "outputId": "12e04a34-4155-409b-cffa-2190074b393d",
        "colab": {
          "referenced_widgets": [
            "bc8a350802ea457693b61c3da9836ca0"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bc8a350802ea457693b61c3da9836ca0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/98 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "Could not allocate tensor with 511378944 bytes. There is not enough GPU video memory available!",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m     batch[k] \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:595\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[1;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 595\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    596\u001b[0m         pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m    597\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    598\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    599\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    600\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_encoder,\n\u001b[0;32m    601\u001b[0m     )\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    603\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\u001b[38;5;241m*\u001b[39mencoder_outputs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:639\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    633\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m    635\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    636\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    637\u001b[0m )\n\u001b[1;32m--> 639\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    640\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    642\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    643\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    646\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    647\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:468\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    461\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    462\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    463\u001b[0m         hidden_states,\n\u001b[0;32m    464\u001b[0m         layer_head_mask,\n\u001b[0;32m    465\u001b[0m         output_attentions,\n\u001b[0;32m    466\u001b[0m     )\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 468\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:413\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    409\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    410\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    411\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    412\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 413\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_before\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# in ViT, layernorm is applied before self-attention\u001b[39;49;00m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    419\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:343\u001b[0m, in \u001b[0;36mViTAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    339\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    340\u001b[0m     head_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    341\u001b[0m     output_attentions: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor], Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m--> 343\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    345\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    347\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32me:\\CondaEnvs\\pyt_dml\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:277\u001b[0m, in \u001b[0;36mViTSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    274\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[0;32m    275\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m--> 277\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_probs_dropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    288\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Could not allocate tensor with 511378944 bytes. There is not enough GPU video memory available!"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "    # train\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for batch in tqdm(train_dataloader):\n",
        "        # get the inputs\n",
        "        for k,v in batch.items():\n",
        "            batch[k] = v.to(device)\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n",
        "\n",
        "    # evaluate\n",
        "    model.eval()\n",
        "    valid_cer = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(eval_dataloader):\n",
        "            # run batch generation\n",
        "            outputs = model.generate(batch[\"pixel_values\"].to(device))\n",
        "\n",
        "            # compute metrics\n",
        "            cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
        "            valid_cer += cer\n",
        "\n",
        "    print(\"Validation CER:\", valid_cer / len(eval_dataloader))\n",
        "\n",
        "model.save_pretrained(\".\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyt_dml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}