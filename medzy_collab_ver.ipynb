{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andreas-Lukito/Medzy_ocr/blob/main/medzy_collab_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTA1H6Nl7y3H"
      },
      "source": [
        "# Medzy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "59b4u3Ps74MQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe13909b-d1b3-4771-9efb-17558c1be1c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ropZ1fZs7y3J"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTPI0yn97y3K"
      },
      "source": [
        "This project aims to develop a machine learning model capable of interpreting doctorsâ€™ handwriting on prescriptions. By accurately detecting and translating challenging handwriting, the model will empower patients to read their prescriptions independently, making it easier for them to purchase their medications without confusion if they run out of medecine or to check if the cleric gave the correct medicine.\n",
        "\n",
        "This model will use Tensor flows' keras convolutional neural network as reference to this <a href = \"https://www.tensorflow.org/tutorials/images/cnn\">documentation</a>. The model will also be trained using this <a href=\"https://www.kaggle.com/datasets/mamun1113/doctors-handwritten-prescription-bd-dataset\">dataset</a> from kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q4r4lHa7y3K"
      },
      "source": [
        "## Importing needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install keras-tuner"
      ],
      "metadata": {
        "id": "P3t431onHVK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2w4RtNy7y3L"
      },
      "outputs": [],
      "source": [
        "# basic python libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# data preprocessing libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# tensor flow libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "# Fine tuning libraries\n",
        "import keras_tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CYZYxby7y3M"
      },
      "source": [
        "## GPU Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm4t9hQm7y3M"
      },
      "source": [
        "This section of the code is contributed by <a href = \"https://github.com/SonicRay241\">Rayhan Permana</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhxbB89y7y3M"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wtiLBKY7y3M"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4wqQVYk7y3N"
      },
      "source": [
        "### Train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FRXIZPm7y3N"
      },
      "source": [
        "#### Train Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KNhV5Kn7y3N"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/project_medzy/dataset/Training\"\n",
        "train_labels = pd.read_csv(os.path.join(train_path,\"training_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odY8-Ktw7y3N"
      },
      "outputs": [],
      "source": [
        "train_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBkf7bI17y3O"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRTc1BYW7y3O"
      },
      "outputs": [],
      "source": [
        "medicine_enc = LabelEncoder()\n",
        "train_name_enc = to_categorical(medicine_enc.fit_transform(train_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# train_labels[\"MEDICINE_NAME_ENC\"] = train_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AILslWbq7y3O"
      },
      "outputs": [],
      "source": [
        "len(train_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWtoJUEo7y3O"
      },
      "source": [
        "after encoding there are 78 unique values/medicines since we are using label encoder, we will put them all in to a seperate column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zCAJn5x7y3O"
      },
      "source": [
        "#### Train Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmeDWfaj7y3O"
      },
      "outputs": [],
      "source": [
        "#the image width and height to pass to the model\n",
        "img_width = 420\n",
        "img_height = np.round(img_width/3, 0).astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwJpdHhI7y3P"
      },
      "outputs": [],
      "source": [
        "train_images = []\n",
        "train_files = glob.glob(\"/content/drive/MyDrive/project_medzy/dataset/Training/training_words/*.png\")\n",
        "for picture in train_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    res, image = cv2.threshold(image,210,255,cv2.THRESH_BINARY)\n",
        "    image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
        "            cv2.THRESH_BINARY,15,1)\n",
        "    # image = cv2.GaussianBlur(image,(5,5),5)\n",
        "    res, image = cv2.threshold(image,220,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    res, image = cv2.threshold(image,220,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "\n",
        "\n",
        "    train_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(train_images)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0LIn0NJ7y3P"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", train_images[0].shape)\n",
        "print(\"Labels shape:\", train_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF3As-W7y3P"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxo2NUHq7y3P"
      },
      "outputs": [],
      "source": [
        "train_images[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XErBGvx27y3P"
      },
      "outputs": [],
      "source": [
        "plt.imshow(train_images[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSUKMKDr7y3Q"
      },
      "outputs": [],
      "source": [
        "print(\"Example label:\", train_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqo7wqOu7y3Q"
      },
      "source": [
        "### Validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOtgSJdH7y3Q"
      },
      "source": [
        "#### validation Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTUWcIxO7y3Q"
      },
      "outputs": [],
      "source": [
        "validation_path = \"/content/drive/MyDrive/project_medzy/dataset/Validation\"\n",
        "validation_labels = pd.read_csv(os.path.join(validation_path,\"validation_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTU956Ar7y3Q"
      },
      "outputs": [],
      "source": [
        "validation_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-PhdxsA7y3Q"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3anGXzQ_7y3Q"
      },
      "outputs": [],
      "source": [
        "validation_name_enc = to_categorical(medicine_enc.transform(validation_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# validation_labels[\"MEDECINE_NAME_ENC\"] = validation_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYyaINCt7y3Q"
      },
      "outputs": [],
      "source": [
        "type(validation_name_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9iL_Myr7y3R"
      },
      "outputs": [],
      "source": [
        "len(validation_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfpxrIq_7y3R"
      },
      "source": [
        "#### Validation Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3i5LkHE7y3R"
      },
      "outputs": [],
      "source": [
        "validation_images = []\n",
        "validation_files = glob.glob(\"/content/drive/MyDrive/project_medzy/dataset/Validation/validation_words/*.png\")\n",
        "for picture in validation_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    res, image = cv2.threshold(image,210,255,cv2.THRESH_BINARY)\n",
        "    image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
        "            cv2.THRESH_BINARY,15,1)\n",
        "    # image = cv2.GaussianBlur(image,(5,5),5)\n",
        "    res, image = cv2.threshold(image,220,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    res, image = cv2.threshold(image,220,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "\n",
        "\n",
        "    validation_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "validation_images = np.array(validation_images)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(validation_images)\n",
        "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgRu-s_I7y3R"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", validation_images.shape)\n",
        "print(\"Labels shape:\", validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8OAOAXL7y3R"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sh7lioj7y3R"
      },
      "outputs": [],
      "source": [
        "validation_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz7AdhGs7y3S"
      },
      "outputs": [],
      "source": [
        "plt.imshow(validation_images[0], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWnI2TFt7y3S"
      },
      "outputs": [],
      "source": [
        "print(\"Example label:\", validation_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8NjRQJt7y3T"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YGUOoZV7y3T"
      },
      "source": [
        "#### Test Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jkg1hH77y3T"
      },
      "outputs": [],
      "source": [
        "test_path = \"/content/drive/MyDrive/project_medzy/dataset/Testing\"\n",
        "test_labels = pd.read_csv(os.path.join(test_path,\"testing_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpeWrMOY7y3T"
      },
      "outputs": [],
      "source": [
        "test_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdmY3GCR7y3T"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkD5ncn87y3T"
      },
      "outputs": [],
      "source": [
        "test_name_enc = to_categorical(medicine_enc.transform(test_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# test_labels[\"train_medecine_name_enc\"] = test_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITBEu5ZN7y3T"
      },
      "outputs": [],
      "source": [
        "len(test_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpdxh0jc7y3T"
      },
      "source": [
        "#### Testing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUGvLEL97y3U"
      },
      "outputs": [],
      "source": [
        "test_images = []\n",
        "test_files = glob.glob(\"/content/drive/MyDrive/project_medzy/dataset/Testing/testing_words/*.png\")\n",
        "for picture in test_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    res, image = cv2.threshold(image,210,255,cv2.THRESH_BINARY)\n",
        "    image = cv2.adaptiveThreshold(image,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
        "            cv2.THRESH_BINARY,15,1)\n",
        "    # image = cv2.GaussianBlur(image,(5,5),5)\n",
        "    res, image = cv2.threshold(image,220,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "    res, image = cv2.threshold(image,220,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
        "\n",
        "\n",
        "    test_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(test_images)\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou9F3IHg7y3U"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", test_images.shape)\n",
        "print(\"Labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkJSlwRk7y3U"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAj6XltF7y3U"
      },
      "outputs": [],
      "source": [
        "test_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbMfnElS7y3U"
      },
      "outputs": [],
      "source": [
        "plt.imshow(test_images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uKMTUX97y3U"
      },
      "outputs": [],
      "source": [
        "print(\"Example label:\", test_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q4NwUD37y3U"
      },
      "source": [
        "## Building the artificial neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS-zayeR7y3V"
      },
      "source": [
        "#### Make a model create function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDQ6UsSo7y3V"
      },
      "source": [
        "##### Parameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S077J9oc7y3V"
      },
      "outputs": [],
      "source": [
        "#number of classes to determine how many neurons are in the output layer\n",
        "num_classes = len(train_labels[\"MEDICINE_NAME\"].unique())\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2k-oXelt7y3V"
      },
      "outputs": [],
      "source": [
        "#the image size to determine the shape for the convolutional neural network to scan\n",
        "train_images[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPv5ibTD7y3V"
      },
      "source": [
        "#### Custom f1 score metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpI3GCny7y3V"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKTUbdnc7y3V"
      },
      "source": [
        "#### Create a model builder for gridsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0H7pzkH7y3W"
      },
      "source": [
        "the even filter shapes aren't recommended because it lacks the ability to devide the previous layer pixels arould the output pixel <a hre = \"https://medium.com/analytics-vidhya/how-to-choose-the-size-of-the-convolution-filter-or-kernel-size-for-cnn-86a55a1e2d15\">(Pandey, 2020)</a>.\n",
        "\n",
        "<a href = \"https://medium.com/@nerdjock/convolutional-neural-network-lesson-9-activation-functions-in-cnns-57def9c6e759\">Machine Learning in Plain English (2023)</a> The most common activation functions are \"relu\" and \"leaky relu\" therefore we would pass it in the grid search.\n",
        "\n",
        "Max pooling excells in image classification, due to how max pooling captures the most prominent features and reduce the variance of the input <a href = \"https://www.linkedin.com/advice/1/how-do-you-choose-appropriate-pooling-method-2uvmc#adaptive-pooling\">(Awad et. al, n.d.)</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1CXJxTu7y3W"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def create_model(hp):\n",
        "    input_shape = (img_height, img_width, 1)\n",
        "    model = Sequential()\n",
        "    metrics = ['accuracy',\n",
        "            Precision(name = 'precision'),\n",
        "            Recall(name = 'recall'),\n",
        "            f1_score]\n",
        "\n",
        "    #input layer. The shape of the input layer must be huge to scale down the image. Ideally (5x5) or (7x7)\n",
        "    model.add(layers.Conv2D(filters = hp.Choice(\"input_filter\", [int(32), int(64), int(128)]),\n",
        "                            kernel_size = hp.Choice(\"input_kernel_size\", [int(3),int(5),int(7)]),\n",
        "                            activation = hp.Choice(\"input_activation\", [\"relu\", \"leaky_relu\"]),\n",
        "                            input_shape = input_shape))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    #Convolutional layer\n",
        "    for i in range(hp.Choice(\"n_conv_layers\", [int(1),int(3)])):\n",
        "        model.add(layers.Conv2D(filters = hp.Choice(f\"conv_filter_{i}\", [int(32), int(64), int(128)]),\n",
        "                            kernel_size = hp.Choice(f\"conv_kernel_size_{i}\", [int(3),int(5),int(7)]),\n",
        "                            activation = hp.Choice(f\"conv_activation_{i}\", [\"relu\", \"leaky_relu\"])))\n",
        "        if i < 1:\n",
        "            model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    #Flatten the resulted image so that the dense layer could extract the patterns and categorize it\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # intermediate layer (note: from previous grid searches, by using only one intermediate layer achived the best score)\n",
        "    model.add(layers.Dense(hp.Choice(f\"n_neurons_dense_{i}\", [int(256), int(512), int(1024)]),\n",
        "                        activation = hp.Choice(f\"dense_activation{i}\", [\"relu\", \"leaky_relu\"])))\n",
        "\n",
        "    #Output layer\n",
        "    #there are 78 classes in the output layer so we will take 78 neurons to classify it\n",
        "    model.add(layers.Dense(78, activation = \"softmax\"))\n",
        "\n",
        "    #optimizers (contributed by Rayhan)\n",
        "    optimizer_name = hp.Choice('optimizer', ['adamw', 'sgd', 'rmsprop'])\n",
        "\n",
        "    if optimizer_name == 'adamw':\n",
        "        optimizer = tf.keras.optimizers.AdamW(learning_rate=hp.Choice('learning_rate', [0.0001, 0.001, 0.005]), weight_decay=1e-4)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=hp.Choice('learning_rate', [0.0001, 0.001, 0.01]), momentum=0.9)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=hp.Choice('learning_rate', [0.0001, 0.001, 0.01]), rho=0.9)\n",
        "\n",
        "    model.compile(metrics = metrics, loss = \"categorical_crossentropy\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqP0fJkx7y3W"
      },
      "source": [
        "In this project, a hyperband gridsearch will be used since the hyperband tuner will stop training the model if the accuracy is low on the validation data <a href = \"https://medium.com/analytics-vidhya/automated-hyperparameter-tuning-with-keras-tuner-and-tensorflow-2-0-31ec83f08a62#:~:text=Overview%20of%20available%20Keras%20Tuners,Tuners%20available%2C%20as%20of%20now.&text=The%20basic%20and%20least%20efficient,from%20a%20search%20space%20randomly.&text=A%20Hyperband%20tuner%20is%20an,accuracy%20on%20the%20validation%20set.&text=Bayesian%20Optimization%20works%20the%20same,Keras%20Tuner%20with%20the%20Documentation.\">(Bag, 2021)</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpcNXkil7y3W"
      },
      "outputs": [],
      "source": [
        "tuner_hyperband = keras_tuner.Hyperband(\n",
        "     hypermodel = create_model,\n",
        "     objective= keras_tuner.Objective(\"val_recall\",\n",
        "                                      direction='max'),\n",
        "     max_epochs=100,\n",
        "     factor = 5,\n",
        "     hyperband_iterations=1, #the number of itterations that the model will go trough the hyperband algorithm\n",
        "     seed = 42, #for reproducibility\n",
        "     overwrite = True,\n",
        "     # directory = \"model_train_log\", #to store logs on the trial results\n",
        "     # project_name = \"medzy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBIJQtLQ7y3W"
      },
      "outputs": [],
      "source": [
        " # Bayesian optimization\n",
        "#tuner_bayes = keras_tuner.BayesianOptimization(\n",
        "#   hypermodel = create_model,\n",
        "#   objective= keras_tuner.Objective(\"val_recall\",\n",
        "#                                     direction='max'),\n",
        "#    max_trials = 17000,\n",
        "#    seed = 42,\n",
        "#    overwrite = True\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kpTpibO7y3W"
      },
      "outputs": [],
      "source": [
        "#tuner_gridSearch = keras_tuner.GridSearch(\n",
        "#    hypermodel = create_model,\n",
        "#    objective = keras_tuner.Objective(\"val_recall\",\n",
        "#                                     direction='max'),\n",
        "#    seed = 42,\n",
        "#    overwrite = True\n",
        "#)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0q3xIi67y3X"
      },
      "source": [
        "#### Start the grid search"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_steps = int(len(validation_images)) #the number of validation for number of batches\n",
        "\n",
        "tuner_hyperband.search(\n",
        "    train_images,\n",
        "    train_name_enc,\n",
        "    epochs = 100,\n",
        "    validation_data = (validation_images, validation_name_enc), #data used to evaluate the model after each epoch\n",
        "    validation_steps = validation_steps, #number of batches used for validation\n",
        "    callbacks = [keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_recall\",  # Metric to monitor\n",
        "            patience=3,  # Number of epochs with no improvement before stopping\n",
        "            mode=\"max\",  # Since we want to maximize recall\n",
        "            restore_best_weights=True,  # Restores the best model weights at the end\n",
        "            verbose=1  # Prints when early stopping happens\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor = \"val_f1_score\",\n",
        "                patience = 3,\n",
        "                mode = \"max\",\n",
        "                restore_best_weights = True,\n",
        "                verbose = 1\n",
        "            ),\n",
        "            keras.callbacks.EarlyStopping(\n",
        "                monitor = \"val_precision\",\n",
        "                patience = 3,\n",
        "                mode = \"max\",\n",
        "                restore_best_weights = True,\n",
        "                verbose = 1\n",
        "            )]\n",
        ")"
      ],
      "metadata": {
        "id": "-IQkzOArfWmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp = tuner_hyperband.get_best_hyperparameters()[0]\n",
        "best_hp.values"
      ],
      "metadata": {
        "id": "6j5Xr6pGf8MF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}