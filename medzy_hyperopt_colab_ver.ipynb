{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andreas-Lukito/Medzy_ocr/blob/main/medzy_hyperopt_colab_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc0HHa_PGtJZ"
      },
      "source": [
        "# Medzy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u31q74K4GtJb"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqiL40_6GtJb"
      },
      "source": [
        "This project aims to develop a machine learning model capable of interpreting doctorsâ€™ handwriting on prescriptions. By accurately detecting and translating challenging handwriting, the model will empower patients to read their prescriptions independently, making it easier for them to purchase their medications without confusion if they run out of medecine or to check if the cleric gave the correct medicine.\n",
        "\n",
        "This model will use Tensor flows' keras convolutional neural network as reference to this <a href = \"https://www.tensorflow.org/tutorials/images/cnn\">documentation</a>. The model will also be trained using this <a href=\"https://www.kaggle.com/datasets/mamun1113/doctors-handwritten-prescription-bd-dataset\">dataset</a> from kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohxPss_mGtJc"
      },
      "source": [
        "## Importing needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperopt\n",
        "!pip install pynvml"
      ],
      "metadata": {
        "id": "e12rEA6lGykr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsj5gt4NGtJd"
      },
      "outputs": [],
      "source": [
        "# basic python libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# data preprocessing libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# tensor flow libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# Fine tuning libraries\n",
        "import hyperopt\n",
        "from hyperopt import hp, fmin, tpe, Trials, space_eval, STATUS_OK\n",
        "\n",
        "# library for gpu utilization\n",
        "import pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIXSh1PiGtJe"
      },
      "source": [
        "## GPU check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKWjbKhiGtJe",
        "outputId": "cc4cfaa4-6401-4a98-ea4e-5e1098a2dd77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow is not using the GPU\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Check if TensorFlow is using the GPU\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"TensorFlow is using the GPU\")\n",
        "\n",
        "    # Initialize the pynvml library\n",
        "    pynvml.nvmlInit()\n",
        "\n",
        "    # Get the number of GPU devices\n",
        "    num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "\n",
        "    # Iterate over GPU devices\n",
        "    for i in range(num_gpus):\n",
        "        # Get the device identifier\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "        # Get the full GPU name\n",
        "        gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
        "        print(\"GPU Name:\", gpu_name)\n",
        "\n",
        "    # Shutdown the pynvml library\n",
        "    pynvml.nvmlShutdown()\n",
        "else:\n",
        "    print(\"TensorFlow is not using the GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUp8IrvTGtJf",
        "outputId": "417e2d1b-0620-4964-957e-0d5eb26326f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gts3HoBTGtJg"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDQj0FStGtJg"
      },
      "source": [
        "### Train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOlj8mLGtJg"
      },
      "source": [
        "#### Train Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjoGCE_5GtJg"
      },
      "outputs": [],
      "source": [
        "train_path = \"./Dataset/archive/dataset/Training\"\n",
        "train_labels = pd.read_csv(os.path.join(train_path,\"training_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN2gisjrGtJh",
        "outputId": "fb555e7a-1841-4df4-e436-91f0ce9f2c3d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IMAGE</th>\n",
              "      <th>MEDICINE_NAME</th>\n",
              "      <th>GENERIC_NAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   IMAGE MEDICINE_NAME GENERIC_NAME\n",
              "0  0.png         Aceta  Paracetamol\n",
              "1  1.png         Aceta  Paracetamol\n",
              "2  2.png         Aceta  Paracetamol\n",
              "3  3.png         Aceta  Paracetamol\n",
              "4  4.png         Aceta  Paracetamol"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqhKnRLGtJh"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96zeKeBZGtJh"
      },
      "outputs": [],
      "source": [
        "medicine_enc = LabelEncoder()\n",
        "train_name_enc = to_categorical(medicine_enc.fit_transform(train_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# train_labels[\"MEDICINE_NAME_ENC\"] = train_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3GuZEFJGtJh",
        "outputId": "510273af-bcb7-473a-8992-9615c641b66a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9OxfOO0GtJh"
      },
      "source": [
        "after encoding there are 78 unique values/medicines since we are using label encoder, we will put them all in to a seperate column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rFEOQaGtJh"
      },
      "source": [
        "#### Train Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvOKyAnRGtJi"
      },
      "outputs": [],
      "source": [
        "#the image width and height to pass to the model\n",
        "img_width = 420\n",
        "img_height = np.round(img_width/3, 0).astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73P5qIzjGtJi"
      },
      "outputs": [],
      "source": [
        "train_images = []\n",
        "train_files = glob.glob(\"./Dataset/archive/dataset/training/training_words/*.png\")\n",
        "for picture in train_files:\n",
        "    image = cv2.resize(cv2.imread(picture), (img_width, img_height))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "\n",
        "    #apply adaptive treshold\n",
        "    image = cv2.adaptiveThreshold(image,\n",
        "                                         255, # the max value\n",
        "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                         cv2.THRESH_BINARY, #the treshold we are using\n",
        "                                         41, #how many pixels to look at\n",
        "                                         10 #noise reduction\n",
        "                                         )\n",
        "\n",
        "    #sharpening the image\n",
        "    # Create the sharpening kernel\n",
        "    kernel = np.array([[-1, -1, 1],\n",
        "                        [-1,  8, -1],\n",
        "                        [-1, -2, -1]])\n",
        "\n",
        "    #increase the contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(7,7))\n",
        "    image = clahe.apply(image)\n",
        "\n",
        "    #blur the image so that the lines are more defined\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "\n",
        "    # Sharpen the image\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "    train_images.append(image)\n",
        "\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(train_images)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YhHFS-nGtJi",
        "outputId": "74d24462-8f05-4553-e33c-51c3c8e82114"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (140, 420)\n",
            "Labels shape: (3120, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset shape:\", train_images[0].shape)\n",
        "print(\"Labels shape:\", train_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Hu_KwEGtJi"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYXSUF7yGtJj",
        "outputId": "d4d56543-7d16-4e0a-f4a4-f02d902f7421"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       ...,\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpEyS13DGtJk",
        "outputId": "361af81f-db78-4618-a4cc-3493ad3bbf7c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2c50e46a050>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(train_images[20], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VAkpk9gGtJk",
        "outputId": "f6ae83c4-1e7c-4695-f70a-700f388adc57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example label: IMAGE                  3.png\n",
            "MEDICINE_NAME          Aceta\n",
            "GENERIC_NAME     Paracetamol\n",
            "Name: 3, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Example label:\", train_labels.iloc[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e0FkNlSGtJl"
      },
      "source": [
        "### Validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxrNpj-TGtJl"
      },
      "source": [
        "#### validation Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asLNtGGnGtJl"
      },
      "outputs": [],
      "source": [
        "validation_path = \"./Dataset/archive/dataset/Validation\"\n",
        "validation_labels = pd.read_csv(os.path.join(validation_path,\"validation_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A59YkxHbGtJl",
        "outputId": "3ec07ad8-ae1a-4f9e-a01c-a4cf704ea35a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IMAGE</th>\n",
              "      <th>MEDICINE_NAME</th>\n",
              "      <th>GENERIC_NAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   IMAGE MEDICINE_NAME GENERIC_NAME\n",
              "0  0.png         Aceta  Paracetamol\n",
              "1  1.png         Aceta  Paracetamol\n",
              "2  2.png         Aceta  Paracetamol\n",
              "3  3.png         Aceta  Paracetamol\n",
              "4  4.png         Aceta  Paracetamol"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EspgN4ZhGtJm"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEGcgKSrGtJm"
      },
      "outputs": [],
      "source": [
        "validation_name_enc = to_categorical(medicine_enc.transform(validation_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# validation_labels[\"MEDECINE_NAME_ENC\"] = validation_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-HL34pvGtJm",
        "outputId": "26f82b4e-8c9c-4feb-f5ad-bec1b24ff735"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(validation_name_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_1xCiLsGtJn",
        "outputId": "e67e7933-1ae3-4784-ad4b-4109eecb6f89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(validation_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLzfJuW9GtJn"
      },
      "source": [
        "#### Validation Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT5j06V1GtJn"
      },
      "outputs": [],
      "source": [
        "validation_images = []\n",
        "validation_files = glob.glob(\"./Dataset/archive/dataset/Validation/validation_words/*.png\")\n",
        "for picture in validation_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    #apply adaptive treshold\n",
        "    image = cv2.adaptiveThreshold(image,\n",
        "                                         255, # the max value\n",
        "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                         cv2.THRESH_BINARY, #the treshold we are using\n",
        "                                         41, #how many pixels to look at\n",
        "                                         10 #noise reduction\n",
        "                                         )\n",
        "\n",
        "    #sharpening the image\n",
        "    # Create the sharpening kernel\n",
        "    kernel = np.array([[-1, -1, 1],\n",
        "                        [-1,  8, -1],\n",
        "                        [-1, -2, -1]])\n",
        "\n",
        "    #increase the contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(7,7))\n",
        "    image = clahe.apply(image)\n",
        "\n",
        "    #blur the image so that the lines are more defined\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "\n",
        "    # Sharpen the image\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "\n",
        "    validation_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "validation_images = np.array(validation_images)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(validation_images)\n",
        "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUWTjKQ5GtJn",
        "outputId": "8c0d6a12-22b0-404f-d39c-627b7b3125d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (780, 140, 420)\n",
            "Labels shape: (780, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset shape:\", validation_images.shape)\n",
        "print(\"Labels shape:\", validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRoGOtCTGtJo"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhb5q2_KGtJo",
        "outputId": "81f04cc6-13c8-46aa-ba3e-cb826064d850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       ...,\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255],\n",
              "       [255, 255, 255, ..., 255, 255, 255]], dtype=uint8)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validation_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLDzjiTjGtJo",
        "outputId": "ab25d178-ad59-42c2-9cce-a12951b73029"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2c5255d21a0>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(validation_images[0], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwc4lycfGtJv",
        "outputId": "72b11f50-7ff3-49ba-8e68-b60219fc5b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example label: IMAGE                  0.png\n",
            "MEDICINE_NAME          Aceta\n",
            "GENERIC_NAME     Paracetamol\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Example label:\", validation_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfXPIJN2GtJv"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBdes_TOGtJv"
      },
      "source": [
        "#### Test Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okmf7MGVGtJv"
      },
      "outputs": [],
      "source": [
        "test_path = \"./Dataset/archive/dataset/Testing\"\n",
        "test_labels = pd.read_csv(os.path.join(test_path,\"testing_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U63Q5QFKGtJw",
        "outputId": "3c89d4d1-8e8b-4097-be8a-e274dec5c77b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IMAGE</th>\n",
              "      <th>MEDICINE_NAME</th>\n",
              "      <th>GENERIC_NAME</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.png</td>\n",
              "      <td>Aceta</td>\n",
              "      <td>Paracetamol</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   IMAGE MEDICINE_NAME GENERIC_NAME\n",
              "0  0.png         Aceta  Paracetamol\n",
              "1  1.png         Aceta  Paracetamol\n",
              "2  2.png         Aceta  Paracetamol\n",
              "3  3.png         Aceta  Paracetamol\n",
              "4  4.png         Aceta  Paracetamol"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvqF5rqGtJw"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR6GPdVOGtJw"
      },
      "outputs": [],
      "source": [
        "test_name_enc = to_categorical(medicine_enc.transform(test_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# test_labels[\"train_medecine_name_enc\"] = test_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi-SnPVhGtJw",
        "outputId": "fb9e1846-d8e6-4e5e-a973-fd202ac625d4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeJH3OkMGtJw"
      },
      "source": [
        "#### Testing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikoo83DHGtJx"
      },
      "outputs": [],
      "source": [
        "test_images = []\n",
        "test_files = glob.glob(\"./Dataset/archive/dataset/Testing/testing_words/*.png\")\n",
        "for picture in test_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    #apply adaptive treshold\n",
        "    image = cv2.adaptiveThreshold(image,\n",
        "                                         255, # the max value\n",
        "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                         cv2.THRESH_BINARY, #the treshold we are using\n",
        "                                         41, #how many pixels to look at\n",
        "                                         10 #noise reduction\n",
        "                                         )\n",
        "\n",
        "    #sharpening the image\n",
        "    # Create the sharpening kernel\n",
        "    kernel = np.array([[-1, -1, 1],\n",
        "                        [-1,  8, -1],\n",
        "                        [-1, -2, -1]])\n",
        "\n",
        "    #increase the contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(7,7))\n",
        "    image = clahe.apply(image)\n",
        "\n",
        "    #blur the image so that the lines are more defined\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "\n",
        "    # Sharpen the image\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "    test_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(test_images)\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og3oHqMiGtJx",
        "outputId": "7ee0aa3d-980f-4151-bafb-c8eeb155ecba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (780, 140, 420)\n",
            "Labels shape: (780, 3)\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset shape:\", test_images.shape)\n",
        "print(\"Labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRICNfFMGtJx"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSLQZ0C_GtJy",
        "outputId": "2b04ceff-9808-44d9-b0a3-c6a3fbe6f981"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(140, 420)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRb5WGV8GtJy",
        "outputId": "0edaf654-eb6c-41c7-dae6-309b2191597e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x2c5282767d0>"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(test_images[0], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VoBw2moGtJy",
        "outputId": "22f6bf11-5d9b-4b60-8993-cfb5fd814a70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example label: IMAGE                  0.png\n",
            "MEDICINE_NAME          Aceta\n",
            "GENERIC_NAME     Paracetamol\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Example label:\", test_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5eQd0nEGtJy"
      },
      "source": [
        "## Building the artificial neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGDLHhDLGtJz"
      },
      "source": [
        "#### Make a model create function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnyhLojVGtJz"
      },
      "source": [
        "##### Parameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSymO6AFGtJz",
        "outputId": "ef65b4c0-7daf-478f-945d-d0ca0bb4922e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#number of classes to determine how many neurons are in the output layer\n",
        "num_classes = len(train_labels[\"MEDICINE_NAME\"].unique())\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXx0BIZbGtJz",
        "outputId": "8a59b95f-3380-4af8-df78-a16f7c02f816"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(140, 420)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#the image size to determine the shape for the convolutional neural network to scan\n",
        "train_images[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKdWqFwzGtJ0"
      },
      "source": [
        "#### Custom metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aBDZLA9GtJ0"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return keras.backend.cast(recall, \"float16\")\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return keras.backend.cast(precision, \"float16\")\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    f1 = 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "    return keras.backend.cast(f1, \"float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM89xQcmGtJ0"
      },
      "source": [
        "#### Create a model builder for gridsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOaTV6HvGtJ0"
      },
      "source": [
        "the even filter shapes aren't recommended because it lacks the ability to devide the previous layer pixels arould the output pixel <a hre = \"https://medium.com/analytics-vidhya/how-to-choose-the-size-of-the-convolution-filter-or-kernel-size-for-cnn-86a55a1e2d15\">(Pandey, 2020)</a>.\n",
        "\n",
        "<a href = \"https://medium.com/@nerdjock/convolutional-neural-network-lesson-9-activation-functions-in-cnns-57def9c6e759\">Machine Learning in Plain English (2023)</a> The most common activation functions are \"relu\" and \"leaky relu\" therefore we would pass it in the grid search.\n",
        "\n",
        "Max pooling excells in image classification, due to how max pooling captures the most prominent features and reduce the variance of the input <a href = \"https://www.linkedin.com/advice/1/how-do-you-choose-appropriate-pooling-method-2uvmc#adaptive-pooling\">(Awad et. al, n.d.)</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsN-QAxeGtJ1"
      },
      "outputs": [],
      "source": [
        "# Model Parameters to Grid Search\n",
        "params_space = {\n",
        "    \"filter_choice\" : hp.choice(\"filter_choice\", [32, 64, 128]),\n",
        "    \"kernel_size\" : hp.choice(\"kernel_size\", [3, 5, 7]),\n",
        "    \"n_neurons\" : hp.choice(\"n_neurons\", [256, 512, 1024]),\n",
        "    \"learning_rate\": hp.uniform(\"learning_rate\",0.001,1),\n",
        "    \"activation\" : hp.choice(\"activation\",['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', 'softmax']),\n",
        "    \"n_conv_layers\": hp.choice(\"n_conv_layers\", [1, 3]),\n",
        "    \"n_pool_layers\": hp.choice(\"n_pool_layers\", [1, 2, 3]),\n",
        "    \"dense_optimizer\": hp.choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n",
        "    \"dropout_rate\": hp.uniform(\"dropout_rate\", 0.1, 0.5)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DXEF9E4GtJ1"
      },
      "outputs": [],
      "source": [
        "def create_model(params):\n",
        "    input_shape = (img_height, img_width, 1)\n",
        "    model = Sequential()\n",
        "    metrics = ['accuracy', recall_m, precision_m]\n",
        "\n",
        "    # Input layer\n",
        "    model.add(layers.Conv2D(\n",
        "        filters = params[\"filter_choice\"],\n",
        "        kernel_size = params[\"kernel_size\"],\n",
        "        activation= params[\"activation\"],\n",
        "        input_shape=input_shape\n",
        "    ))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # Convolutional layers\n",
        "    for i in range(params[\"n_conv_layers\"]):\n",
        "        model.add(layers.Conv2D(\n",
        "            filters= params[\"filter_choice\"],\n",
        "            kernel_size = params[\"kernel_size\"],\n",
        "            activation= params[\"activation\"]\n",
        "        ))\n",
        "        if i < params[\"n_pool_layers\"]:\n",
        "            model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # drop out layers to prevent overfitting\n",
        "    model.add(layers.Dropout(params['dropout_rate']))\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Intermediate dense layer\n",
        "    model.add(layers.Dense(\n",
        "        params[\"n_neurons\"],\n",
        "        activation= params[\"activation\"]\n",
        "    ))\n",
        "\n",
        "    # another drop out layers to prevent overfitting\n",
        "    model.add(layers.Dropout(params['dropout_rate']))\n",
        "\n",
        "    # Output layer (78 classes)\n",
        "    model.add(layers.Dense(78, activation=\"softmax\"))\n",
        "\n",
        "    # Optimizer selection\n",
        "    optimizer_name = params[\"dense_optimizer\"]\n",
        "    if optimizer_name == 'adam': #dynamic learning rate\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate = params[\"learning_rate\"])\n",
        "    elif optimizer_name == 'sgd': #static learning rate\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate = params[\"learning_rate\"])\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics = metrics)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                      mode = 'min', #to match the fmin function from hyperopt\n",
        "                                                                    #we will search for the model with the least loss\n",
        "                                                      verbose=1,\n",
        "                                                      patience=3)\n",
        "\n",
        "    model.fit(train_images, train_name_enc,\n",
        "              validation_data=(validation_images, validation_name_enc),\n",
        "              callbacks=[early_stopping])\n",
        "\n",
        "    loss, accuracy, recall, precision = model.evaluate(validation_images, validation_name_enc)\n",
        "    print(f\"validation Accuracy: {accuracy}\")\n",
        "    print(f\"validation recall: {recall}\")\n",
        "    print(f\"validation precision: {precision}\")\n",
        "    return {\n",
        "        'loss' : loss,\n",
        "        'status' :STATUS_OK,\n",
        "        'model' : model,\n",
        "        'params' : params\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEqAcWtRGtJ1"
      },
      "source": [
        "In this project, a hyperopt gridsearch will be implemented as reference to <a href = \"https://medium.com/@icaro_vazquez/neural-network-hyperparameter-optimization-with-hyperopt-f3e0cb4346c8\">(Icaro, 2024)</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imGHOy3WGtJ1"
      },
      "outputs": [],
      "source": [
        "# initialize trials to keep the history of every trial\n",
        "trial =  Trials()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWhAK90eGtJ2"
      },
      "source": [
        "#### Start the grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CpNxzD3GtJ2",
        "outputId": "a57ad8c4-5e17-40e3-b4a2-c569c8451b1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1/98 [..............................] - ETA: 2:43 - loss: 4.9575 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 1:35 - loss: 5.9244 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 1:31 - loss: 35.9869 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/98 [>.............................] - ETA: 1:32 - loss: 43.4229 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0156            \n",
            " 5/98 [>.............................] - ETA: 1:34 - loss: 48.0111 - accuracy: 0.0125 - recall_m: 0.0125 - precision_m: 0.0125\n",
            " 6/98 [>.............................] - ETA: 1:32 - loss: 61.1733 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0104\n",
            " 7/98 [=>............................] - ETA: 1:29 - loss: 69.1576 - accuracy: 0.0089 - recall_m: 0.0089 - precision_m: 0.0089\n",
            " 8/98 [=>............................] - ETA: 1:28 - loss: 83.3158 - accuracy: 0.0117 - recall_m: 0.0117 - precision_m: 0.0117\n",
            " 9/98 [=>............................] - ETA: 1:27 - loss: 96.3767 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0104\n",
            "10/98 [==>...........................] - ETA: 1:26 - loss: 109.3865 - accuracy: 0.0125 - recall_m: 0.0125 - precision_m: 0.0125\n",
            "11/98 [==>...........................] - ETA: 1:25 - loss: 117.8750 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0142\n",
            "12/98 [==>...........................] - ETA: 1:24 - loss: 119.0412 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0130\n",
            "13/98 [==>...........................] - ETA: 1:23 - loss: 126.4715 - accuracy: 0.0120 - recall_m: 0.0120 - precision_m: 0.0120\n",
            "14/98 [===>..........................] - ETA: 1:21 - loss: 134.2151 - accuracy: 0.0112 - recall_m: 0.0112 - precision_m: 0.0112\n",
            "15/98 [===>..........................] - ETA: 1:20 - loss: 140.9113 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0104\n",
            "16/98 [===>..........................] - ETA: 1:19 - loss: 145.7720 - accuracy: 0.0098 - recall_m: 0.0098 - precision_m: 0.0098\n",
            "17/98 [====>.........................] - ETA: 1:18 - loss: 151.6115 - accuracy: 0.0129 - recall_m: 0.0129 - precision_m: 0.0129\n",
            "18/98 [====>.........................] - ETA: 1:17 - loss: 163.7138 - accuracy: 0.0139 - recall_m: 0.0139 - precision_m: 0.0139\n",
            "19/98 [====>.........................] - ETA: 1:17 - loss: 166.4434 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0148\n",
            "20/98 [=====>........................] - ETA: 1:16 - loss: 181.1895 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "21/98 [=====>........................] - ETA: 1:14 - loss: 187.7414 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0149\n",
            "22/98 [=====>........................] - ETA: 1:13 - loss: 193.7530 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0143\n",
            "23/98 [======>.......................] - ETA: 1:13 - loss: 198.1357 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0150\n",
            "24/98 [======>.......................] - ETA: 1:12 - loss: 200.0125 - accuracy: 0.0143 - recall_m: 0.0143 - precision_m: 0.0144\n",
            "25/98 [======>.......................] - ETA: 1:10 - loss: 203.1879 - accuracy: 0.0150 - recall_m: 0.0150 - precision_m: 0.0150\n",
            "26/98 [======>.......................] - ETA: 1:10 - loss: 205.7705 - accuracy: 0.0144 - recall_m: 0.0144 - precision_m: 0.0145\n",
            "27/98 [=======>......................] - ETA: 1:09 - loss: 209.2755 - accuracy: 0.0139 - recall_m: 0.0139 - precision_m: 0.0139\n",
            "28/98 [=======>......................] - ETA: 1:11 - loss: 212.0721 - accuracy: 0.0134 - recall_m: 0.0134 - precision_m: 0.0134\n",
            "29/98 [=======>......................] - ETA: 1:13 - loss: 216.5434 - accuracy: 0.0140 - recall_m: 0.0140 - precision_m: 0.0140\n",
            "30/98 [========>.....................] - ETA: 1:11 - loss: 222.3499 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0158\n",
            "31/98 [========>.....................] - ETA: 1:10 - loss: 226.5366 - accuracy: 0.0151 - recall_m: 0.0151 - precision_m: 0.0153\n",
            "32/98 [========>.....................] - ETA: 1:09 - loss: 230.0776 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0148\n",
            "33/98 [=========>....................] - ETA: 1:08 - loss: 235.6909 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0144\n",
            "34/98 [=========>....................] - ETA: 1:12 - loss: 242.3855 - accuracy: 0.0138 - recall_m: 0.0138 - precision_m: 0.0139\n",
            "35/98 [=========>....................] - ETA: 1:10 - loss: 246.8078 - accuracy: 0.0134 - recall_m: 0.0134 - precision_m: 0.0135\n",
            "36/98 [==========>...................] - ETA: 1:09 - loss: 251.9021 - accuracy: 0.0139 - recall_m: 0.0139 - precision_m: 0.0140\n",
            "37/98 [==========>...................] - ETA: 1:07 - loss: 256.7618 - accuracy: 0.0152 - recall_m: 0.0152 - precision_m: 0.0153\n",
            "38/98 [==========>...................] - ETA: 1:06 - loss: 260.4751 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0149\n",
            "39/98 [==========>...................] - ETA: 1:05 - loss: 263.4544 - accuracy: 0.0152 - recall_m: 0.0152 - precision_m: 0.0154\n",
            "40/98 [===========>..................] - ETA: 1:04 - loss: 270.8006 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0150\n",
            "41/98 [===========>..................] - ETA: 1:02 - loss: 274.5383 - accuracy: 0.0152 - recall_m: 0.0152 - precision_m: 0.0154\n",
            "42/98 [===========>..................] - ETA: 1:01 - loss: 280.5031 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0150\n",
            "43/98 [============>.................] - ETA: 1:00 - loss: 285.9066 - accuracy: 0.0153 - recall_m: 0.0153 - precision_m: 0.0154\n",
            "44/98 [============>.................] - ETA: 58s - loss: 291.7925 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0151 \n",
            "45/98 [============>.................] - ETA: 57s - loss: 296.4698 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0147\n",
            "46/98 [=============>................] - ETA: 56s - loss: 300.8054 - accuracy: 0.0163 - recall_m: 0.0163 - precision_m: 0.0164\n",
            "47/98 [=============>................] - ETA: 55s - loss: 305.7922 - accuracy: 0.0166 - recall_m: 0.0166 - precision_m: 0.0168\n",
            "48/98 [=============>................] - ETA: 53s - loss: 309.5489 - accuracy: 0.0163 - recall_m: 0.0163 - precision_m: 0.0164\n",
            "49/98 [==============>...............] - ETA: 52s - loss: 312.1847 - accuracy: 0.0159 - recall_m: 0.0159 - precision_m: 0.0161\n",
            "50/98 [==============>...............] - ETA: 51s - loss: 313.8854 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "51/98 [==============>...............] - ETA: 50s - loss: 318.8038 - accuracy: 0.0159 - recall_m: 0.0159 - precision_m: 0.0161\n",
            "52/98 [==============>...............] - ETA: 49s - loss: 321.1512 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "53/98 [===============>..............] - ETA: 50s - loss: 323.9167 - accuracy: 0.0153 - recall_m: 0.0153 - precision_m: 0.0154\n",
            "54/98 [===============>..............] - ETA: 49s - loss: 330.3645 - accuracy: 0.0150 - recall_m: 0.0150 - precision_m: 0.0152\n",
            "55/98 [===============>..............] - ETA: 48s - loss: 335.8921 - accuracy: 0.0153 - recall_m: 0.0153 - precision_m: 0.0155\n",
            "56/98 [================>.............] - ETA: 47s - loss: 340.0776 - accuracy: 0.0151 - recall_m: 0.0151 - precision_m: 0.0152\n",
            "57/98 [================>.............] - ETA: 48s - loss: 345.0793 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0149\n",
            "58/98 [================>.............] - ETA: 47s - loss: 351.0737 - accuracy: 0.0145 - recall_m: 0.0145 - precision_m: 0.0147\n",
            "59/98 [=================>............] - ETA: 46s - loss: 354.9457 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0149\n",
            "60/98 [=================>............] - ETA: 45s - loss: 359.8504 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0147\n",
            "61/98 [=================>............] - ETA: 43s - loss: 362.5157 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0150\n",
            "62/98 [=================>............] - ETA: 42s - loss: 369.4519 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0147\n",
            "63/98 [==================>...........] - ETA: 41s - loss: 372.6004 - accuracy: 0.0144 - recall_m: 0.0144 - precision_m: 0.0145\n",
            "64/98 [==================>...........] - ETA: 39s - loss: 377.5372 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0143\n",
            "65/98 [==================>...........] - ETA: 38s - loss: 383.4861 - accuracy: 0.0139 - recall_m: 0.0139 - precision_m: 0.0140\n",
            "66/98 [===================>..........] - ETA: 37s - loss: 387.0373 - accuracy: 0.0137 - recall_m: 0.0137 - precision_m: 0.0138\n",
            "67/98 [===================>..........] - ETA: 35s - loss: 390.8363 - accuracy: 0.0135 - recall_m: 0.0135 - precision_m: 0.0136\n",
            "68/98 [===================>..........] - ETA: 34s - loss: 394.1862 - accuracy: 0.0133 - recall_m: 0.0133 - precision_m: 0.0134\n",
            "69/98 [====================>.........] - ETA: 33s - loss: 396.5596 - accuracy: 0.0131 - recall_m: 0.0131 - precision_m: 0.0132\n",
            "70/98 [====================>.........] - ETA: 32s - loss: 397.9944 - accuracy: 0.0134 - recall_m: 0.0134 - precision_m: 0.0135\n",
            "71/98 [====================>.........] - ETA: 32s - loss: 400.3863 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0133\n",
            "72/98 [=====================>........] - ETA: 31s - loss: 401.9155 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0131\n",
            "73/98 [=====================>........] - ETA: 29s - loss: 402.5099 - accuracy: 0.0133 - recall_m: 0.0133 - precision_m: 0.0134\n",
            "74/98 [=====================>........] - ETA: 28s - loss: 403.5435 - accuracy: 0.0135 - recall_m: 0.0135 - precision_m: 0.0136\n",
            "75/98 [=====================>........] - ETA: 27s - loss: 407.3973 - accuracy: 0.0133 - recall_m: 0.0133 - precision_m: 0.0134\n",
            "76/98 [======================>.......] - ETA: 26s - loss: 409.3956 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0132\n",
            "77/98 [======================>.......] - ETA: 25s - loss: 411.3800 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0131\n",
            "78/98 [======================>.......] - ETA: 24s - loss: 411.6732 - accuracy: 0.0128 - recall_m: 0.0128 - precision_m: 0.0129\n",
            "79/98 [=======================>......] - ETA: 22s - loss: 412.8251 - accuracy: 0.0127 - recall_m: 0.0127 - precision_m: 0.0127\n",
            "80/98 [=======================>......] - ETA: 21s - loss: 413.3450 - accuracy: 0.0133 - recall_m: 0.0133 - precision_m: 0.0134\n",
            "81/98 [=======================>......] - ETA: 20s - loss: 413.5352 - accuracy: 0.0131 - recall_m: 0.0131 - precision_m: 0.0132\n",
            "82/98 [========================>.....] - ETA: 19s - loss: 413.1370 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0130\n",
            "83/98 [========================>.....] - ETA: 17s - loss: 413.7288 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0133\n",
            "84/98 [========================>.....] - ETA: 16s - loss: 414.4467 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0131\n",
            "85/98 [=========================>....] - ETA: 15s - loss: 413.5504 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0133\n",
            "86/98 [=========================>....] - ETA: 14s - loss: 413.0551 - accuracy: 0.0131 - recall_m: 0.0131 - precision_m: 0.0132\n",
            "87/98 [=========================>....] - ETA: 12s - loss: 413.2381 - accuracy: 0.0129 - recall_m: 0.0129 - precision_m: 0.0130\n",
            "88/98 [=========================>....] - ETA: 11s - loss: 412.1404 - accuracy: 0.0135 - recall_m: 0.0135 - precision_m: 0.0136\n",
            "89/98 [==========================>...] - ETA: 10s - loss: 410.9247 - accuracy: 0.0133 - recall_m: 0.0133 - precision_m: 0.0134\n",
            "90/98 [==========================>...] - ETA: 9s - loss: 410.3028 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0133 \n",
            "91/98 [==========================>...] - ETA: 8s - loss: 408.8112 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0131\n",
            "92/98 [===========================>..] - ETA: 6s - loss: 407.9279 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0133\n",
            "93/98 [===========================>..] - ETA: 5s - loss: 405.8269 - accuracy: 0.0134 - recall_m: 0.0134 - precision_m: 0.0135\n",
            "94/98 [===========================>..] - ETA: 4s - loss: 404.4012 - accuracy: 0.0133 - recall_m: 0.0133 - precision_m: 0.0134\n",
            "95/98 [============================>.] - ETA: 3s - loss: 403.3065 - accuracy: 0.0132 - recall_m: 0.0132 - precision_m: 0.0132\n",
            "96/98 [============================>.] - ETA: 2s - loss: 401.7246 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0131\n",
            "97/98 [============================>.] - ETA: 1s - loss: 400.1662 - accuracy: 0.0129 - recall_m: 0.0129 - precision_m: 0.0130\n",
            "98/98 [==============================] - ETA: 0s - loss: 399.4293 - accuracy: 0.0128 - recall_m: 0.0128 - precision_m: 0.0128\n",
            "98/98 [==============================] - 121s 1s/step - loss: 399.4293 - accuracy: 0.0128 - recall_m: 0.0128 - precision_m: 0.0128 - val_loss: 311.0947 - val_accuracy: 0.0128 - val_recall_m: 0.0125 - val_precision_m: 0.0125\n",
            "\n",
            " 1/25 [>.............................] - ETA: 7s - loss: 442.3707 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 6s - loss: 458.6408 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 5s - loss: 436.7954 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 5s - loss: 441.3813 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 4s - loss: 433.0302 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 4s - loss: 405.2713 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 4s - loss: 389.4785 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 3s - loss: 398.3468 - accuracy: 0.0234 - recall_m: 0.0234 - precision_m: 0.0234            \n",
            " 9/25 [=========>....................] - ETA: 3s - loss: 394.8456 - accuracy: 0.0347 - recall_m: 0.0347 - precision_m: 0.0347\n",
            "10/25 [===========>..................] - ETA: 3s - loss: 363.6512 - accuracy: 0.0312 - recall_m: 0.0312 - precision_m: 0.0312\n",
            "11/25 [============>.................] - ETA: 5s - loss: 354.7356 - accuracy: 0.0284 - recall_m: 0.0284 - precision_m: 0.0284\n",
            "12/25 [=============>................] - ETA: 6s - loss: 353.6703 - accuracy: 0.0260 - recall_m: 0.0260 - precision_m: 0.0260\n",
            "13/25 [==============>...............] - ETA: 5s - loss: 355.6180 - accuracy: 0.0240 - recall_m: 0.0240 - precision_m: 0.0240\n",
            "14/25 [===============>..............] - ETA: 4s - loss: 341.2373 - accuracy: 0.0223 - recall_m: 0.0223 - precision_m: 0.0223\n",
            "15/25 [=================>............] - ETA: 4s - loss: 340.6453 - accuracy: 0.0208 - recall_m: 0.0208 - precision_m: 0.0208\n",
            "16/25 [==================>...........] - ETA: 3s - loss: 329.6988 - accuracy: 0.0195 - recall_m: 0.0195 - precision_m: 0.0195\n",
            "17/25 [===================>..........] - ETA: 3s - loss: 326.3586 - accuracy: 0.0184 - recall_m: 0.0184 - precision_m: 0.0184\n",
            "18/25 [====================>.........] - ETA: 2s - loss: 315.2867 - accuracy: 0.0174 - recall_m: 0.0174 - precision_m: 0.0174\n",
            "19/25 [=====================>........] - ETA: 2s - loss: 310.9540 - accuracy: 0.0164 - recall_m: 0.0164 - precision_m: 0.0164\n",
            "20/25 [=======================>......] - ETA: 1s - loss: 308.7993 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0156\n",
            "21/25 [========================>.....] - ETA: 1s - loss: 310.0376 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0149\n",
            "22/25 [=========================>....] - ETA: 1s - loss: 315.5824 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0142\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 312.0414 - accuracy: 0.0136 - recall_m: 0.0136 - precision_m: 0.0136\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 311.3151 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0130\n",
            "25/25 [==============================] - ETA: 0s - loss: 311.0947 - accuracy: 0.0128 - recall_m: 0.0125 - precision_m: 0.0125\n",
            "25/25 [==============================] - 9s 348ms/step - loss: 311.0947 - accuracy: 0.0128 - recall_m: 0.0125 - precision_m: 0.0125\n",
            "\n",
            "validation Accuracy: 0.012820512987673283              \n",
            "validation recall: 0.012500000186264515                \n",
            "validation precision: 0.012500000186264515             \n",
            " 1/98 [..............................] - ETA: 8:26 - loss: 4.3568 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 2:29 - loss: 4.7589 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 2:14 - loss: 4.9459 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            " 4/98 [>.............................] - ETA: 2:13 - loss: 5.1283 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/98 [>.............................] - ETA: 2:12 - loss: 5.1413 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/98 [>.............................] - ETA: 2:10 - loss: 5.1102 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/98 [=>............................] - ETA: 2:11 - loss: 5.1577 - accuracy: 0.0089 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/98 [=>............................] - ETA: 2:09 - loss: 5.0590 - accuracy: 0.0078 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/98 [=>............................] - ETA: 2:38 - loss: 5.0241 - accuracy: 0.0069 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/98 [==>...........................] - ETA: 2:34 - loss: 5.0349 - accuracy: 0.0094 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/98 [==>...........................] - ETA: 2:28 - loss: 5.0155 - accuracy: 0.0085 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/98 [==>...........................] - ETA: 2:25 - loss: 5.0084 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/98 [==>...........................] - ETA: 2:20 - loss: 4.9932 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/98 [===>..........................] - ETA: 2:17 - loss: 4.9814 - accuracy: 0.0089 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/98 [===>..........................] - ETA: 2:14 - loss: 4.9649 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/98 [===>..........................] - ETA: 2:12 - loss: 4.9458 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/98 [====>.........................] - ETA: 2:14 - loss: 4.9151 - accuracy: 0.0110 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/98 [====>.........................] - ETA: 2:18 - loss: 4.9214 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/98 [====>.........................] - ETA: 2:15 - loss: 4.9050 - accuracy: 0.0099 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/98 [=====>........................] - ETA: 2:12 - loss: 4.8958 - accuracy: 0.0109 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/98 [=====>........................] - ETA: 2:20 - loss: 4.8909 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/98 [=====>........................] - ETA: 2:16 - loss: 4.8620 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/98 [======>.......................] - ETA: 2:13 - loss: 4.8604 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/98 [======>.......................] - ETA: 2:10 - loss: 4.8503 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/98 [======>.......................] - ETA: 2:07 - loss: 4.8476 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "26/98 [======>.......................] - ETA: 2:05 - loss: 4.8406 - accuracy: 0.0108 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "27/98 [=======>......................] - ETA: 2:02 - loss: 4.8258 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "28/98 [=======>......................] - ETA: 2:00 - loss: 4.8193 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "29/98 [=======>......................] - ETA: 1:58 - loss: 4.8120 - accuracy: 0.0129 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "30/98 [========>.....................] - ETA: 2:03 - loss: 4.8088 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "31/98 [========>.....................] - ETA: 2:00 - loss: 4.8029 - accuracy: 0.0131 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "32/98 [========>.....................] - ETA: 1:58 - loss: 4.7987 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "33/98 [=========>....................] - ETA: 1:55 - loss: 4.7903 - accuracy: 0.0133 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "34/98 [=========>....................] - ETA: 1:53 - loss: 4.7857 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "35/98 [=========>....................] - ETA: 1:50 - loss: 4.7813 - accuracy: 0.0152 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "36/98 [==========>...................] - ETA: 1:48 - loss: 4.7742 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "37/98 [==========>...................] - ETA: 1:45 - loss: 4.7640 - accuracy: 0.0152 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "38/98 [==========>...................] - ETA: 1:45 - loss: 4.7627 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "39/98 [==========>...................] - ETA: 1:45 - loss: 4.7531 - accuracy: 0.0152 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "40/98 [===========>..................] - ETA: 1:43 - loss: 4.7472 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "41/98 [===========>..................] - ETA: 1:41 - loss: 4.7473 - accuracy: 0.0152 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "42/98 [===========>..................] - ETA: 1:39 - loss: 4.7589 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "43/98 [============>.................] - ETA: 1:36 - loss: 4.7553 - accuracy: 0.0145 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "44/98 [============>.................] - ETA: 1:35 - loss: 4.7536 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "45/98 [============>.................] - ETA: 1:33 - loss: 4.7488 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "46/98 [=============>................] - ETA: 1:30 - loss: 4.7462 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "47/98 [=============>................] - ETA: 1:28 - loss: 4.7423 - accuracy: 0.0133 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "48/98 [=============>................] - ETA: 1:26 - loss: 4.7394 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "49/98 [==============>...............] - ETA: 1:27 - loss: 4.7348 - accuracy: 0.0140 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "50/98 [==============>...............] - ETA: 1:25 - loss: 4.7335 - accuracy: 0.0144 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "51/98 [==============>...............] - ETA: 1:23 - loss: 4.7300 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "52/98 [==============>...............] - ETA: 1:21 - loss: 4.7313 - accuracy: 0.0150 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "53/98 [===============>..............] - ETA: 1:19 - loss: 4.7300 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "54/98 [===============>..............] - ETA: 1:17 - loss: 4.7297 - accuracy: 0.0145 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "55/98 [===============>..............] - ETA: 1:15 - loss: 4.7226 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "56/98 [================>.............] - ETA: 1:13 - loss: 4.7197 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "57/98 [================>.............] - ETA: 1:11 - loss: 4.7187 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "58/98 [================>.............] - ETA: 1:09 - loss: 4.7179 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "59/98 [=================>............] - ETA: 1:07 - loss: 4.7169 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "60/98 [=================>............] - ETA: 1:05 - loss: 4.7136 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "61/98 [=================>............] - ETA: 1:03 - loss: 4.7115 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "62/98 [=================>............] - ETA: 1:01 - loss: 4.7073 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "63/98 [==================>...........] - ETA: 1:00 - loss: 4.7071 - accuracy: 0.0144 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "64/98 [==================>...........] - ETA: 58s - loss: 4.7051 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "65/98 [==================>...........] - ETA: 56s - loss: 4.7032 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "66/98 [===================>..........] - ETA: 54s - loss: 4.7032 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "67/98 [===================>..........] - ETA: 52s - loss: 4.7010 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "68/98 [===================>..........] - ETA: 50s - loss: 4.6978 - accuracy: 0.0133 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "69/98 [====================>.........] - ETA: 49s - loss: 4.6965 - accuracy: 0.0131 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "70/98 [====================>.........] - ETA: 47s - loss: 4.6954 - accuracy: 0.0129 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "71/98 [====================>.........] - ETA: 45s - loss: 4.6936 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "72/98 [=====================>........] - ETA: 43s - loss: 4.6955 - accuracy: 0.0126 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "73/98 [=====================>........] - ETA: 42s - loss: 4.6924 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "74/98 [=====================>........] - ETA: 40s - loss: 4.6949 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "75/98 [=====================>........] - ETA: 39s - loss: 4.6942 - accuracy: 0.0129 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "76/98 [======================>.......] - ETA: 37s - loss: 4.6925 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "77/98 [======================>.......] - ETA: 35s - loss: 4.6883 - accuracy: 0.0126 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "78/98 [======================>.......] - ETA: 34s - loss: 4.6893 - accuracy: 0.0124 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "79/98 [=======================>......] - ETA: 33s - loss: 4.6862 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "80/98 [=======================>......] - ETA: 31s - loss: 4.6870 - accuracy: 0.0121 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "81/98 [=======================>......] - ETA: 29s - loss: 4.6850 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "82/98 [========================>.....] - ETA: 27s - loss: 4.6827 - accuracy: 0.0118 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "83/98 [========================>.....] - ETA: 25s - loss: 4.6821 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "84/98 [========================>.....] - ETA: 24s - loss: 4.6831 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "85/98 [=========================>....] - ETA: 22s - loss: 4.6805 - accuracy: 0.0114 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "86/98 [=========================>....] - ETA: 20s - loss: 4.6797 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "87/98 [=========================>....] - ETA: 18s - loss: 4.6808 - accuracy: 0.0111 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "88/98 [=========================>....] - ETA: 17s - loss: 4.6804 - accuracy: 0.0110 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "89/98 [==========================>...] - ETA: 15s - loss: 4.6793 - accuracy: 0.0109 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "90/98 [==========================>...] - ETA: 13s - loss: 4.6756 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "91/98 [==========================>...] - ETA: 11s - loss: 4.6735 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "92/98 [===========================>..] - ETA: 10s - loss: 4.6728 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "93/98 [===========================>..] - ETA: 8s - loss: 4.6716 - accuracy: 0.0114 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "94/98 [===========================>..] - ETA: 6s - loss: 4.6730 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "95/98 [============================>.] - ETA: 5s - loss: 4.6717 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "96/98 [============================>.] - ETA: 3s - loss: 4.6720 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "97/98 [============================>.] - ETA: 1s - loss: 4.6721 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - ETA: 0s - loss: 4.6709 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - 182s 2s/step - loss: 4.6709 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 - val_loss: 4.5959 - val_accuracy: 0.0128 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
            "\n",
            " 1/25 [>.............................] - ETA: 8s - loss: 4.9089 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 9s - loss: 4.4816 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 28s - loss: 4.4213 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 34s - loss: 4.4125 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 26s - loss: 4.4767 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 22s - loss: 4.5014 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 18s - loss: 4.5404 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 15s - loss: 4.5367 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/25 [=========>....................] - ETA: 13s - loss: 4.5414 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/25 [===========>..................] - ETA: 12s - loss: 4.4997 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/25 [============>.................] - ETA: 10s - loss: 4.4805 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/25 [=============>................] - ETA: 9s - loss: 4.4498 - accuracy: 0.0260 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00     \n",
            "13/25 [==============>...............] - ETA: 8s - loss: 4.4775 - accuracy: 0.0240 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/25 [===============>..............] - ETA: 7s - loss: 4.5128 - accuracy: 0.0223 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/25 [=================>............] - ETA: 8s - loss: 4.5142 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/25 [==================>...........] - ETA: 7s - loss: 4.5630 - accuracy: 0.0195 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/25 [===================>..........] - ETA: 6s - loss: 4.5682 - accuracy: 0.0184 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/25 [====================>.........] - ETA: 5s - loss: 4.5886 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/25 [=====================>........] - ETA: 4s - loss: 4.5935 - accuracy: 0.0164 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/25 [=======================>......] - ETA: 3s - loss: 4.6076 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/25 [========================>.....] - ETA: 2s - loss: 4.5891 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/25 [=========================>....] - ETA: 2s - loss: 4.5810 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/25 [==========================>...] - ETA: 1s - loss: 4.5629 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.5883 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.5959 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - 16s 634ms/step - loss: 4.5959 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: 0.0                                                              \n",
            "validation precision: 0.0                                                           \n",
            " 1/98 [..............................] - ETA: 10:20 - loss: 4.3552 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 4:57 - loss: 4.5808 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00     \n",
            " 3/98 [..............................] - ETA: 4:55 - loss: 4.6509 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/98 [>.............................] - ETA: 4:55 - loss: 4.6932 - accuracy: 0.0078 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/98 [>.............................] - ETA: 4:55 - loss: 4.8040 - accuracy: 0.0063 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/98 [>.............................] - ETA: 4:47 - loss: 4.8258 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/98 [=>............................] - ETA: 4:46 - loss: 4.8316 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/98 [=>............................] - ETA: 4:38 - loss: 4.8677 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/98 [=>............................] - ETA: 4:35 - loss: 4.8769 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/98 [==>...........................] - ETA: 4:32 - loss: 4.8285 - accuracy: 0.0219 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/98 [==>...........................] - ETA: 4:28 - loss: 4.8700 - accuracy: 0.0199 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/98 [==>...........................] - ETA: 4:26 - loss: 4.9877 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/98 [==>...........................] - ETA: 4:22 - loss: 5.0897 - accuracy: 0.0192 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/98 [===>..........................] - ETA: 4:17 - loss: 5.1417 - accuracy: 0.0179 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/98 [===>..........................] - ETA: 4:13 - loss: 5.1782 - accuracy: 0.0167 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/98 [===>..........................] - ETA: 4:09 - loss: 5.1970 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/98 [====>.........................] - ETA: 4:19 - loss: 5.2384 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/98 [====>.........................] - ETA: 4:14 - loss: 5.2480 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/98 [====>.........................] - ETA: 4:09 - loss: 5.2375 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/98 [=====>........................] - ETA: 4:05 - loss: 5.2435 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/98 [=====>........................] - ETA: 4:02 - loss: 5.2589 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/98 [=====>........................] - ETA: 3:58 - loss: 5.2913 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/98 [======>.......................] - ETA: 3:55 - loss: 5.2812 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/98 [======>.......................] - ETA: 3:51 - loss: 5.2787 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/98 [======>.......................] - ETA: 3:48 - loss: 5.2615 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "26/98 [======>.......................] - ETA: 3:46 - loss: 5.2631 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "27/98 [=======>......................] - ETA: 3:43 - loss: 5.2460 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "28/98 [=======>......................] - ETA: 3:40 - loss: 5.2453 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "29/98 [=======>......................] - ETA: 3:36 - loss: 5.2334 - accuracy: 0.0129 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "30/98 [========>.....................] - ETA: 3:34 - loss: 5.2152 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "31/98 [========>.....................] - ETA: 3:29 - loss: 5.2139 - accuracy: 0.0141 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "32/98 [========>.....................] - ETA: 3:26 - loss: 5.2099 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "33/98 [=========>....................] - ETA: 3:23 - loss: 5.1884 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "34/98 [=========>....................] - ETA: 3:19 - loss: 5.1793 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "35/98 [=========>....................] - ETA: 3:16 - loss: 5.1780 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "36/98 [==========>...................] - ETA: 3:12 - loss: 5.1697 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "37/98 [==========>...................] - ETA: 3:09 - loss: 5.1610 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "38/98 [==========>...................] - ETA: 3:05 - loss: 5.1555 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "39/98 [==========>...................] - ETA: 3:02 - loss: 5.1547 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "40/98 [===========>..................] - ETA: 2:59 - loss: 5.1524 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "41/98 [===========>..................] - ETA: 2:56 - loss: 5.1358 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "42/98 [===========>..................] - ETA: 2:53 - loss: 5.1282 - accuracy: 0.0126 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "43/98 [============>.................] - ETA: 2:49 - loss: 5.1252 - accuracy: 0.0124 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "44/98 [============>.................] - ETA: 2:46 - loss: 5.1208 - accuracy: 0.0121 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "45/98 [============>.................] - ETA: 2:43 - loss: 5.1115 - accuracy: 0.0118 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "46/98 [=============>................] - ETA: 2:40 - loss: 5.1127 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "47/98 [=============>................] - ETA: 2:37 - loss: 5.1092 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "48/98 [=============>................] - ETA: 2:34 - loss: 5.0980 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "49/98 [==============>...............] - ETA: 2:31 - loss: 5.0890 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "50/98 [==============>...............] - ETA: 2:32 - loss: 5.0787 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "51/98 [==============>...............] - ETA: 2:29 - loss: 5.0728 - accuracy: 0.0110 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "52/98 [==============>...............] - ETA: 2:26 - loss: 5.0664 - accuracy: 0.0114 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "53/98 [===============>..............] - ETA: 2:25 - loss: 5.0625 - accuracy: 0.0118 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "54/98 [===============>..............] - ETA: 2:21 - loss: 5.0558 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "55/98 [===============>..............] - ETA: 2:18 - loss: 5.0449 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "56/98 [================>.............] - ETA: 2:14 - loss: 5.0436 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "57/98 [================>.............] - ETA: 2:11 - loss: 5.0338 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "58/98 [================>.............] - ETA: 2:08 - loss: 5.0242 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "59/98 [=================>............] - ETA: 2:04 - loss: 5.0181 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "60/98 [=================>............] - ETA: 2:01 - loss: 5.0140 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "61/98 [=================>............] - ETA: 1:58 - loss: 5.0182 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "62/98 [=================>............] - ETA: 1:54 - loss: 5.0140 - accuracy: 0.0126 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "63/98 [==================>...........] - ETA: 1:51 - loss: 5.0111 - accuracy: 0.0124 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "64/98 [==================>...........] - ETA: 1:50 - loss: 5.0055 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "65/98 [==================>...........] - ETA: 1:46 - loss: 5.0007 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "66/98 [===================>..........] - ETA: 1:43 - loss: 4.9966 - accuracy: 0.0118 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "67/98 [===================>..........] - ETA: 1:40 - loss: 4.9944 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "68/98 [===================>..........] - ETA: 1:37 - loss: 4.9871 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "69/98 [====================>.........] - ETA: 1:34 - loss: 4.9831 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "70/98 [====================>.........] - ETA: 1:31 - loss: 4.9799 - accuracy: 0.0121 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "71/98 [====================>.........] - ETA: 1:28 - loss: 4.9783 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "72/98 [=====================>........] - ETA: 1:25 - loss: 4.9762 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "73/98 [=====================>........] - ETA: 1:22 - loss: 4.9730 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "74/98 [=====================>........] - ETA: 1:18 - loss: 4.9684 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "75/98 [=====================>........] - ETA: 1:15 - loss: 4.9606 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "76/98 [======================>.......] - ETA: 1:11 - loss: 4.9586 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "77/98 [======================>.......] - ETA: 1:08 - loss: 4.9544 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "78/98 [======================>.......] - ETA: 1:05 - loss: 4.9503 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "79/98 [=======================>......] - ETA: 1:01 - loss: 4.9469 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "80/98 [=======================>......] - ETA: 58s - loss: 4.9456 - accuracy: 0.0121 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "81/98 [=======================>......] - ETA: 55s - loss: 4.9422 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "82/98 [========================>.....] - ETA: 51s - loss: 4.9381 - accuracy: 0.0118 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "83/98 [========================>.....] - ETA: 48s - loss: 4.9335 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "84/98 [========================>.....] - ETA: 45s - loss: 4.9311 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "85/98 [=========================>....] - ETA: 42s - loss: 4.9292 - accuracy: 0.0121 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "86/98 [=========================>....] - ETA: 38s - loss: 4.9252 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "87/98 [=========================>....] - ETA: 35s - loss: 4.9209 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "88/98 [=========================>....] - ETA: 32s - loss: 4.9194 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "89/98 [==========================>...] - ETA: 29s - loss: 4.9160 - accuracy: 0.0116 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "90/98 [==========================>...] - ETA: 25s - loss: 4.9154 - accuracy: 0.0118 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "91/98 [==========================>...] - ETA: 22s - loss: 4.9107 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "92/98 [===========================>..] - ETA: 19s - loss: 4.9106 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "93/98 [===========================>..] - ETA: 16s - loss: 4.9061 - accuracy: 0.0114 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "94/98 [===========================>..] - ETA: 13s - loss: 4.9055 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "95/98 [============================>.] - ETA: 9s - loss: 4.9052 - accuracy: 0.0112 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "96/98 [============================>.] - ETA: 6s - loss: 4.9024 - accuracy: 0.0111 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "97/98 [============================>.] - ETA: 3s - loss: 4.9000 - accuracy: 0.0110 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - ETA: 0s - loss: 4.9005 - accuracy: 0.0109 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - 338s 3s/step - loss: 4.9005 - accuracy: 0.0109 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 - val_loss: 4.5661 - val_accuracy: 0.0128 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
            "\n",
            " 1/25 [>.............................] - ETA: 16s - loss: 4.3576 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 15s - loss: 4.6271 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 15s - loss: 4.4575 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 14s - loss: 4.5177 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 14s - loss: 4.5922 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 13s - loss: 4.6112 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 12s - loss: 4.5860 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 12s - loss: 4.5843 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/25 [=========>....................] - ETA: 11s - loss: 4.5098 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/25 [===========>..................] - ETA: 10s - loss: 4.5244 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/25 [============>.................] - ETA: 9s - loss: 4.5238 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "12/25 [=============>................] - ETA: 9s - loss: 4.5580 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/25 [==============>...............] - ETA: 8s - loss: 4.5365 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/25 [===============>..............] - ETA: 7s - loss: 4.5242 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/25 [=================>............] - ETA: 7s - loss: 4.4978 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/25 [==================>...........] - ETA: 6s - loss: 4.5081 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/25 [===================>..........] - ETA: 5s - loss: 4.5203 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/25 [====================>.........] - ETA: 4s - loss: 4.5118 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            "19/25 [=====================>........] - ETA: 4s - loss: 4.5191 - accuracy: 0.0164 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/25 [=======================>......] - ETA: 3s - loss: 4.5527 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/25 [========================>.....] - ETA: 2s - loss: 4.5266 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/25 [=========================>....] - ETA: 2s - loss: 4.5237 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/25 [==========================>...] - ETA: 1s - loss: 4.5405 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.5577 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.5661 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - 17s 694ms/step - loss: 4.5661 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                          \n",
            "validation recall: 0.0                                                             \n",
            "validation precision: 0.0                                                          \n",
            " 1/98 [..............................] - ETA: 2:39 - loss: 261.5641 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 1:33 - loss: 13493331820544.0000 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0156 \n",
            " 3/98 [..............................] - ETA: 1:32 - loss: 10437054693376.0000 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0104\n",
            " 4/98 [>.............................] - ETA: 2:35 - loss: 8534560342016.0000 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0156 \n",
            " 5/98 [>.............................] - ETA: 2:51 - loss: 7460708941824.0000 - accuracy: 0.0125 - recall_m: 0.0125 - precision_m: 0.0125\n",
            " 6/98 [>.............................] - ETA: 2:32 - loss: 6225678303232.0000 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0104\n",
            " 7/98 [=>............................] - ETA: 2:20 - loss: 5336311267328.0000 - accuracy: 0.0089 - recall_m: 0.0089 - precision_m: 0.0089\n",
            " 8/98 [=>............................] - ETA: 2:12 - loss: 4669272227840.0000 - accuracy: 0.0078 - recall_m: 0.0078 - precision_m: 0.0078\n",
            " 9/98 [=>............................] - ETA: 2:08 - loss: 4150464086016.0000 - accuracy: 0.0139 - recall_m: 0.0104 - precision_m: 0.0105\n",
            "10/98 [==>...........................] - ETA: 2:02 - loss: 3735417782272.0000 - accuracy: 0.0125 - recall_m: 0.0094 - precision_m: 0.0095\n",
            "11/98 [==>...........................] - ETA: 1:57 - loss: 3395834347520.0000 - accuracy: 0.0114 - recall_m: 0.0085 - precision_m: 0.0086\n",
            "12/98 [==>...........................] - ETA: 1:53 - loss: 3112848064512.0000 - accuracy: 0.0130 - recall_m: 0.0078 - precision_m: 0.0079\n",
            "13/98 [==>...........................] - ETA: 1:49 - loss: 2873398394880.0000 - accuracy: 0.0168 - recall_m: 0.0072 - precision_m: 0.0073\n",
            "14/98 [===>..........................] - ETA: 1:47 - loss: 2668155633664.0000 - accuracy: 0.0156 - recall_m: 0.0067 - precision_m: 0.0068\n",
            "15/98 [===>..........................] - ETA: 1:44 - loss: 2490278608896.0000 - accuracy: 0.0167 - recall_m: 0.0063 - precision_m: 0.0063\n",
            "16/98 [===>..........................] - ETA: 1:41 - loss: 2334636113920.0000 - accuracy: 0.0156 - recall_m: 0.0059 - precision_m: 0.0059\n",
            "17/98 [====>.........................] - ETA: 1:54 - loss: 2197304639488.0000 - accuracy: 0.0165 - recall_m: 0.0055 - precision_m: 0.0056\n",
            "18/98 [====>.........................] - ETA: 1:51 - loss: 2075232043008.0000 - accuracy: 0.0156 - recall_m: 0.0052 - precision_m: 0.0053\n",
            "19/98 [====>.........................] - ETA: 1:48 - loss: 1966009352192.0000 - accuracy: 0.0148 - recall_m: 0.0049 - precision_m: 0.0050\n",
            "20/98 [=====>........................] - ETA: 1:45 - loss: 1867708891136.0000 - accuracy: 0.0141 - recall_m: 0.0047 - precision_m: 0.0047\n",
            "21/98 [=====>........................] - ETA: 1:42 - loss: 1778770378752.0000 - accuracy: 0.0134 - recall_m: 0.0045 - precision_m: 0.0045\n",
            "22/98 [=====>........................] - ETA: 1:40 - loss: 1697917173760.0000 - accuracy: 0.0128 - recall_m: 0.0043 - precision_m: 0.0043\n",
            "23/98 [======>.......................] - ETA: 1:37 - loss: 1624094670848.0000 - accuracy: 0.0136 - recall_m: 0.0041 - precision_m: 0.0041\n",
            "24/98 [======>.......................] - ETA: 1:35 - loss: 1556424032256.0000 - accuracy: 0.0143 - recall_m: 0.0039 - precision_m: 0.0039\n",
            "25/98 [======>.......................] - ETA: 1:33 - loss: 1494167060480.0000 - accuracy: 0.0137 - recall_m: 0.0037 - precision_m: 0.0038\n",
            "26/98 [======>.......................] - ETA: 1:30 - loss: 1436699197440.0000 - accuracy: 0.0132 - recall_m: 0.0036 - precision_m: 0.0036\n",
            "27/98 [=======>......................] - ETA: 1:28 - loss: 1383488028672.0000 - accuracy: 0.0139 - recall_m: 0.0035 - precision_m: 0.0035\n",
            "28/98 [=======>......................] - ETA: 1:27 - loss: 1334077816832.0000 - accuracy: 0.0145 - recall_m: 0.0033 - precision_m: 0.0034\n",
            "29/98 [=======>......................] - ETA: 1:25 - loss: 1288075083776.0000 - accuracy: 0.0140 - recall_m: 0.0032 - precision_m: 0.0033\n",
            "30/98 [========>.....................] - ETA: 1:22 - loss: 1245139304448.0000 - accuracy: 0.0135 - recall_m: 0.0031 - precision_m: 0.0032\n",
            "31/98 [========>.....................] - ETA: 1:21 - loss: 1204973469696.0000 - accuracy: 0.0141 - recall_m: 0.0030 - precision_m: 0.0031\n",
            "32/98 [========>.....................] - ETA: 1:19 - loss: 1167318056960.0000 - accuracy: 0.0137 - recall_m: 0.0029 - precision_m: 0.0030\n",
            "33/98 [=========>....................] - ETA: 1:17 - loss: 1131944738816.0000 - accuracy: 0.0133 - recall_m: 0.0028 - precision_m: 0.0029\n",
            "34/98 [=========>....................] - ETA: 1:16 - loss: 1098652319744.0000 - accuracy: 0.0129 - recall_m: 0.0028 - precision_m: 0.0028\n",
            "35/98 [=========>....................] - ETA: 1:14 - loss: 1067262214144.0000 - accuracy: 0.0134 - recall_m: 0.0027 - precision_m: 0.0027\n",
            "36/98 [==========>...................] - ETA: 1:13 - loss: 1037616021504.0000 - accuracy: 0.0130 - recall_m: 0.0026 - precision_m: 0.0026\n",
            "37/98 [==========>...................] - ETA: 1:11 - loss: 1009572380672.0000 - accuracy: 0.0135 - recall_m: 0.0034 - precision_m: 0.0038\n",
            "38/98 [==========>...................] - ETA: 1:11 - loss: 983004676096.0000 - accuracy: 0.0132 - recall_m: 0.0033 - precision_m: 0.0037 \n",
            "39/98 [==========>...................] - ETA: 1:14 - loss: 957799399424.0000 - accuracy: 0.0128 - recall_m: 0.0032 - precision_m: 0.0037\n",
            "40/98 [===========>..................] - ETA: 1:12 - loss: 933854445568.0000 - accuracy: 0.0125 - recall_m: 0.0031 - precision_m: 0.0036\n",
            "41/98 [===========>..................] - ETA: 1:10 - loss: 911077539840.0000 - accuracy: 0.0122 - recall_m: 0.0030 - precision_m: 0.0035\n",
            "42/98 [===========>..................] - ETA: 1:09 - loss: 889385189376.0000 - accuracy: 0.0126 - recall_m: 0.0030 - precision_m: 0.0034\n",
            "43/98 [============>.................] - ETA: 1:07 - loss: 868701831168.0000 - accuracy: 0.0124 - recall_m: 0.0029 - precision_m: 0.0033\n",
            "44/98 [============>.................] - ETA: 1:06 - loss: 848958586880.0000 - accuracy: 0.0121 - recall_m: 0.0028 - precision_m: 0.0032\n",
            "45/98 [============>.................] - ETA: 1:04 - loss: 830092869632.0000 - accuracy: 0.0118 - recall_m: 0.0028 - precision_m: 0.0032\n",
            "46/98 [=============>................] - ETA: 1:02 - loss: 812047335424.0000 - accuracy: 0.0115 - recall_m: 0.0027 - precision_m: 0.0031\n",
            "47/98 [=============>................] - ETA: 1:01 - loss: 794769752064.0000 - accuracy: 0.0126 - recall_m: 0.0033 - precision_m: 0.0137\n",
            "48/98 [=============>................] - ETA: 59s - loss: 778212016128.0000 - accuracy: 0.0124 - recall_m: 0.0033 - precision_m: 0.0134 \n",
            "49/98 [==============>...............] - ETA: 58s - loss: 762330152960.0000 - accuracy: 0.0134 - recall_m: 0.0032 - precision_m: 0.0131\n",
            "50/98 [==============>...............] - ETA: 56s - loss: 747083530240.0000 - accuracy: 0.0131 - recall_m: 0.0031 - precision_m: 0.0128\n",
            "51/98 [==============>...............] - ETA: 55s - loss: 732434857984.0000 - accuracy: 0.0135 - recall_m: 0.0031 - precision_m: 0.0126\n",
            "52/98 [==============>...............] - ETA: 53s - loss: 718349598720.0000 - accuracy: 0.0138 - recall_m: 0.0030 - precision_m: 0.0124\n",
            "53/98 [===============>..............] - ETA: 52s - loss: 704795836416.0000 - accuracy: 0.0147 - recall_m: 0.0029 - precision_m: 0.0121\n",
            "54/98 [===============>..............] - ETA: 51s - loss: 691744014336.0000 - accuracy: 0.0145 - recall_m: 0.0029 - precision_m: 0.0119\n",
            "55/98 [===============>..............] - ETA: 49s - loss: 679166869504.0000 - accuracy: 0.0148 - recall_m: 0.0028 - precision_m: 0.0117\n",
            "56/98 [================>.............] - ETA: 48s - loss: 667038908416.0000 - accuracy: 0.0156 - recall_m: 0.0033 - precision_m: 0.0122\n",
            "57/98 [================>.............] - ETA: 47s - loss: 655336472576.0000 - accuracy: 0.0154 - recall_m: 0.0033 - precision_m: 0.0120\n",
            "58/98 [================>.............] - ETA: 45s - loss: 644037541888.0000 - accuracy: 0.0151 - recall_m: 0.0032 - precision_m: 0.0118\n",
            "59/98 [=================>............] - ETA: 44s - loss: 633121669120.0000 - accuracy: 0.0148 - recall_m: 0.0032 - precision_m: 0.0116\n",
            "60/98 [=================>............] - ETA: 44s - loss: 622569652224.0000 - accuracy: 0.0146 - recall_m: 0.0031 - precision_m: 0.0114\n",
            "61/98 [=================>............] - ETA: 43s - loss: 612363599872.0000 - accuracy: 0.0149 - recall_m: 0.0031 - precision_m: 0.0112\n",
            "62/98 [=================>............] - ETA: 42s - loss: 602486734848.0000 - accuracy: 0.0146 - recall_m: 0.0030 - precision_m: 0.0110\n",
            "63/98 [==================>...........] - ETA: 40s - loss: 592923459584.0000 - accuracy: 0.0149 - recall_m: 0.0030 - precision_m: 0.0109\n",
            "64/98 [==================>...........] - ETA: 39s - loss: 583659028480.0000 - accuracy: 0.0146 - recall_m: 0.0029 - precision_m: 0.0107\n",
            "65/98 [==================>...........] - ETA: 38s - loss: 574679678976.0000 - accuracy: 0.0144 - recall_m: 0.0029 - precision_m: 0.0105\n",
            "66/98 [===================>..........] - ETA: 37s - loss: 565972369408.0000 - accuracy: 0.0142 - recall_m: 0.0028 - precision_m: 0.0104\n",
            "67/98 [===================>..........] - ETA: 35s - loss: 557525041152.0000 - accuracy: 0.0140 - recall_m: 0.0028 - precision_m: 0.0102\n",
            "68/98 [===================>..........] - ETA: 34s - loss: 549326159872.0000 - accuracy: 0.0142 - recall_m: 0.0028 - precision_m: 0.0101\n",
            "69/98 [====================>.........] - ETA: 33s - loss: 541364912128.0000 - accuracy: 0.0140 - recall_m: 0.0027 - precision_m: 0.0099\n",
            "70/98 [====================>.........] - ETA: 32s - loss: 533631107072.0000 - accuracy: 0.0143 - recall_m: 0.0027 - precision_m: 0.0098\n",
            "71/98 [====================>.........] - ETA: 31s - loss: 526115176448.0000 - accuracy: 0.0145 - recall_m: 0.0026 - precision_m: 0.0096\n",
            "72/98 [=====================>........] - ETA: 29s - loss: 518808010752.0000 - accuracy: 0.0143 - recall_m: 0.0026 - precision_m: 0.0095\n",
            "73/98 [=====================>........] - ETA: 28s - loss: 511701057536.0000 - accuracy: 0.0141 - recall_m: 0.0026 - precision_m: 0.0094\n",
            "74/98 [=====================>........] - ETA: 27s - loss: 504786190336.0000 - accuracy: 0.0139 - recall_m: 0.0025 - precision_m: 0.0092\n",
            "75/98 [=====================>........] - ETA: 26s - loss: 498055708672.0000 - accuracy: 0.0137 - recall_m: 0.0025 - precision_m: 0.0091\n",
            "76/98 [======================>.......] - ETA: 24s - loss: 491502338048.0000 - accuracy: 0.0136 - recall_m: 0.0025 - precision_m: 0.0090\n",
            "77/98 [======================>.......] - ETA: 23s - loss: 485119197184.0000 - accuracy: 0.0134 - recall_m: 0.0024 - precision_m: 0.0089\n",
            "78/98 [======================>.......] - ETA: 23s - loss: 478899699712.0000 - accuracy: 0.0132 - recall_m: 0.0024 - precision_m: 0.0088\n",
            "79/98 [=======================>......] - ETA: 22s - loss: 472837685248.0000 - accuracy: 0.0131 - recall_m: 0.0024 - precision_m: 0.0087\n",
            "80/98 [=======================>......] - ETA: 21s - loss: 466927222784.0000 - accuracy: 0.0129 - recall_m: 0.0023 - precision_m: 0.0086\n",
            "81/98 [=======================>......] - ETA: 19s - loss: 461162676224.0000 - accuracy: 0.0127 - recall_m: 0.0023 - precision_m: 0.0084\n",
            "82/98 [========================>.....] - ETA: 18s - loss: 455538769920.0000 - accuracy: 0.0126 - recall_m: 0.0023 - precision_m: 0.0083\n",
            "83/98 [========================>.....] - ETA: 17s - loss: 450050326528.0000 - accuracy: 0.0124 - recall_m: 0.0023 - precision_m: 0.0082\n",
            "84/98 [========================>.....] - ETA: 16s - loss: 444692594688.0000 - accuracy: 0.0123 - recall_m: 0.0022 - precision_m: 0.0081\n",
            "85/98 [=========================>....] - ETA: 15s - loss: 439460921344.0000 - accuracy: 0.0121 - recall_m: 0.0022 - precision_m: 0.0080\n",
            "86/98 [=========================>....] - ETA: 13s - loss: 434350915584.0000 - accuracy: 0.0120 - recall_m: 0.0022 - precision_m: 0.0080\n",
            "87/98 [=========================>....] - ETA: 12s - loss: 429358350336.0000 - accuracy: 0.0119 - recall_m: 0.0022 - precision_m: 0.0079\n",
            "88/98 [=========================>....] - ETA: 11s - loss: 424479293440.0000 - accuracy: 0.0117 - recall_m: 0.0021 - precision_m: 0.0078\n",
            "89/98 [==========================>...] - ETA: 10s - loss: 419709878272.0000 - accuracy: 0.0119 - recall_m: 0.0021 - precision_m: 0.0077\n",
            "90/98 [==========================>...] - ETA: 9s - loss: 415046434816.0000 - accuracy: 0.0118 - recall_m: 0.0021 - precision_m: 0.0076 \n",
            "91/98 [==========================>...] - ETA: 8s - loss: 410485456896.0000 - accuracy: 0.0117 - recall_m: 0.0021 - precision_m: 0.0075\n",
            "92/98 [===========================>..] - ETA: 7s - loss: 406023667712.0000 - accuracy: 0.0115 - recall_m: 0.0020 - precision_m: 0.0074\n",
            "93/98 [===========================>..] - ETA: 5s - loss: 401657823232.0000 - accuracy: 0.0114 - recall_m: 0.0020 - precision_m: 0.0074\n",
            "94/98 [===========================>..] - ETA: 4s - loss: 397384876032.0000 - accuracy: 0.0113 - recall_m: 0.0020 - precision_m: 0.0073\n",
            "95/98 [============================>.] - ETA: 3s - loss: 393201876992.0000 - accuracy: 0.0112 - recall_m: 0.0020 - precision_m: 0.0072\n",
            "96/98 [============================>.] - ETA: 2s - loss: 389106008064.0000 - accuracy: 0.0111 - recall_m: 0.0020 - precision_m: 0.0071\n",
            "97/98 [============================>.] - ETA: 1s - loss: 385094615040.0000 - accuracy: 0.0110 - recall_m: 0.0019 - precision_m: 0.0071\n",
            "98/98 [==============================] - ETA: 0s - loss: 383119785984.0000 - accuracy: 0.0109 - recall_m: 0.0019 - precision_m: 0.0070\n",
            "98/98 [==============================] - 123s 1s/step - loss: 383119785984.0000 - accuracy: 0.0109 - recall_m: 0.0019 - precision_m: 0.0070 - val_loss: 344.8470 - val_accuracy: 0.0128 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
            "\n",
            " 1/25 [>.............................] - ETA: 5s - loss: 5.2829 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 4s - loss: 5.1688 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 5s - loss: 5.0123 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 5s - loss: 5.0431 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 4s - loss: 5.0058 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 4s - loss: 5.0495 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 3s - loss: 5.0403 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 3s - loss: 4.9980 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/25 [=========>....................] - ETA: 3s - loss: 5.0448 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/25 [===========>..................] - ETA: 3s - loss: 5.0351 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/25 [============>.................] - ETA: 2s - loss: 5.0401 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/25 [=============>................] - ETA: 2s - loss: 4.9741 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/25 [==============>...............] - ETA: 2s - loss: 4.9892 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/25 [===============>..............] - ETA: 4s - loss: 5.0206 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/25 [=================>............] - ETA: 4s - loss: 4.9623 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/25 [==================>...........] - ETA: 3s - loss: 4.9443 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/25 [===================>..........] - ETA: 3s - loss: 195.4154 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/25 [====================>.........] - ETA: 2s - loss: 202.0772 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/25 [=====================>........] - ETA: 2s - loss: 191.7487 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/25 [=======================>......] - ETA: 1s - loss: 182.4141 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/25 [========================>.....] - ETA: 1s - loss: 173.9816 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/25 [=========================>....] - ETA: 1s - loss: 166.2756 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/25 [==========================>...] - ETA: 0s - loss: 159.2287 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            "24/25 [===========================>..] - ETA: 0s - loss: 350.1589 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - ETA: 0s - loss: 344.8470 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - 9s 347ms/step - loss: 344.8470 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: 0.0                                                              \n",
            "validation precision: 0.0                                                           \n",
            " 1/98 [..............................] - ETA: 18:35 - loss: 5.0643 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 6:18 - loss: 78.3923 - accuracy: 0.0312 - recall_m: 0.0312 - precision_m: 0.0323            \n",
            " 3/98 [..............................] - ETA: 6:00 - loss: 795.1613 - accuracy: 0.0208 - recall_m: 0.0208 - precision_m: 0.0215\n",
            " 4/98 [>.............................] - ETA: 8:17 - loss: 1347.2844 - accuracy: 0.0234 - recall_m: 0.0234 - precision_m: 0.0239\n",
            " 5/98 [>.............................] - ETA: 7:33 - loss: 1725.9426 - accuracy: 0.0188 - recall_m: 0.0188 - precision_m: 0.0192\n",
            " 6/98 [>.............................] - ETA: 7:06 - loss: 2161.8997 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0160\n",
            " 7/98 [=>............................] - ETA: 6:48 - loss: 2440.2422 - accuracy: 0.0134 - recall_m: 0.0134 - precision_m: 0.0137\n",
            " 8/98 [=>............................] - ETA: 6:32 - loss: 2786.6917 - accuracy: 0.0117 - recall_m: 0.0117 - precision_m: 0.0120\n",
            " 9/98 [=>............................] - ETA: 6:19 - loss: 2699.2485 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0106\n",
            "10/98 [==>...........................] - ETA: 6:08 - loss: 2970.1411 - accuracy: 0.0094 - recall_m: 0.0094 - precision_m: 0.0096\n",
            "11/98 [==>...........................] - ETA: 5:59 - loss: 3097.6438 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0144\n",
            "12/98 [==>...........................] - ETA: 6:14 - loss: 3247.8398 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0132\n",
            "13/98 [==>...........................] - ETA: 6:04 - loss: 3285.1299 - accuracy: 0.0120 - recall_m: 0.0120 - precision_m: 0.0122\n",
            "14/98 [===>..........................] - ETA: 5:58 - loss: 3318.9724 - accuracy: 0.0112 - recall_m: 0.0112 - precision_m: 0.0113\n",
            "15/98 [===>..........................] - ETA: 6:13 - loss: 3396.3994 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0106\n",
            "16/98 [===>..........................] - ETA: 6:03 - loss: 3487.0640 - accuracy: 0.0117 - recall_m: 0.0117 - precision_m: 0.0118\n",
            "17/98 [====>.........................] - ETA: 5:56 - loss: 3595.5552 - accuracy: 0.0110 - recall_m: 0.0110 - precision_m: 0.0111\n",
            "18/98 [====>.........................] - ETA: 5:50 - loss: 3659.5615 - accuracy: 0.0104 - recall_m: 0.0104 - precision_m: 0.0105\n",
            "19/98 [====>.........................] - ETA: 5:53 - loss: 3754.7302 - accuracy: 0.0099 - recall_m: 0.0099 - precision_m: 0.0100\n",
            "20/98 [=====>........................] - ETA: 5:50 - loss: 3827.2058 - accuracy: 0.0094 - recall_m: 0.0094 - precision_m: 0.0095\n",
            "21/98 [=====>........................] - ETA: 5:42 - loss: 3912.3105 - accuracy: 0.0089 - recall_m: 0.0089 - precision_m: 0.0090\n",
            "22/98 [=====>........................] - ETA: 5:45 - loss: 3954.2908 - accuracy: 0.0128 - recall_m: 0.0128 - precision_m: 0.0129\n",
            "23/98 [======>.......................] - ETA: 5:37 - loss: 4012.5459 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0150\n",
            "24/98 [======>.......................] - ETA: 5:36 - loss: 4036.8467 - accuracy: 0.0143 - recall_m: 0.0143 - precision_m: 0.0144\n",
            "25/98 [======>.......................] - ETA: 5:32 - loss: 4060.6782 - accuracy: 0.0150 - recall_m: 0.0150 - precision_m: 0.0151\n",
            "26/98 [======>.......................] - ETA: 5:25 - loss: 4123.4033 - accuracy: 0.0144 - recall_m: 0.0144 - precision_m: 0.0145\n",
            "27/98 [=======>......................] - ETA: 5:29 - loss: 4134.1201 - accuracy: 0.0139 - recall_m: 0.0139 - precision_m: 0.0140\n",
            "28/98 [=======>......................] - ETA: 5:22 - loss: 4138.8237 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "29/98 [=======>......................] - ETA: 5:15 - loss: 4134.8667 - accuracy: 0.0162 - recall_m: 0.0162 - precision_m: 0.0162\n",
            "30/98 [========>.....................] - ETA: 5:09 - loss: 4143.7075 - accuracy: 0.0167 - recall_m: 0.0167 - precision_m: 0.0167\n",
            "31/98 [========>.....................] - ETA: 5:03 - loss: 4164.3940 - accuracy: 0.0171 - recall_m: 0.0171 - precision_m: 0.0172\n",
            "32/98 [========>.....................] - ETA: 4:56 - loss: 4149.8560 - accuracy: 0.0166 - recall_m: 0.0166 - precision_m: 0.0167\n",
            "33/98 [=========>....................] - ETA: 4:56 - loss: 4148.7041 - accuracy: 0.0180 - recall_m: 0.0180 - precision_m: 0.0181\n",
            "34/98 [=========>....................] - ETA: 4:50 - loss: 4148.4629 - accuracy: 0.0184 - recall_m: 0.0184 - precision_m: 0.0184\n",
            "35/98 [=========>....................] - ETA: 4:43 - loss: 4165.2915 - accuracy: 0.0179 - recall_m: 0.0179 - precision_m: 0.0179\n",
            "36/98 [==========>...................] - ETA: 4:37 - loss: 4178.4590 - accuracy: 0.0182 - recall_m: 0.0182 - precision_m: 0.0183\n",
            "37/98 [==========>...................] - ETA: 4:38 - loss: 4174.1562 - accuracy: 0.0177 - recall_m: 0.0177 - precision_m: 0.0178\n",
            "38/98 [==========>...................] - ETA: 4:32 - loss: 4212.5283 - accuracy: 0.0181 - recall_m: 0.0181 - precision_m: 0.0181\n",
            "39/98 [==========>...................] - ETA: 4:26 - loss: 4230.0122 - accuracy: 0.0184 - recall_m: 0.0184 - precision_m: 0.0185\n",
            "40/98 [===========>..................] - ETA: 4:20 - loss: 4252.4380 - accuracy: 0.0180 - recall_m: 0.0180 - precision_m: 0.0180\n",
            "41/98 [===========>..................] - ETA: 4:15 - loss: 4265.4160 - accuracy: 0.0175 - recall_m: 0.0175 - precision_m: 0.0176\n",
            "42/98 [===========>..................] - ETA: 4:13 - loss: 4320.6309 - accuracy: 0.0171 - recall_m: 0.0171 - precision_m: 0.0172\n",
            "43/98 [============>.................] - ETA: 4:07 - loss: 4327.8071 - accuracy: 0.0167 - recall_m: 0.0167 - precision_m: 0.0168\n",
            "44/98 [============>.................] - ETA: 4:02 - loss: 4359.3901 - accuracy: 0.0163 - recall_m: 0.0163 - precision_m: 0.0164\n",
            "45/98 [============>.................] - ETA: 3:57 - loss: 4374.4248 - accuracy: 0.0160 - recall_m: 0.0160 - precision_m: 0.0160\n",
            "46/98 [=============>................] - ETA: 3:51 - loss: 4390.5327 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "47/98 [=============>................] - ETA: 3:50 - loss: 4391.8892 - accuracy: 0.0153 - recall_m: 0.0153 - precision_m: 0.0153\n",
            "48/98 [=============>................] - ETA: 3:45 - loss: 4430.7598 - accuracy: 0.0150 - recall_m: 0.0150 - precision_m: 0.0150\n",
            "49/98 [==============>...............] - ETA: 3:40 - loss: 4450.8149 - accuracy: 0.0153 - recall_m: 0.0153 - precision_m: 0.0153\n",
            "50/98 [==============>...............] - ETA: 3:34 - loss: 4444.6660 - accuracy: 0.0169 - recall_m: 0.0169 - precision_m: 0.0169\n",
            "51/98 [==============>...............] - ETA: 3:29 - loss: 4469.4102 - accuracy: 0.0165 - recall_m: 0.0165 - precision_m: 0.0166\n",
            "52/98 [==============>...............] - ETA: 3:24 - loss: 4503.8311 - accuracy: 0.0168 - recall_m: 0.0168 - precision_m: 0.0169\n",
            "53/98 [===============>..............] - ETA: 3:18 - loss: 4498.7056 - accuracy: 0.0171 - recall_m: 0.0171 - precision_m: 0.0171\n",
            "54/98 [===============>..............] - ETA: 3:16 - loss: 4509.1479 - accuracy: 0.0174 - recall_m: 0.0174 - precision_m: 0.0174\n",
            "55/98 [===============>..............] - ETA: 3:11 - loss: 4524.8652 - accuracy: 0.0170 - recall_m: 0.0170 - precision_m: 0.0171\n",
            "56/98 [================>.............] - ETA: 3:05 - loss: 4540.0562 - accuracy: 0.0167 - recall_m: 0.0167 - precision_m: 0.0168\n",
            "57/98 [================>.............] - ETA: 3:00 - loss: 4559.4360 - accuracy: 0.0164 - recall_m: 0.0164 - precision_m: 0.0165\n",
            "58/98 [================>.............] - ETA: 2:55 - loss: 4579.9097 - accuracy: 0.0162 - recall_m: 0.0162 - precision_m: 0.0162\n",
            "59/98 [=================>............] - ETA: 2:50 - loss: 4590.6943 - accuracy: 0.0159 - recall_m: 0.0159 - precision_m: 0.0159\n",
            "60/98 [=================>............] - ETA: 2:45 - loss: 4602.8252 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "61/98 [=================>............] - ETA: 2:41 - loss: 4640.6489 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "62/98 [=================>............] - ETA: 2:36 - loss: 4682.1704 - accuracy: 0.0151 - recall_m: 0.0151 - precision_m: 0.0152\n",
            "63/98 [==================>...........] - ETA: 2:33 - loss: 4705.3848 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "64/98 [==================>...........] - ETA: 2:28 - loss: 4709.4814 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "65/98 [==================>...........] - ETA: 2:24 - loss: 4727.5151 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "66/98 [===================>..........] - ETA: 2:20 - loss: 4749.8105 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "67/98 [===================>..........] - ETA: 2:16 - loss: 4772.0039 - accuracy: 0.0159 - recall_m: 0.0159 - precision_m: 0.0159\n",
            "68/98 [===================>..........] - ETA: 2:11 - loss: 4794.8716 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "69/98 [====================>.........] - ETA: 2:07 - loss: 4814.5649 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "70/98 [====================>.........] - ETA: 2:03 - loss: 4811.1431 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "71/98 [====================>.........] - ETA: 1:58 - loss: 4801.0469 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "72/98 [=====================>........] - ETA: 1:55 - loss: 4791.2114 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "73/98 [=====================>........] - ETA: 1:50 - loss: 4776.9121 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "74/98 [=====================>........] - ETA: 1:45 - loss: 4753.0942 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0157\n",
            "75/98 [=====================>........] - ETA: 1:41 - loss: 4728.0317 - accuracy: 0.0154 - recall_m: 0.0154 - precision_m: 0.0154\n",
            "76/98 [======================>.......] - ETA: 1:37 - loss: 4714.1289 - accuracy: 0.0152 - recall_m: 0.0152 - precision_m: 0.0152\n",
            "77/98 [======================>.......] - ETA: 1:33 - loss: 4695.1978 - accuracy: 0.0150 - recall_m: 0.0150 - precision_m: 0.0150\n",
            "78/98 [======================>.......] - ETA: 1:28 - loss: 4661.6289 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0148\n",
            "79/98 [=======================>......] - ETA: 1:24 - loss: 4639.3315 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0147\n",
            "80/98 [=======================>......] - ETA: 1:20 - loss: 4610.4302 - accuracy: 0.0145 - recall_m: 0.0145 - precision_m: 0.0145\n",
            "81/98 [=======================>......] - ETA: 1:15 - loss: 4584.0757 - accuracy: 0.0147 - recall_m: 0.0147 - precision_m: 0.0147\n",
            "82/98 [========================>.....] - ETA: 1:10 - loss: 4559.6240 - accuracy: 0.0145 - recall_m: 0.0145 - precision_m: 0.0145\n",
            "83/98 [========================>.....] - ETA: 1:06 - loss: 4540.5430 - accuracy: 0.0143 - recall_m: 0.0143 - precision_m: 0.0143\n",
            "84/98 [========================>.....] - ETA: 1:01 - loss: 4515.0122 - accuracy: 0.0141 - recall_m: 0.0141 - precision_m: 0.0142\n",
            "85/98 [=========================>....] - ETA: 57s - loss: 4491.3926 - accuracy: 0.0147 - recall_m: 0.0147 - precision_m: 0.0147 \n",
            "86/98 [=========================>....] - ETA: 53s - loss: 4465.4492 - accuracy: 0.0145 - recall_m: 0.0145 - precision_m: 0.0146\n",
            "87/98 [=========================>....] - ETA: 48s - loss: 4433.6353 - accuracy: 0.0144 - recall_m: 0.0144 - precision_m: 0.0144\n",
            "88/98 [=========================>....] - ETA: 44s - loss: 4409.2612 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0146\n",
            "89/98 [==========================>...] - ETA: 39s - loss: 4380.5522 - accuracy: 0.0147 - recall_m: 0.0147 - precision_m: 0.0148\n",
            "90/98 [==========================>...] - ETA: 35s - loss: 4366.1636 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0150\n",
            "91/98 [==========================>...] - ETA: 30s - loss: 4348.5840 - accuracy: 0.0151 - recall_m: 0.0151 - precision_m: 0.0151\n",
            "92/98 [===========================>..] - ETA: 26s - loss: 4328.9814 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0150\n",
            "93/98 [===========================>..] - ETA: 22s - loss: 4312.4834 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0148\n",
            "94/98 [===========================>..] - ETA: 17s - loss: 4310.1055 - accuracy: 0.0150 - recall_m: 0.0150 - precision_m: 0.0150\n",
            "95/98 [============================>.] - ETA: 13s - loss: 4293.5752 - accuracy: 0.0148 - recall_m: 0.0148 - precision_m: 0.0148\n",
            "96/98 [============================>.] - ETA: 8s - loss: 4289.1616 - accuracy: 0.0146 - recall_m: 0.0146 - precision_m: 0.0147 \n",
            "97/98 [============================>.] - ETA: 4s - loss: 4280.1997 - accuracy: 0.0145 - recall_m: 0.0145 - precision_m: 0.0145\n",
            "98/98 [==============================] - ETA: 0s - loss: 4279.7168 - accuracy: 0.0144 - recall_m: 0.0143 - precision_m: 0.0144\n",
            "98/98 [==============================] - 453s 5s/step - loss: 4279.7168 - accuracy: 0.0144 - recall_m: 0.0143 - precision_m: 0.0144 - val_loss: 3061.8074 - val_accuracy: 0.0128 - val_recall_m: 0.0125 - val_precision_m: 0.0125\n",
            "\n",
            " 1/25 [>.............................] - ETA: 15s - loss: 4136.4072 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 13s - loss: 3886.8818 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 13s - loss: 4019.1482 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 13s - loss: 3683.8032 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 13s - loss: 3343.9253 - accuracy: 0.0625 - recall_m: 0.0625 - precision_m: 0.0625            \n",
            " 6/25 [======>.......................] - ETA: 12s - loss: 3170.6882 - accuracy: 0.0521 - recall_m: 0.0521 - precision_m: 0.0521\n",
            " 7/25 [=======>......................] - ETA: 11s - loss: 3073.3970 - accuracy: 0.0446 - recall_m: 0.0446 - precision_m: 0.0446\n",
            " 8/25 [========>.....................] - ETA: 10s - loss: 3070.5945 - accuracy: 0.0391 - recall_m: 0.0391 - precision_m: 0.0391\n",
            " 9/25 [=========>....................] - ETA: 10s - loss: 3113.2874 - accuracy: 0.0347 - recall_m: 0.0347 - precision_m: 0.0347\n",
            "10/25 [===========>..................] - ETA: 9s - loss: 3115.0017 - accuracy: 0.0312 - recall_m: 0.0312 - precision_m: 0.0312 \n",
            "11/25 [============>.................] - ETA: 8s - loss: 3061.0540 - accuracy: 0.0284 - recall_m: 0.0284 - precision_m: 0.0284\n",
            "12/25 [=============>................] - ETA: 8s - loss: 3063.0271 - accuracy: 0.0260 - recall_m: 0.0260 - precision_m: 0.0260\n",
            "13/25 [==============>...............] - ETA: 7s - loss: 3070.4448 - accuracy: 0.0240 - recall_m: 0.0240 - precision_m: 0.0240\n",
            "14/25 [===============>..............] - ETA: 6s - loss: 3040.0515 - accuracy: 0.0223 - recall_m: 0.0223 - precision_m: 0.0223\n",
            "15/25 [=================>............] - ETA: 6s - loss: 3026.0786 - accuracy: 0.0208 - recall_m: 0.0208 - precision_m: 0.0208\n",
            "16/25 [==================>...........] - ETA: 5s - loss: 2954.5400 - accuracy: 0.0195 - recall_m: 0.0195 - precision_m: 0.0195\n",
            "17/25 [===================>..........] - ETA: 5s - loss: 3046.4851 - accuracy: 0.0184 - recall_m: 0.0184 - precision_m: 0.0184\n",
            "18/25 [====================>.........] - ETA: 4s - loss: 3051.9136 - accuracy: 0.0174 - recall_m: 0.0174 - precision_m: 0.0174\n",
            "19/25 [=====================>........] - ETA: 3s - loss: 3129.4324 - accuracy: 0.0164 - recall_m: 0.0164 - precision_m: 0.0164\n",
            "20/25 [=======================>......] - ETA: 3s - loss: 3129.8296 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0156\n",
            "21/25 [========================>.....] - ETA: 2s - loss: 3108.7156 - accuracy: 0.0149 - recall_m: 0.0149 - precision_m: 0.0149\n",
            "22/25 [=========================>....] - ETA: 1s - loss: 3113.3538 - accuracy: 0.0142 - recall_m: 0.0142 - precision_m: 0.0142\n",
            "23/25 [==========================>...] - ETA: 1s - loss: 3118.6824 - accuracy: 0.0136 - recall_m: 0.0136 - precision_m: 0.0136\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 3052.9783 - accuracy: 0.0130 - recall_m: 0.0130 - precision_m: 0.0130\n",
            "25/25 [==============================] - ETA: 0s - loss: 3061.8074 - accuracy: 0.0128 - recall_m: 0.0125 - precision_m: 0.0125\n",
            "25/25 [==============================] - 16s 630ms/step - loss: 3061.8074 - accuracy: 0.0128 - recall_m: 0.0125 - precision_m: 0.0125\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: 0.012500000186264515                                             \n",
            "validation precision: 0.012500000186264515                                          \n",
            " 1/98 [..............................] - ETA: 3:49 - loss: 153.1929 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 2:02 - loss: 14603234304.0000 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 3:57 - loss: 492974247390327877926912.0000 - accuracy: 0.0208 - recall_m: 0.0208 - precision_m: 0.0208\n",
            " 4/98 [>.............................] - ETA: 4:04 - loss: 25232684232746049067213869232947200.0000 - accuracy: 0.0156 - recall_m: 0.0156 - precision_m: 0.0156\n",
            " 5/98 [>.............................] - ETA: 3:29 - loss: nan - accuracy: 0.0188 - recall_m: nan - precision_m: nan                                           \n",
            " 6/98 [>.............................] - ETA: 3:12 - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan\n",
            " 7/98 [=>............................] - ETA: 3:39 - loss: nan - accuracy: 0.0179 - recall_m: nan - precision_m: nan\n",
            " 8/98 [=>............................] - ETA: 3:23 - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan\n",
            " 9/98 [=>............................] - ETA: 3:10 - loss: nan - accuracy: 0.0139 - recall_m: nan - precision_m: nan\n",
            "10/98 [==>...........................] - ETA: 2:59 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "11/98 [==>...........................] - ETA: 3:15 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "12/98 [==>...........................] - ETA: 3:05 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "13/98 [==>...........................] - ETA: 2:56 - loss: nan - accuracy: 0.0096 - recall_m: nan - precision_m: nan\n",
            "14/98 [===>..........................] - ETA: 2:49 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "15/98 [===>..........................] - ETA: 2:42 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "16/98 [===>..........................] - ETA: 2:49 - loss: nan - accuracy: 0.0098 - recall_m: nan - precision_m: nan\n",
            "17/98 [====>.........................] - ETA: 2:43 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "18/98 [====>.........................] - ETA: 2:38 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "19/98 [====>.........................] - ETA: 2:33 - loss: nan - accuracy: 0.0099 - recall_m: nan - precision_m: nan\n",
            "20/98 [=====>........................] - ETA: 2:28 - loss: nan - accuracy: 0.0094 - recall_m: nan - precision_m: nan\n",
            "21/98 [=====>........................] - ETA: 2:34 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "22/98 [=====>........................] - ETA: 2:30 - loss: nan - accuracy: 0.0085 - recall_m: nan - precision_m: nan\n",
            "23/98 [======>.......................] - ETA: 2:25 - loss: nan - accuracy: 0.0095 - recall_m: nan - precision_m: nan\n",
            "24/98 [======>.......................] - ETA: 2:22 - loss: nan - accuracy: 0.0091 - recall_m: nan - precision_m: nan\n",
            "25/98 [======>.......................] - ETA: 2:18 - loss: nan - accuracy: 0.0088 - recall_m: nan - precision_m: nan\n",
            "26/98 [======>.......................] - ETA: 2:14 - loss: nan - accuracy: 0.0084 - recall_m: nan - precision_m: nan\n",
            "27/98 [=======>......................] - ETA: 2:10 - loss: nan - accuracy: 0.0081 - recall_m: nan - precision_m: nan\n",
            "28/98 [=======>......................] - ETA: 2:07 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "29/98 [=======>......................] - ETA: 2:04 - loss: nan - accuracy: 0.0097 - recall_m: nan - precision_m: nan\n",
            "30/98 [========>.....................] - ETA: 2:01 - loss: nan - accuracy: 0.0094 - recall_m: nan - precision_m: nan\n",
            "31/98 [========>.....................] - ETA: 1:58 - loss: nan - accuracy: 0.0091 - recall_m: nan - precision_m: nan\n",
            "32/98 [========>.....................] - ETA: 1:55 - loss: nan - accuracy: 0.0088 - recall_m: nan - precision_m: nan\n",
            "33/98 [=========>....................] - ETA: 1:52 - loss: nan - accuracy: 0.0095 - recall_m: nan - precision_m: nan\n",
            "34/98 [=========>....................] - ETA: 1:50 - loss: nan - accuracy: 0.0092 - recall_m: nan - precision_m: nan\n",
            "35/98 [=========>....................] - ETA: 1:52 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "36/98 [==========>...................] - ETA: 1:50 - loss: nan - accuracy: 0.0087 - recall_m: nan - precision_m: nan\n",
            "37/98 [==========>...................] - ETA: 1:47 - loss: nan - accuracy: 0.0093 - recall_m: nan - precision_m: nan\n",
            "38/98 [==========>...................] - ETA: 1:44 - loss: nan - accuracy: 0.0090 - recall_m: nan - precision_m: nan\n",
            "39/98 [==========>...................] - ETA: 1:42 - loss: nan - accuracy: 0.0088 - recall_m: nan - precision_m: nan\n",
            "40/98 [===========>..................] - ETA: 1:44 - loss: nan - accuracy: 0.0086 - recall_m: nan - precision_m: nan\n",
            "41/98 [===========>..................] - ETA: 1:41 - loss: nan - accuracy: 0.0099 - recall_m: nan - precision_m: nan\n",
            "42/98 [===========>..................] - ETA: 1:38 - loss: nan - accuracy: 0.0097 - recall_m: nan - precision_m: nan\n",
            "43/98 [============>.................] - ETA: 1:36 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "44/98 [============>.................] - ETA: 1:34 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "45/98 [============>.................] - ETA: 1:35 - loss: nan - accuracy: 0.0111 - recall_m: nan - precision_m: nan\n",
            "46/98 [=============>................] - ETA: 1:33 - loss: nan - accuracy: 0.0115 - recall_m: nan - precision_m: nan\n",
            "47/98 [=============>................] - ETA: 1:30 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "48/98 [=============>................] - ETA: 1:28 - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "49/98 [==============>...............] - ETA: 1:26 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "50/98 [==============>...............] - ETA: 1:24 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "51/98 [==============>...............] - ETA: 1:22 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "52/98 [==============>...............] - ETA: 1:20 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "53/98 [===============>..............] - ETA: 1:18 - loss: nan - accuracy: 0.0118 - recall_m: nan - precision_m: nan\n",
            "54/98 [===============>..............] - ETA: 1:16 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "55/98 [===============>..............] - ETA: 1:14 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "56/98 [================>.............] - ETA: 1:12 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "57/98 [================>.............] - ETA: 1:10 - loss: nan - accuracy: 0.0115 - recall_m: nan - precision_m: nan\n",
            "58/98 [================>.............] - ETA: 1:08 - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "59/98 [=================>............] - ETA: 1:06 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "60/98 [=================>............] - ETA: 1:04 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "61/98 [=================>............] - ETA: 1:03 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "62/98 [=================>............] - ETA: 1:01 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "63/98 [==================>...........] - ETA: 59s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan \n",
            "64/98 [==================>...........] - ETA: 57s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "65/98 [==================>...........] - ETA: 55s - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "66/98 [===================>..........] - ETA: 53s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "67/98 [===================>..........] - ETA: 51s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "68/98 [===================>..........] - ETA: 50s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "69/98 [====================>.........] - ETA: 48s - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "70/98 [====================>.........] - ETA: 46s - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "71/98 [====================>.........] - ETA: 44s - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "72/98 [=====================>........] - ETA: 43s - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "73/98 [=====================>........] - ETA: 41s - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "74/98 [=====================>........] - ETA: 39s - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "75/98 [=====================>........] - ETA: 38s - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "76/98 [======================>.......] - ETA: 36s - loss: nan - accuracy: 0.0115 - recall_m: nan - precision_m: nan\n",
            "77/98 [======================>.......] - ETA: 34s - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "78/98 [======================>.......] - ETA: 32s - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "79/98 [=======================>......] - ETA: 31s - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "80/98 [=======================>......] - ETA: 29s - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "81/98 [=======================>......] - ETA: 27s - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "82/98 [========================>.....] - ETA: 26s - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "83/98 [========================>.....] - ETA: 24s - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "84/98 [========================>.....] - ETA: 22s - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "85/98 [=========================>....] - ETA: 21s - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "86/98 [=========================>....] - ETA: 19s - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "87/98 [=========================>....] - ETA: 17s - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "88/98 [=========================>....] - ETA: 16s - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "89/98 [==========================>...] - ETA: 14s - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "90/98 [==========================>...] - ETA: 12s - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "91/98 [==========================>...] - ETA: 11s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "92/98 [===========================>..] - ETA: 9s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan \n",
            "93/98 [===========================>..] - ETA: 8s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "94/98 [===========================>..] - ETA: 6s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "95/98 [============================>.] - ETA: 4s - loss: nan - accuracy: 0.0135 - recall_m: nan - precision_m: nan\n",
            "96/98 [============================>.] - ETA: 3s - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "97/98 [============================>.] - ETA: 1s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - 165s 2s/step - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan - val_loss: nan - val_accuracy: 0.0128 - val_recall_m: nan - val_precision_m: nan\n",
            "\n",
            " 1/25 [>.............................] - ETA: 7s - loss: nan - accuracy: 0.3125 - recall_m: nan - precision_m: nan\n",
            " 2/25 [=>............................] - ETA: 7s - loss: nan - accuracy: 0.1562 - recall_m: nan - precision_m: nan\n",
            " 3/25 [==>...........................] - ETA: 6s - loss: nan - accuracy: 0.1042 - recall_m: nan - precision_m: nan\n",
            " 4/25 [===>..........................] - ETA: 6s - loss: nan - accuracy: 0.0781 - recall_m: nan - precision_m: nan\n",
            " 5/25 [=====>........................] - ETA: 6s - loss: nan - accuracy: 0.0625 - recall_m: nan - precision_m: nan\n",
            " 6/25 [======>.......................] - ETA: 5s - loss: nan - accuracy: 0.0521 - recall_m: nan - precision_m: nan\n",
            " 7/25 [=======>......................] - ETA: 5s - loss: nan - accuracy: 0.0446 - recall_m: nan - precision_m: nan\n",
            " 8/25 [========>.....................] - ETA: 5s - loss: nan - accuracy: 0.0391 - recall_m: nan - precision_m: nan\n",
            " 9/25 [=========>....................] - ETA: 4s - loss: nan - accuracy: 0.0347 - recall_m: nan - precision_m: nan\n",
            "10/25 [===========>..................] - ETA: 4s - loss: nan - accuracy: 0.0312 - recall_m: nan - precision_m: nan\n",
            "11/25 [============>.................] - ETA: 4s - loss: nan - accuracy: 0.0284 - recall_m: nan - precision_m: nan\n",
            "12/25 [=============>................] - ETA: 3s - loss: nan - accuracy: 0.0260 - recall_m: nan - precision_m: nan\n",
            "13/25 [==============>...............] - ETA: 3s - loss: nan - accuracy: 0.0240 - recall_m: nan - precision_m: nan\n",
            "14/25 [===============>..............] - ETA: 3s - loss: nan - accuracy: 0.0223 - recall_m: nan - precision_m: nan\n",
            "15/25 [=================>............] - ETA: 3s - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan\n",
            "16/25 [==================>...........] - ETA: 2s - loss: nan - accuracy: 0.0195 - recall_m: nan - precision_m: nan\n",
            "17/25 [===================>..........] - ETA: 2s - loss: nan - accuracy: 0.0184 - recall_m: nan - precision_m: nan\n",
            "18/25 [====================>.........] - ETA: 2s - loss: nan - accuracy: 0.0174 - recall_m: nan - precision_m: nan\n",
            "19/25 [=====================>........] - ETA: 1s - loss: nan - accuracy: 0.0164 - recall_m: nan - precision_m: nan\n",
            "20/25 [=======================>......] - ETA: 1s - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan\n",
            "21/25 [========================>.....] - ETA: 1s - loss: nan - accuracy: 0.0149 - recall_m: nan - precision_m: nan\n",
            "22/25 [=========================>....] - ETA: 0s - loss: nan - accuracy: 0.0142 - recall_m: nan - precision_m: nan\n",
            "23/25 [==========================>...] - ETA: 0s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "24/25 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - 7s 298ms/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: nan                                                              \n",
            "validation precision: nan                                                           \n",
            " 1/98 [..............................] - ETA: 17:25 - loss: 4.3566 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 7:13 - loss: 4.3305 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            " 3/98 [..............................] - ETA: 6:40 - loss: 4.4171 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/98 [>.............................] - ETA: 6:24 - loss: 4.4335 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/98 [>.............................] - ETA: 6:47 - loss: 4.4586 - accuracy: 0.0063 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            " 6/98 [>.............................] - ETA: 6:27 - loss: 4.5709 - accuracy: 0.0052 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/98 [=>............................] - ETA: 7:14 - loss: 4.5984 - accuracy: 0.0045 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/98 [=>............................] - ETA: 7:01 - loss: 4.6861 - accuracy: 0.0078 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/98 [=>............................] - ETA: 6:49 - loss: 4.6759 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/98 [==>...........................] - ETA: 6:37 - loss: 4.7032 - accuracy: 0.0094 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/98 [==>...........................] - ETA: 6:26 - loss: 4.7107 - accuracy: 0.0085 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/98 [==>...........................] - ETA: 6:17 - loss: 4.7052 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/98 [==>...........................] - ETA: 6:08 - loss: 4.7155 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/98 [===>..........................] - ETA: 6:00 - loss: 4.7032 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/98 [===>..........................] - ETA: 5:50 - loss: 4.6924 - accuracy: 0.0167 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/98 [===>..........................] - ETA: 5:44 - loss: 4.6952 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/98 [====>.........................] - ETA: 5:37 - loss: 4.6880 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/98 [====>.........................] - ETA: 5:31 - loss: 4.7004 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/98 [====>.........................] - ETA: 5:25 - loss: 4.6959 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/98 [=====>........................] - ETA: 5:19 - loss: 4.6889 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/98 [=====>........................] - ETA: 5:13 - loss: 4.6875 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/98 [=====>........................] - ETA: 5:07 - loss: 4.6891 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/98 [======>.......................] - ETA: 5:16 - loss: 4.6908 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/98 [======>.......................] - ETA: 5:13 - loss: 4.6918 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/98 [======>.......................] - ETA: 5:08 - loss: 4.6943 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "26/98 [======>.......................] - ETA: 5:02 - loss: 4.6931 - accuracy: 0.0108 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "27/98 [=======>......................] - ETA: 4:59 - loss: 4.6832 - accuracy: 0.0116 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "28/98 [=======>......................] - ETA: 4:55 - loss: 4.6737 - accuracy: 0.0112 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "29/98 [=======>......................] - ETA: 4:50 - loss: 4.6740 - accuracy: 0.0108 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "30/98 [========>.....................] - ETA: 4:45 - loss: 4.6687 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "31/98 [========>.....................] - ETA: 4:40 - loss: 4.6662 - accuracy: 0.0131 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "32/98 [========>.....................] - ETA: 4:36 - loss: 4.6652 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "33/98 [=========>....................] - ETA: 4:33 - loss: 4.6629 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "34/98 [=========>....................] - ETA: 4:36 - loss: 4.6587 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "35/98 [=========>....................] - ETA: 4:31 - loss: 4.6632 - accuracy: 0.0116 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "36/98 [==========>...................] - ETA: 4:27 - loss: 4.6636 - accuracy: 0.0113 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "37/98 [==========>...................] - ETA: 4:23 - loss: 4.6583 - accuracy: 0.0110 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "38/98 [==========>...................] - ETA: 4:24 - loss: 4.6558 - accuracy: 0.0115 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "39/98 [==========>...................] - ETA: 4:19 - loss: 4.6532 - accuracy: 0.0112 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "40/98 [===========>..................] - ETA: 4:13 - loss: 4.6461 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "41/98 [===========>..................] - ETA: 4:12 - loss: 4.6424 - accuracy: 0.0122 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "42/98 [===========>..................] - ETA: 4:07 - loss: 4.6414 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "43/98 [============>.................] - ETA: 4:06 - loss: 4.6386 - accuracy: 0.0116 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "44/98 [============>.................] - ETA: 4:00 - loss: 4.6371 - accuracy: 0.0114 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "45/98 [============>.................] - ETA: 3:55 - loss: 4.6322 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "46/98 [=============>................] - ETA: 3:50 - loss: 4.6320 - accuracy: 0.0129 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "47/98 [=============>................] - ETA: 3:44 - loss: 4.6273 - accuracy: 0.0133 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "48/98 [=============>................] - ETA: 3:40 - loss: 4.6268 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "49/98 [==============>...............] - ETA: 3:38 - loss: 4.6227 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "50/98 [==============>...............] - ETA: 3:34 - loss: 4.6184 - accuracy: 0.0131 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "51/98 [==============>...............] - ETA: 3:29 - loss: 4.6237 - accuracy: 0.0129 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "52/98 [==============>...............] - ETA: 3:24 - loss: 4.6215 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "53/98 [===============>..............] - ETA: 3:19 - loss: 4.6191 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "54/98 [===============>..............] - ETA: 3:14 - loss: 4.6173 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "55/98 [===============>..............] - ETA: 3:09 - loss: 4.6164 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "56/98 [================>.............] - ETA: 3:04 - loss: 4.6162 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "57/98 [================>.............] - ETA: 3:00 - loss: 4.6161 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "58/98 [================>.............] - ETA: 2:55 - loss: 4.6128 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "59/98 [=================>............] - ETA: 2:53 - loss: 4.6122 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "60/98 [=================>............] - ETA: 2:48 - loss: 4.6091 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "61/98 [=================>............] - ETA: 2:43 - loss: 4.6104 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "62/98 [=================>............] - ETA: 2:40 - loss: 4.6108 - accuracy: 0.0126 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "63/98 [==================>...........] - ETA: 2:35 - loss: 4.6094 - accuracy: 0.0124 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "64/98 [==================>...........] - ETA: 2:30 - loss: 4.6053 - accuracy: 0.0127 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "65/98 [==================>...........] - ETA: 2:26 - loss: 4.6078 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "66/98 [===================>..........] - ETA: 2:23 - loss: 4.6040 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "67/98 [===================>..........] - ETA: 2:18 - loss: 4.6023 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "68/98 [===================>..........] - ETA: 2:13 - loss: 4.6092 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "69/98 [====================>.........] - ETA: 2:08 - loss: 4.6205 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "70/98 [====================>.........] - ETA: 2:03 - loss: 4.6249 - accuracy: 0.0143 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "71/98 [====================>.........] - ETA: 2:00 - loss: 4.6288 - accuracy: 0.0141 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "72/98 [=====================>........] - ETA: 1:56 - loss: 4.6331 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "73/98 [=====================>........] - ETA: 1:52 - loss: 4.6330 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "74/98 [=====================>........] - ETA: 1:47 - loss: 4.6331 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "75/98 [=====================>........] - ETA: 1:42 - loss: 4.6378 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "76/98 [======================>.......] - ETA: 1:38 - loss: 4.6420 - accuracy: 0.0144 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "77/98 [======================>.......] - ETA: 1:33 - loss: 4.6424 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "78/98 [======================>.......] - ETA: 1:28 - loss: 4.6410 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "79/98 [=======================>......] - ETA: 1:24 - loss: 4.6413 - accuracy: 0.0150 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "80/98 [=======================>......] - ETA: 1:19 - loss: 4.6398 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "81/98 [=======================>......] - ETA: 1:15 - loss: 4.6434 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "82/98 [========================>.....] - ETA: 1:10 - loss: 4.6474 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "83/98 [========================>.....] - ETA: 1:06 - loss: 4.6502 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "84/98 [========================>.....] - ETA: 1:01 - loss: 4.6550 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "85/98 [=========================>....] - ETA: 57s - loss: 4.6539 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "86/98 [=========================>....] - ETA: 52s - loss: 4.6504 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "87/98 [=========================>....] - ETA: 48s - loss: 4.6513 - accuracy: 0.0154 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "88/98 [=========================>....] - ETA: 43s - loss: 4.6513 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "89/98 [==========================>...] - ETA: 39s - loss: 4.6532 - accuracy: 0.0154 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "90/98 [==========================>...] - ETA: 34s - loss: 4.6583 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "91/98 [==========================>...] - ETA: 30s - loss: 4.6571 - accuracy: 0.0155 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "92/98 [===========================>..] - ETA: 26s - loss: 4.6609 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "93/98 [===========================>..] - ETA: 21s - loss: 4.6621 - accuracy: 0.0155 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "94/98 [===========================>..] - ETA: 17s - loss: 4.6633 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "95/98 [============================>.] - ETA: 13s - loss: 4.6637 - accuracy: 0.0158 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "96/98 [============================>.] - ETA: 8s - loss: 4.6622 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "97/98 [============================>.] - ETA: 4s - loss: 4.6627 - accuracy: 0.0155 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - ETA: 0s - loss: 4.6623 - accuracy: 0.0154 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - 452s 5s/step - loss: 4.6623 - accuracy: 0.0154 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 - val_loss: 4.5924 - val_accuracy: 0.0128 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
            "\n",
            " 1/25 [>.............................] - ETA: 19s - loss: 4.0101 - accuracy: 0.3125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 18s - loss: 4.4988 - accuracy: 0.1562 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 17s - loss: 4.3893 - accuracy: 0.1042 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 16s - loss: 4.2602 - accuracy: 0.0781 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 16s - loss: 4.2527 - accuracy: 0.0625 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 15s - loss: 4.2381 - accuracy: 0.0521 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 14s - loss: 4.3702 - accuracy: 0.0446 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 13s - loss: 4.4830 - accuracy: 0.0391 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/25 [=========>....................] - ETA: 13s - loss: 4.6023 - accuracy: 0.0347 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/25 [===========>..................] - ETA: 12s - loss: 4.6789 - accuracy: 0.0312 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/25 [============>.................] - ETA: 11s - loss: 4.7203 - accuracy: 0.0284 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/25 [=============>................] - ETA: 10s - loss: 4.7177 - accuracy: 0.0260 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/25 [==============>...............] - ETA: 9s - loss: 4.6825 - accuracy: 0.0240 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "14/25 [===============>..............] - ETA: 9s - loss: 4.6759 - accuracy: 0.0223 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/25 [=================>............] - ETA: 8s - loss: 4.6537 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/25 [==================>...........] - ETA: 7s - loss: 4.6549 - accuracy: 0.0195 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/25 [===================>..........] - ETA: 6s - loss: 4.6318 - accuracy: 0.0184 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/25 [====================>.........] - ETA: 5s - loss: 4.6453 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/25 [=====================>........] - ETA: 4s - loss: 4.6613 - accuracy: 0.0164 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/25 [=======================>......] - ETA: 4s - loss: 4.6329 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/25 [========================>.....] - ETA: 3s - loss: 4.6070 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/25 [=========================>....] - ETA: 2s - loss: 4.5922 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/25 [==========================>...] - ETA: 1s - loss: 4.6068 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.5909 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.5924 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - 20s 801ms/step - loss: 4.5924 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: 0.0                                                              \n",
            "validation precision: 0.0                                                           \n",
            " 1/98 [..............................] - ETA: 2:33 - loss: 85.6311 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 2:00 - loss: 252122300416.0000 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 1:55 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan                                \n",
            " 4/98 [>.............................] - ETA: 1:54 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            " 5/98 [>.............................] - ETA: 1:50 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            " 6/98 [>.............................] - ETA: 1:48 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            " 7/98 [=>............................] - ETA: 1:48 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            " 8/98 [=>............................] - ETA: 1:45 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            " 9/98 [=>............................] - ETA: 1:47 - loss: nan - accuracy: 0.0069 - recall_m: nan - precision_m: nan\n",
            "10/98 [==>...........................] - ETA: 1:45 - loss: nan - accuracy: 0.0063 - recall_m: nan - precision_m: nan\n",
            "11/98 [==>...........................] - ETA: 1:41 - loss: nan - accuracy: 0.0057 - recall_m: nan - precision_m: nan\n",
            "12/98 [==>...........................] - ETA: 1:40 - loss: nan - accuracy: 0.0052 - recall_m: nan - precision_m: nan\n",
            "13/98 [==>...........................] - ETA: 1:37 - loss: nan - accuracy: 0.0048 - recall_m: nan - precision_m: nan\n",
            "14/98 [===>..........................] - ETA: 1:35 - loss: nan - accuracy: 0.0045 - recall_m: nan - precision_m: nan\n",
            "15/98 [===>..........................] - ETA: 1:33 - loss: nan - accuracy: 0.0083 - recall_m: nan - precision_m: nan\n",
            "16/98 [===>..........................] - ETA: 1:32 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            "17/98 [====>.........................] - ETA: 1:30 - loss: nan - accuracy: 0.0074 - recall_m: nan - precision_m: nan\n",
            "18/98 [====>.........................] - ETA: 1:30 - loss: nan - accuracy: 0.0069 - recall_m: nan - precision_m: nan\n",
            "19/98 [====>.........................] - ETA: 1:28 - loss: nan - accuracy: 0.0066 - recall_m: nan - precision_m: nan\n",
            "20/98 [=====>........................] - ETA: 1:26 - loss: nan - accuracy: 0.0063 - recall_m: nan - precision_m: nan\n",
            "21/98 [=====>........................] - ETA: 1:25 - loss: nan - accuracy: 0.0074 - recall_m: nan - precision_m: nan\n",
            "22/98 [=====>........................] - ETA: 1:24 - loss: nan - accuracy: 0.0071 - recall_m: nan - precision_m: nan\n",
            "23/98 [======>.......................] - ETA: 1:23 - loss: nan - accuracy: 0.0068 - recall_m: nan - precision_m: nan\n",
            "24/98 [======>.......................] - ETA: 1:22 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            "25/98 [======>.......................] - ETA: 1:20 - loss: nan - accuracy: 0.0088 - recall_m: nan - precision_m: nan\n",
            "26/98 [======>.......................] - ETA: 1:18 - loss: nan - accuracy: 0.0096 - recall_m: nan - precision_m: nan\n",
            "27/98 [=======>......................] - ETA: 1:17 - loss: nan - accuracy: 0.0093 - recall_m: nan - precision_m: nan\n",
            "28/98 [=======>......................] - ETA: 1:16 - loss: nan - accuracy: 0.0100 - recall_m: nan - precision_m: nan\n",
            "29/98 [=======>......................] - ETA: 1:15 - loss: nan - accuracy: 0.0097 - recall_m: nan - precision_m: nan\n",
            "30/98 [========>.....................] - ETA: 1:14 - loss: nan - accuracy: 0.0115 - recall_m: nan - precision_m: nan\n",
            "31/98 [========>.....................] - ETA: 1:12 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "32/98 [========>.....................] - ETA: 1:11 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "33/98 [=========>....................] - ETA: 1:09 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "34/98 [=========>....................] - ETA: 1:08 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "35/98 [=========>....................] - ETA: 1:07 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "36/98 [==========>...................] - ETA: 1:06 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "37/98 [==========>...................] - ETA: 1:04 - loss: nan - accuracy: 0.0118 - recall_m: nan - precision_m: nan\n",
            "38/98 [==========>...................] - ETA: 1:03 - loss: nan - accuracy: 0.0115 - recall_m: nan - precision_m: nan\n",
            "39/98 [==========>...................] - ETA: 1:02 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "40/98 [===========>..................] - ETA: 1:01 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "41/98 [===========>..................] - ETA: 1:00 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "42/98 [===========>..................] - ETA: 59s - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan \n",
            "43/98 [============>.................] - ETA: 57s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "44/98 [============>.................] - ETA: 56s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "45/98 [============>.................] - ETA: 55s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "46/98 [=============>................] - ETA: 54s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "47/98 [=============>................] - ETA: 53s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "48/98 [=============>................] - ETA: 52s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "49/98 [==============>...............] - ETA: 51s - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "50/98 [==============>...............] - ETA: 50s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "51/98 [==============>...............] - ETA: 48s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "52/98 [==============>...............] - ETA: 47s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "53/98 [===============>..............] - ETA: 46s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "54/98 [===============>..............] - ETA: 46s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "55/98 [===============>..............] - ETA: 44s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "56/98 [================>.............] - ETA: 43s - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "57/98 [================>.............] - ETA: 42s - loss: nan - accuracy: 0.0137 - recall_m: nan - precision_m: nan\n",
            "58/98 [================>.............] - ETA: 41s - loss: nan - accuracy: 0.0140 - recall_m: nan - precision_m: nan\n",
            "59/98 [=================>............] - ETA: 40s - loss: nan - accuracy: 0.0138 - recall_m: nan - precision_m: nan\n",
            "60/98 [=================>............] - ETA: 39s - loss: nan - accuracy: 0.0135 - recall_m: nan - precision_m: nan\n",
            "61/98 [=================>............] - ETA: 38s - loss: nan - accuracy: 0.0138 - recall_m: nan - precision_m: nan\n",
            "62/98 [=================>............] - ETA: 37s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "63/98 [==================>...........] - ETA: 36s - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "64/98 [==================>...........] - ETA: 35s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "65/98 [==================>...........] - ETA: 34s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "66/98 [===================>..........] - ETA: 33s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "67/98 [===================>..........] - ETA: 32s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "68/98 [===================>..........] - ETA: 31s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "69/98 [====================>.........] - ETA: 30s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "70/98 [====================>.........] - ETA: 28s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "71/98 [====================>.........] - ETA: 27s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "72/98 [=====================>........] - ETA: 26s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "73/98 [=====================>........] - ETA: 25s - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "74/98 [=====================>........] - ETA: 24s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "75/98 [=====================>........] - ETA: 23s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "76/98 [======================>.......] - ETA: 22s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "77/98 [======================>.......] - ETA: 21s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "78/98 [======================>.......] - ETA: 20s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "79/98 [=======================>......] - ETA: 19s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "80/98 [=======================>......] - ETA: 18s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "81/98 [=======================>......] - ETA: 17s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "82/98 [========================>.....] - ETA: 16s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "83/98 [========================>.....] - ETA: 15s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "84/98 [========================>.....] - ETA: 14s - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "85/98 [=========================>....] - ETA: 13s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "86/98 [=========================>....] - ETA: 12s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "87/98 [=========================>....] - ETA: 11s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "88/98 [=========================>....] - ETA: 10s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "89/98 [==========================>...] - ETA: 9s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan \n",
            "90/98 [==========================>...] - ETA: 8s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "91/98 [==========================>...] - ETA: 7s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "92/98 [===========================>..] - ETA: 6s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "93/98 [===========================>..] - ETA: 5s - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "94/98 [===========================>..] - ETA: 4s - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "95/98 [============================>.] - ETA: 3s - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "96/98 [============================>.] - ETA: 2s - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "97/98 [============================>.] - ETA: 1s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - 107s 1s/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan - val_loss: nan - val_accuracy: 0.0128 - val_recall_m: nan - val_precision_m: nan\n",
            "\n",
            " 1/25 [>.............................] - ETA: 6s - loss: nan - accuracy: 0.3125 - recall_m: nan - precision_m: nan\n",
            " 2/25 [=>............................] - ETA: 5s - loss: nan - accuracy: 0.1562 - recall_m: nan - precision_m: nan\n",
            " 3/25 [==>...........................] - ETA: 4s - loss: nan - accuracy: 0.1042 - recall_m: nan - precision_m: nan\n",
            " 4/25 [===>..........................] - ETA: 5s - loss: nan - accuracy: 0.0781 - recall_m: nan - precision_m: nan\n",
            " 5/25 [=====>........................] - ETA: 4s - loss: nan - accuracy: 0.0625 - recall_m: nan - precision_m: nan\n",
            " 6/25 [======>.......................] - ETA: 4s - loss: nan - accuracy: 0.0521 - recall_m: nan - precision_m: nan\n",
            " 7/25 [=======>......................] - ETA: 4s - loss: nan - accuracy: 0.0446 - recall_m: nan - precision_m: nan\n",
            " 8/25 [========>.....................] - ETA: 4s - loss: nan - accuracy: 0.0391 - recall_m: nan - precision_m: nan\n",
            " 9/25 [=========>....................] - ETA: 4s - loss: nan - accuracy: 0.0347 - recall_m: nan - precision_m: nan\n",
            "10/25 [===========>..................] - ETA: 3s - loss: nan - accuracy: 0.0312 - recall_m: nan - precision_m: nan\n",
            "11/25 [============>.................] - ETA: 3s - loss: nan - accuracy: 0.0284 - recall_m: nan - precision_m: nan\n",
            "12/25 [=============>................] - ETA: 3s - loss: nan - accuracy: 0.0260 - recall_m: nan - precision_m: nan\n",
            "13/25 [==============>...............] - ETA: 2s - loss: nan - accuracy: 0.0240 - recall_m: nan - precision_m: nan\n",
            "14/25 [===============>..............] - ETA: 2s - loss: nan - accuracy: 0.0223 - recall_m: nan - precision_m: nan\n",
            "15/25 [=================>............] - ETA: 2s - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan\n",
            "16/25 [==================>...........] - ETA: 2s - loss: nan - accuracy: 0.0195 - recall_m: nan - precision_m: nan\n",
            "17/25 [===================>..........] - ETA: 1s - loss: nan - accuracy: 0.0184 - recall_m: nan - precision_m: nan\n",
            "18/25 [====================>.........] - ETA: 1s - loss: nan - accuracy: 0.0174 - recall_m: nan - precision_m: nan\n",
            "19/25 [=====================>........] - ETA: 1s - loss: nan - accuracy: 0.0164 - recall_m: nan - precision_m: nan\n",
            "20/25 [=======================>......] - ETA: 1s - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan\n",
            "21/25 [========================>.....] - ETA: 0s - loss: nan - accuracy: 0.0149 - recall_m: nan - precision_m: nan\n",
            "22/25 [=========================>....] - ETA: 0s - loss: nan - accuracy: 0.0142 - recall_m: nan - precision_m: nan\n",
            "23/25 [==========================>...] - ETA: 0s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "24/25 [===========================>..] - ETA: 0s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - 6s 232ms/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: nan                                                              \n",
            "validation precision: nan                                                           \n",
            " 1/98 [..............................] - ETA: 39:41 - loss: 28.2838 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 34:18 - loss: 41311595921408.0000 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 32:16 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan                              \n",
            " 4/98 [>.............................] - ETA: 28:00 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 5/98 [>.............................] - ETA: 24:58 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 6/98 [>.............................] - ETA: 24:21 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 7/98 [=>............................] - ETA: 23:11 - loss: nan - accuracy: 0.0045 - recall_m: nan - precision_m: nan    \n",
            " 8/98 [=>............................] - ETA: 23:31 - loss: nan - accuracy: 0.0039 - recall_m: nan - precision_m: nan\n",
            " 9/98 [=>............................] - ETA: 22:35 - loss: nan - accuracy: 0.0035 - recall_m: nan - precision_m: nan\n",
            "10/98 [==>...........................] - ETA: 22:52 - loss: nan - accuracy: 0.0031 - recall_m: nan - precision_m: nan\n",
            "11/98 [==>...........................] - ETA: 22:28 - loss: nan - accuracy: 0.0085 - recall_m: nan - precision_m: nan\n",
            "12/98 [==>...........................] - ETA: 22:03 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            "13/98 [==>...........................] - ETA: 21:34 - loss: nan - accuracy: 0.0072 - recall_m: nan - precision_m: nan\n",
            "14/98 [===>..........................] - ETA: 21:11 - loss: nan - accuracy: 0.0067 - recall_m: nan - precision_m: nan\n",
            "15/98 [===>..........................] - ETA: 20:58 - loss: nan - accuracy: 0.0063 - recall_m: nan - precision_m: nan\n",
            "16/98 [===>..........................] - ETA: 20:34 - loss: nan - accuracy: 0.0059 - recall_m: nan - precision_m: nan\n",
            "17/98 [====>.........................] - ETA: 20:37 - loss: nan - accuracy: 0.0055 - recall_m: nan - precision_m: nan\n",
            "18/98 [====>.........................] - ETA: 20:14 - loss: nan - accuracy: 0.0052 - recall_m: nan - precision_m: nan\n",
            "19/98 [====>.........................] - ETA: 20:07 - loss: nan - accuracy: 0.0049 - recall_m: nan - precision_m: nan\n",
            "20/98 [=====>........................] - ETA: 19:51 - loss: nan - accuracy: 0.0047 - recall_m: nan - precision_m: nan\n",
            "21/98 [=====>........................] - ETA: 19:27 - loss: nan - accuracy: 0.0060 - recall_m: nan - precision_m: nan\n",
            "22/98 [=====>........................] - ETA: 19:07 - loss: nan - accuracy: 0.0057 - recall_m: nan - precision_m: nan\n",
            "23/98 [======>.......................] - ETA: 18:50 - loss: nan - accuracy: 0.0054 - recall_m: nan - precision_m: nan\n",
            "24/98 [======>.......................] - ETA: 18:32 - loss: nan - accuracy: 0.0065 - recall_m: nan - precision_m: nan\n",
            "25/98 [======>.......................] - ETA: 18:14 - loss: nan - accuracy: 0.0063 - recall_m: nan - precision_m: nan\n",
            "26/98 [======>.......................] - ETA: 17:54 - loss: nan - accuracy: 0.0060 - recall_m: nan - precision_m: nan\n",
            "27/98 [=======>......................] - ETA: 17:34 - loss: nan - accuracy: 0.0058 - recall_m: nan - precision_m: nan\n",
            "28/98 [=======>......................] - ETA: 17:19 - loss: nan - accuracy: 0.0067 - recall_m: nan - precision_m: nan\n",
            "29/98 [=======>......................] - ETA: 17:00 - loss: nan - accuracy: 0.0065 - recall_m: nan - precision_m: nan\n",
            "30/98 [========>.....................] - ETA: 16:46 - loss: nan - accuracy: 0.0073 - recall_m: nan - precision_m: nan\n",
            "31/98 [========>.....................] - ETA: 16:30 - loss: nan - accuracy: 0.0071 - recall_m: nan - precision_m: nan\n",
            "32/98 [========>.....................] - ETA: 16:16 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            "33/98 [=========>....................] - ETA: 15:56 - loss: nan - accuracy: 0.0076 - recall_m: nan - precision_m: nan\n",
            "34/98 [=========>....................] - ETA: 15:40 - loss: nan - accuracy: 0.0092 - recall_m: nan - precision_m: nan\n",
            "35/98 [=========>....................] - ETA: 15:23 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "36/98 [==========>...................] - ETA: 15:11 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "37/98 [==========>...................] - ETA: 14:55 - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "38/98 [==========>...................] - ETA: 14:43 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "39/98 [==========>...................] - ETA: 14:25 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "40/98 [===========>..................] - ETA: 14:10 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "41/98 [===========>..................] - ETA: 13:52 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "42/98 [===========>..................] - ETA: 13:35 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "43/98 [============>.................] - ETA: 13:20 - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "44/98 [============>.................] - ETA: 13:05 - loss: nan - accuracy: 0.0135 - recall_m: nan - precision_m: nan\n",
            "45/98 [============>.................] - ETA: 12:53 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "46/98 [=============>................] - ETA: 12:37 - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "47/98 [=============>................] - ETA: 12:23 - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "48/98 [=============>................] - ETA: 12:09 - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "49/98 [==============>...............] - ETA: 11:54 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "50/98 [==============>...............] - ETA: 11:38 - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "51/98 [==============>...............] - ETA: 11:23 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "52/98 [==============>...............] - ETA: 11:09 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "53/98 [===============>..............] - ETA: 10:54 - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "54/98 [===============>..............] - ETA: 10:41 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "55/98 [===============>..............] - ETA: 10:24 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "56/98 [================>.............] - ETA: 10:09 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "57/98 [================>.............] - ETA: 9:53 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan \n",
            "58/98 [================>.............] - ETA: 9:39 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "59/98 [=================>............] - ETA: 9:23 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "60/98 [=================>............] - ETA: 9:06 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "61/98 [=================>............] - ETA: 8:51 - loss: nan - accuracy: 0.0118 - recall_m: nan - precision_m: nan\n",
            "62/98 [=================>............] - ETA: 8:37 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "63/98 [==================>...........] - ETA: 8:23 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "64/98 [==================>...........] - ETA: 8:08 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "65/98 [==================>...........] - ETA: 7:52 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "66/98 [===================>..........] - ETA: 7:42 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "67/98 [===================>..........] - ETA: 7:29 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "68/98 [===================>..........] - ETA: 7:15 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "69/98 [====================>.........] - ETA: 7:00 - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "70/98 [====================>.........] - ETA: 6:46 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "71/98 [====================>.........] - ETA: 6:31 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "72/98 [=====================>........] - ETA: 6:16 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "73/98 [=====================>........] - ETA: 6:01 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "74/98 [=====================>........] - ETA: 5:47 - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "75/98 [=====================>........] - ETA: 5:33 - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "76/98 [======================>.......] - ETA: 5:19 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "77/98 [======================>.......] - ETA: 5:04 - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "78/98 [======================>.......] - ETA: 4:50 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "79/98 [=======================>......] - ETA: 4:36 - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "80/98 [=======================>......] - ETA: 4:22 - loss: nan - accuracy: 0.0137 - recall_m: nan - precision_m: nan\n",
            "81/98 [=======================>......] - ETA: 4:07 - loss: nan - accuracy: 0.0135 - recall_m: nan - precision_m: nan\n",
            "82/98 [========================>.....] - ETA: 3:52 - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "83/98 [========================>.....] - ETA: 3:37 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "84/98 [========================>.....] - ETA: 3:23 - loss: nan - accuracy: 0.0138 - recall_m: nan - precision_m: nan\n",
            "85/98 [=========================>....] - ETA: 3:08 - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "86/98 [=========================>....] - ETA: 2:54 - loss: nan - accuracy: 0.0138 - recall_m: nan - precision_m: nan\n",
            "87/98 [=========================>....] - ETA: 2:39 - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "88/98 [=========================>....] - ETA: 2:25 - loss: nan - accuracy: 0.0135 - recall_m: nan - precision_m: nan\n",
            "89/98 [==========================>...] - ETA: 2:10 - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "90/98 [==========================>...] - ETA: 1:55 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "91/98 [==========================>...] - ETA: 1:41 - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "92/98 [===========================>..] - ETA: 1:26 - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "93/98 [===========================>..] - ETA: 1:12 - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "94/98 [===========================>..] - ETA: 57s - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan \n",
            "95/98 [============================>.] - ETA: 43s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "96/98 [============================>.] - ETA: 28s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "97/98 [============================>.] - ETA: 14s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan \n",
            "98/98 [==============================] - 1466s 15s/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan - val_loss: nan - val_accuracy: 0.0128 - val_recall_m: nan - val_precision_m: nan\n",
            "\n",
            " 1/25 [>.............................] - ETA: 41s - loss: nan - accuracy: 0.3125 - recall_m: nan - precision_m: nan\n",
            " 2/25 [=>............................] - ETA: 41s - loss: nan - accuracy: 0.1562 - recall_m: nan - precision_m: nan\n",
            " 3/25 [==>...........................] - ETA: 41s - loss: nan - accuracy: 0.1042 - recall_m: nan - precision_m: nan\n",
            " 4/25 [===>..........................] - ETA: 37s - loss: nan - accuracy: 0.0781 - recall_m: nan - precision_m: nan\n",
            " 5/25 [=====>........................] - ETA: 34s - loss: nan - accuracy: 0.0625 - recall_m: nan - precision_m: nan\n",
            " 6/25 [======>.......................] - ETA: 32s - loss: nan - accuracy: 0.0521 - recall_m: nan - precision_m: nan\n",
            " 7/25 [=======>......................] - ETA: 31s - loss: nan - accuracy: 0.0446 - recall_m: nan - precision_m: nan\n",
            " 8/25 [========>.....................] - ETA: 29s - loss: nan - accuracy: 0.0391 - recall_m: nan - precision_m: nan\n",
            " 9/25 [=========>....................] - ETA: 27s - loss: nan - accuracy: 0.0347 - recall_m: nan - precision_m: nan\n",
            "10/25 [===========>..................] - ETA: 25s - loss: nan - accuracy: 0.0312 - recall_m: nan - precision_m: nan\n",
            "11/25 [============>.................] - ETA: 24s - loss: nan - accuracy: 0.0284 - recall_m: nan - precision_m: nan\n",
            "12/25 [=============>................] - ETA: 22s - loss: nan - accuracy: 0.0260 - recall_m: nan - precision_m: nan\n",
            "13/25 [==============>...............] - ETA: 20s - loss: nan - accuracy: 0.0240 - recall_m: nan - precision_m: nan\n",
            "14/25 [===============>..............] - ETA: 18s - loss: nan - accuracy: 0.0223 - recall_m: nan - precision_m: nan\n",
            "15/25 [=================>............] - ETA: 17s - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan\n",
            "16/25 [==================>...........] - ETA: 15s - loss: nan - accuracy: 0.0195 - recall_m: nan - precision_m: nan\n",
            "17/25 [===================>..........] - ETA: 13s - loss: nan - accuracy: 0.0184 - recall_m: nan - precision_m: nan\n",
            "18/25 [====================>.........] - ETA: 12s - loss: nan - accuracy: 0.0174 - recall_m: nan - precision_m: nan\n",
            "19/25 [=====================>........] - ETA: 10s - loss: nan - accuracy: 0.0164 - recall_m: nan - precision_m: nan\n",
            "20/25 [=======================>......] - ETA: 8s - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan \n",
            "21/25 [========================>.....] - ETA: 6s - loss: nan - accuracy: 0.0149 - recall_m: nan - precision_m: nan\n",
            "22/25 [=========================>....] - ETA: 5s - loss: nan - accuracy: 0.0142 - recall_m: nan - precision_m: nan\n",
            "23/25 [==========================>...] - ETA: 3s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "24/25 [===========================>..] - ETA: 1s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - 42s 2s/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                           \n",
            "validation recall: nan                                                              \n",
            "validation precision: nan                                                           \n",
            " 1/98 [..............................] - ETA: 16:33 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 2/98 [..............................] - ETA: 7:55 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan \n",
            " 3/98 [..............................] - ETA: 7:56 - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan    \n",
            " 4/98 [>.............................] - ETA: 7:33 - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan\n",
            " 5/98 [>.............................] - ETA: 7:23 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            " 6/98 [>.............................] - ETA: 7:22 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            " 7/98 [=>............................] - ETA: 7:10 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            " 8/98 [=>............................] - ETA: 7:06 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            " 9/98 [=>............................] - ETA: 6:59 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "10/98 [==>...........................] - ETA: 6:56 - loss: nan - accuracy: 0.0094 - recall_m: nan - precision_m: nan\n",
            "11/98 [==>...........................] - ETA: 6:51 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "12/98 [==>...........................] - ETA: 6:45 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "13/98 [==>...........................] - ETA: 6:42 - loss: nan - accuracy: 0.0096 - recall_m: nan - precision_m: nan\n",
            "14/98 [===>..........................] - ETA: 6:37 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "15/98 [===>..........................] - ETA: 6:32 - loss: nan - accuracy: 0.0083 - recall_m: nan - precision_m: nan\n",
            "16/98 [===>..........................] - ETA: 6:27 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            "17/98 [====>.........................] - ETA: 6:22 - loss: nan - accuracy: 0.0074 - recall_m: nan - precision_m: nan\n",
            "18/98 [====>.........................] - ETA: 6:17 - loss: nan - accuracy: 0.0069 - recall_m: nan - precision_m: nan\n",
            "19/98 [====>.........................] - ETA: 6:11 - loss: nan - accuracy: 0.0066 - recall_m: nan - precision_m: nan\n",
            "20/98 [=====>........................] - ETA: 6:06 - loss: nan - accuracy: 0.0063 - recall_m: nan - precision_m: nan\n",
            "21/98 [=====>........................] - ETA: 6:01 - loss: nan - accuracy: 0.0074 - recall_m: nan - precision_m: nan\n",
            "22/98 [=====>........................] - ETA: 5:57 - loss: nan - accuracy: 0.0085 - recall_m: nan - precision_m: nan\n",
            "23/98 [======>.......................] - ETA: 5:52 - loss: nan - accuracy: 0.0095 - recall_m: nan - precision_m: nan\n",
            "24/98 [======>.......................] - ETA: 5:47 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "25/98 [======>.......................] - ETA: 5:43 - loss: nan - accuracy: 0.0100 - recall_m: nan - precision_m: nan\n",
            "26/98 [======>.......................] - ETA: 5:39 - loss: nan - accuracy: 0.0096 - recall_m: nan - precision_m: nan\n",
            "27/98 [=======>......................] - ETA: 5:35 - loss: nan - accuracy: 0.0093 - recall_m: nan - precision_m: nan\n",
            "28/98 [=======>......................] - ETA: 5:30 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "29/98 [=======>......................] - ETA: 5:25 - loss: nan - accuracy: 0.0108 - recall_m: nan - precision_m: nan\n",
            "30/98 [========>.....................] - ETA: 5:20 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "31/98 [========>.....................] - ETA: 5:15 - loss: nan - accuracy: 0.0101 - recall_m: nan - precision_m: nan\n",
            "32/98 [========>.....................] - ETA: 5:10 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "33/98 [=========>....................] - ETA: 5:05 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "34/98 [=========>....................] - ETA: 5:00 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "35/98 [=========>....................] - ETA: 4:55 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "36/98 [==========>...................] - ETA: 4:51 - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "37/98 [==========>...................] - ETA: 4:46 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "38/98 [==========>...................] - ETA: 4:42 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "39/98 [==========>...................] - ETA: 4:37 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "40/98 [===========>..................] - ETA: 4:33 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "41/98 [===========>..................] - ETA: 4:29 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "42/98 [===========>..................] - ETA: 4:25 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "43/98 [============>.................] - ETA: 4:21 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "44/98 [============>.................] - ETA: 4:16 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "45/98 [============>.................] - ETA: 4:12 - loss: nan - accuracy: 0.0111 - recall_m: nan - precision_m: nan\n",
            "46/98 [=============>................] - ETA: 4:07 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "47/98 [=============>................] - ETA: 4:02 - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "48/98 [=============>................] - ETA: 3:57 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "49/98 [==============>...............] - ETA: 3:53 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "50/98 [==============>...............] - ETA: 3:48 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "51/98 [==============>...............] - ETA: 3:43 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "52/98 [==============>...............] - ETA: 3:39 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "53/98 [===============>..............] - ETA: 3:34 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "54/98 [===============>..............] - ETA: 3:29 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "55/98 [===============>..............] - ETA: 3:24 - loss: nan - accuracy: 0.0108 - recall_m: nan - precision_m: nan\n",
            "56/98 [================>.............] - ETA: 3:20 - loss: nan - accuracy: 0.0106 - recall_m: nan - precision_m: nan\n",
            "57/98 [================>.............] - ETA: 3:15 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "58/98 [================>.............] - ETA: 3:10 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "59/98 [=================>............] - ETA: 3:05 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "60/98 [=================>............] - ETA: 3:01 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "61/98 [=================>............] - ETA: 2:56 - loss: nan - accuracy: 0.0118 - recall_m: nan - precision_m: nan\n",
            "62/98 [=================>............] - ETA: 2:51 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "63/98 [==================>...........] - ETA: 2:46 - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "64/98 [==================>...........] - ETA: 2:42 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "65/98 [==================>...........] - ETA: 2:37 - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "66/98 [===================>..........] - ETA: 2:32 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "67/98 [===================>..........] - ETA: 2:27 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "68/98 [===================>..........] - ETA: 2:22 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "69/98 [====================>.........] - ETA: 2:18 - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "70/98 [====================>.........] - ETA: 2:13 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "71/98 [====================>.........] - ETA: 2:10 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "72/98 [=====================>........] - ETA: 2:05 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "73/98 [=====================>........] - ETA: 2:00 - loss: nan - accuracy: 0.0124 - recall_m: nan - precision_m: nan\n",
            "74/98 [=====================>........] - ETA: 1:55 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "75/98 [=====================>........] - ETA: 1:51 - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "76/98 [======================>.......] - ETA: 1:46 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "77/98 [======================>.......] - ETA: 1:41 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "78/98 [======================>.......] - ETA: 1:36 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "79/98 [=======================>......] - ETA: 1:31 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "80/98 [=======================>......] - ETA: 1:26 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "81/98 [=======================>......] - ETA: 1:21 - loss: nan - accuracy: 0.0123 - recall_m: nan - precision_m: nan\n",
            "82/98 [========================>.....] - ETA: 1:17 - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "83/98 [========================>.....] - ETA: 1:12 - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "84/98 [========================>.....] - ETA: 1:07 - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "85/98 [=========================>....] - ETA: 1:02 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "86/98 [=========================>....] - ETA: 57s - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan \n",
            "87/98 [=========================>....] - ETA: 52s - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "88/98 [=========================>....] - ETA: 48s - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "89/98 [==========================>...] - ETA: 43s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "90/98 [==========================>...] - ETA: 38s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "91/98 [==========================>...] - ETA: 33s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "92/98 [===========================>..] - ETA: 29s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "93/98 [===========================>..] - ETA: 24s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "94/98 [===========================>..] - ETA: 19s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "95/98 [============================>.] - ETA: 14s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "96/98 [============================>.] - ETA: 9s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan \n",
            "97/98 [============================>.] - ETA: 4s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - 502s 5s/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan - val_loss: nan - val_accuracy: 0.0128 - val_recall_m: nan - val_precision_m: nan\n",
            "\n",
            " 1/25 [>.............................] - ETA: 22s - loss: nan - accuracy: 0.3125 - recall_m: nan - precision_m: nan\n",
            " 2/25 [=>............................] - ETA: 22s - loss: nan - accuracy: 0.1562 - recall_m: nan - precision_m: nan\n",
            " 3/25 [==>...........................] - ETA: 21s - loss: nan - accuracy: 0.1042 - recall_m: nan - precision_m: nan\n",
            " 4/25 [===>..........................] - ETA: 19s - loss: nan - accuracy: 0.0781 - recall_m: nan - precision_m: nan\n",
            " 5/25 [=====>........................] - ETA: 19s - loss: nan - accuracy: 0.0625 - recall_m: nan - precision_m: nan\n",
            " 6/25 [======>.......................] - ETA: 18s - loss: nan - accuracy: 0.0521 - recall_m: nan - precision_m: nan\n",
            " 7/25 [=======>......................] - ETA: 17s - loss: nan - accuracy: 0.0446 - recall_m: nan - precision_m: nan\n",
            " 8/25 [========>.....................] - ETA: 16s - loss: nan - accuracy: 0.0391 - recall_m: nan - precision_m: nan\n",
            " 9/25 [=========>....................] - ETA: 15s - loss: nan - accuracy: 0.0347 - recall_m: nan - precision_m: nan\n",
            "10/25 [===========>..................] - ETA: 15s - loss: nan - accuracy: 0.0312 - recall_m: nan - precision_m: nan\n",
            "11/25 [============>.................] - ETA: 14s - loss: nan - accuracy: 0.0284 - recall_m: nan - precision_m: nan\n",
            "12/25 [=============>................] - ETA: 13s - loss: nan - accuracy: 0.0260 - recall_m: nan - precision_m: nan\n",
            "13/25 [==============>...............] - ETA: 12s - loss: nan - accuracy: 0.0240 - recall_m: nan - precision_m: nan\n",
            "14/25 [===============>..............] - ETA: 11s - loss: nan - accuracy: 0.0223 - recall_m: nan - precision_m: nan\n",
            "15/25 [=================>............] - ETA: 10s - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan\n",
            "16/25 [==================>...........] - ETA: 9s - loss: nan - accuracy: 0.0195 - recall_m: nan - precision_m: nan \n",
            "17/25 [===================>..........] - ETA: 8s - loss: nan - accuracy: 0.0184 - recall_m: nan - precision_m: nan\n",
            "18/25 [====================>.........] - ETA: 7s - loss: nan - accuracy: 0.0174 - recall_m: nan - precision_m: nan\n",
            "19/25 [=====================>........] - ETA: 6s - loss: nan - accuracy: 0.0164 - recall_m: nan - precision_m: nan\n",
            "20/25 [=======================>......] - ETA: 5s - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan\n",
            "21/25 [========================>.....] - ETA: 4s - loss: nan - accuracy: 0.0149 - recall_m: nan - precision_m: nan\n",
            "22/25 [=========================>....] - ETA: 3s - loss: nan - accuracy: 0.0142 - recall_m: nan - precision_m: nan\n",
            "23/25 [==========================>...] - ETA: 2s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "24/25 [===========================>..] - ETA: 1s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - 25s 1s/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                             \n",
            "validation recall: nan                                                                \n",
            "validation precision: nan                                                             \n",
            " 1/98 [..............................] - ETA: 9:16 - loss: 4.3571 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 7:11 - loss: 4.7575 - accuracy: 0.0312 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            " 3/98 [..............................] - ETA: 8:19 - loss: 5.2248 - accuracy: 0.0312 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/98 [>.............................] - ETA: 7:39 - loss: 5.6256 - accuracy: 0.0312 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/98 [>.............................] - ETA: 7:21 - loss: 5.8986 - accuracy: 0.0250 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/98 [>.............................] - ETA: 7:03 - loss: 6.1043 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/98 [=>............................] - ETA: 6:53 - loss: 6.1385 - accuracy: 0.0179 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/98 [=>............................] - ETA: 6:42 - loss: 6.1655 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/98 [=>............................] - ETA: 6:29 - loss: 6.1571 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/98 [==>...........................] - ETA: 6:20 - loss: 6.0908 - accuracy: 0.0188 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/98 [==>...........................] - ETA: 6:13 - loss: 6.1248 - accuracy: 0.0170 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/98 [==>...........................] - ETA: 6:06 - loss: 6.1270 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/98 [==>...........................] - ETA: 6:00 - loss: 6.1143 - accuracy: 0.0168 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/98 [===>..........................] - ETA: 5:56 - loss: 6.0340 - accuracy: 0.0179 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/98 [===>..........................] - ETA: 5:50 - loss: 5.9910 - accuracy: 0.0167 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/98 [===>..........................] - ETA: 5:45 - loss: 5.9849 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/98 [====>.........................] - ETA: 6:00 - loss: 5.9362 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/98 [====>.........................] - ETA: 5:54 - loss: 5.9103 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/98 [====>.........................] - ETA: 5:47 - loss: 5.8831 - accuracy: 0.0148 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/98 [=====>........................] - ETA: 5:42 - loss: 5.8781 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/98 [=====>........................] - ETA: 5:35 - loss: 5.8321 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/98 [=====>........................] - ETA: 5:30 - loss: 5.8775 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/98 [======>.......................] - ETA: 5:25 - loss: 5.8675 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/98 [======>.......................] - ETA: 5:19 - loss: 5.8432 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/98 [======>.......................] - ETA: 5:13 - loss: 5.8270 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "26/98 [======>.......................] - ETA: 5:09 - loss: 5.8046 - accuracy: 0.0120 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "27/98 [=======>......................] - ETA: 5:03 - loss: 5.7924 - accuracy: 0.0116 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "28/98 [=======>......................] - ETA: 5:07 - loss: 5.7673 - accuracy: 0.0123 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "29/98 [=======>......................] - ETA: 5:02 - loss: 5.7533 - accuracy: 0.0119 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "30/98 [========>.....................] - ETA: 4:57 - loss: 5.7485 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "31/98 [========>.....................] - ETA: 4:51 - loss: 5.7299 - accuracy: 0.0121 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "32/98 [========>.....................] - ETA: 4:55 - loss: 5.7098 - accuracy: 0.0117 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "33/98 [=========>....................] - ETA: 4:49 - loss: 5.6837 - accuracy: 0.0114 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "34/98 [=========>....................] - ETA: 4:45 - loss: 5.6611 - accuracy: 0.0110 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "35/98 [=========>....................] - ETA: 4:40 - loss: 5.6497 - accuracy: 0.0107 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "36/98 [==========>...................] - ETA: 4:35 - loss: 5.6332 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "37/98 [==========>...................] - ETA: 4:30 - loss: 5.6177 - accuracy: 0.0101 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "38/98 [==========>...................] - ETA: 4:25 - loss: 5.6192 - accuracy: 0.0099 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "39/98 [==========>...................] - ETA: 4:19 - loss: 5.6085 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "40/98 [===========>..................] - ETA: 4:15 - loss: 5.5954 - accuracy: 0.0102 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "41/98 [===========>..................] - ETA: 4:10 - loss: 5.5896 - accuracy: 0.0099 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "42/98 [===========>..................] - ETA: 4:05 - loss: 5.5813 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "43/98 [============>.................] - ETA: 4:00 - loss: 5.5620 - accuracy: 0.0102 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "44/98 [============>.................] - ETA: 3:56 - loss: 5.5528 - accuracy: 0.0107 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "45/98 [============>.................] - ETA: 3:51 - loss: 5.5381 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "46/98 [=============>................] - ETA: 3:46 - loss: 5.5251 - accuracy: 0.0109 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "47/98 [=============>................] - ETA: 3:45 - loss: 5.5205 - accuracy: 0.0106 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "48/98 [=============>................] - ETA: 3:41 - loss: 5.5126 - accuracy: 0.0104 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "49/98 [==============>...............] - ETA: 3:36 - loss: 5.5025 - accuracy: 0.0102 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "50/98 [==============>...............] - ETA: 3:31 - loss: 5.4906 - accuracy: 0.0100 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "51/98 [==============>...............] - ETA: 3:27 - loss: 5.4819 - accuracy: 0.0098 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "52/98 [==============>...............] - ETA: 3:22 - loss: 5.4747 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "53/98 [===============>..............] - ETA: 3:18 - loss: 5.4569 - accuracy: 0.0094 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "54/98 [===============>..............] - ETA: 3:13 - loss: 5.4517 - accuracy: 0.0093 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "55/98 [===============>..............] - ETA: 3:12 - loss: 5.4485 - accuracy: 0.0091 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "56/98 [================>.............] - ETA: 3:06 - loss: 5.4523 - accuracy: 0.0089 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "57/98 [================>.............] - ETA: 3:02 - loss: 5.4429 - accuracy: 0.0088 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "58/98 [================>.............] - ETA: 2:57 - loss: 5.4399 - accuracy: 0.0086 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "59/98 [=================>............] - ETA: 2:53 - loss: 5.4424 - accuracy: 0.0085 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "60/98 [=================>............] - ETA: 2:48 - loss: 5.4423 - accuracy: 0.0083 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "61/98 [=================>............] - ETA: 2:43 - loss: 5.4401 - accuracy: 0.0082 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "62/98 [=================>............] - ETA: 2:39 - loss: 5.4277 - accuracy: 0.0081 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "63/98 [==================>...........] - ETA: 2:34 - loss: 5.4275 - accuracy: 0.0079 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "64/98 [==================>...........] - ETA: 2:29 - loss: 5.4223 - accuracy: 0.0078 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "65/98 [==================>...........] - ETA: 2:25 - loss: 5.4160 - accuracy: 0.0077 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "66/98 [===================>..........] - ETA: 2:20 - loss: 5.4117 - accuracy: 0.0085 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "67/98 [===================>..........] - ETA: 2:15 - loss: 5.4038 - accuracy: 0.0084 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "68/98 [===================>..........] - ETA: 2:11 - loss: 5.3965 - accuracy: 0.0083 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "69/98 [====================>.........] - ETA: 2:08 - loss: 5.3867 - accuracy: 0.0091 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "70/98 [====================>.........] - ETA: 2:03 - loss: 5.3808 - accuracy: 0.0089 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "71/98 [====================>.........] - ETA: 2:00 - loss: 5.3738 - accuracy: 0.0092 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "72/98 [=====================>........] - ETA: 1:55 - loss: 5.3676 - accuracy: 0.0091 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "73/98 [=====================>........] - ETA: 1:51 - loss: 5.3649 - accuracy: 0.0090 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "74/98 [=====================>........] - ETA: 1:46 - loss: 5.3630 - accuracy: 0.0089 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "75/98 [=====================>........] - ETA: 1:42 - loss: 5.3586 - accuracy: 0.0088 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "76/98 [======================>.......] - ETA: 1:37 - loss: 5.3500 - accuracy: 0.0090 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "77/98 [======================>.......] - ETA: 1:34 - loss: 5.3435 - accuracy: 0.0089 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "78/98 [======================>.......] - ETA: 1:29 - loss: 5.3391 - accuracy: 0.0092 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "79/98 [=======================>......] - ETA: 1:25 - loss: 5.3376 - accuracy: 0.0091 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "80/98 [=======================>......] - ETA: 1:20 - loss: 5.3336 - accuracy: 0.0094 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "81/98 [=======================>......] - ETA: 1:15 - loss: 5.3319 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "82/98 [========================>.....] - ETA: 1:11 - loss: 5.3256 - accuracy: 0.0099 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "83/98 [========================>.....] - ETA: 1:06 - loss: 5.3220 - accuracy: 0.0098 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "84/98 [========================>.....] - ETA: 1:02 - loss: 5.3193 - accuracy: 0.0097 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "85/98 [=========================>....] - ETA: 57s - loss: 5.3172 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "86/98 [=========================>....] - ETA: 53s - loss: 5.3129 - accuracy: 0.0098 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "87/98 [=========================>....] - ETA: 48s - loss: 5.3127 - accuracy: 0.0097 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "88/98 [=========================>....] - ETA: 44s - loss: 5.3086 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "89/98 [==========================>...] - ETA: 39s - loss: 5.3048 - accuracy: 0.0098 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "90/98 [==========================>...] - ETA: 35s - loss: 5.3033 - accuracy: 0.0097 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "91/98 [==========================>...] - ETA: 31s - loss: 5.2965 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "92/98 [===========================>..] - ETA: 26s - loss: 5.2921 - accuracy: 0.0095 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "93/98 [===========================>..] - ETA: 22s - loss: 5.2902 - accuracy: 0.0094 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "94/98 [===========================>..] - ETA: 17s - loss: 5.2872 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "95/98 [============================>.] - ETA: 13s - loss: 5.2887 - accuracy: 0.0095 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "96/98 [============================>.] - ETA: 8s - loss: 5.2858 - accuracy: 0.0098 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "97/98 [============================>.] - ETA: 4s - loss: 5.2794 - accuracy: 0.0097 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - ETA: 0s - loss: 5.2795 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - 457s 5s/step - loss: 5.2795 - accuracy: 0.0096 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 - val_loss: 4.9280 - val_accuracy: 0.0128 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
            "\n",
            " 1/25 [>.............................] - ETA: 20s - loss: 6.3705 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 19s - loss: 5.4497 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 18s - loss: 5.0497 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 17s - loss: 5.0270 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 16s - loss: 4.8466 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 15s - loss: 4.9382 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 15s - loss: 4.8473 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 14s - loss: 4.8682 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/25 [=========>....................] - ETA: 13s - loss: 4.8050 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/25 [===========>..................] - ETA: 12s - loss: 4.9601 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/25 [============>.................] - ETA: 11s - loss: 5.0323 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/25 [=============>................] - ETA: 10s - loss: 5.0792 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/25 [==============>...............] - ETA: 9s - loss: 5.0702 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "14/25 [===============>..............] - ETA: 9s - loss: 5.0351 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/25 [=================>............] - ETA: 8s - loss: 5.0590 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/25 [==================>...........] - ETA: 7s - loss: 5.0488 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/25 [===================>..........] - ETA: 6s - loss: 4.9748 - accuracy: 0.0074 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            "18/25 [====================>.........] - ETA: 5s - loss: 4.9306 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/25 [=====================>........] - ETA: 4s - loss: 4.9204 - accuracy: 0.0164 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/25 [=======================>......] - ETA: 4s - loss: 4.9355 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/25 [========================>.....] - ETA: 3s - loss: 4.9768 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/25 [=========================>....] - ETA: 2s - loss: 4.9481 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/25 [==========================>...] - ETA: 1s - loss: 4.9300 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/25 [===========================>..] - ETA: 0s - loss: 4.9445 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.9280 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - 20s 804ms/step - loss: 4.9280 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                              \n",
            "validation recall: 0.0                                                                 \n",
            "validation precision: 0.0                                                              \n",
            " 1/98 [..............................] - ETA: 16:12 - loss: 4.3569 - accuracy: 0.0312 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 10:34 - loss: 4.7975 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 10:15 - loss: 4.7834 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/98 [>.............................] - ETA: 9:31 - loss: 5.0957 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            " 5/98 [>.............................] - ETA: 9:00 - loss: 5.1119 - accuracy: 0.0250 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/98 [>.............................] - ETA: 8:47 - loss: 5.1589 - accuracy: 0.0208 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/98 [=>............................] - ETA: 8:32 - loss: 5.1578 - accuracy: 0.0179 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/98 [=>............................] - ETA: 8:22 - loss: 5.1020 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/98 [=>............................] - ETA: 8:14 - loss: 5.0771 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/98 [==>...........................] - ETA: 8:07 - loss: 5.0807 - accuracy: 0.0125 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/98 [==>...........................] - ETA: 7:59 - loss: 5.0537 - accuracy: 0.0170 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/98 [==>...........................] - ETA: 7:51 - loss: 5.0245 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/98 [==>...........................] - ETA: 7:44 - loss: 5.0086 - accuracy: 0.0168 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/98 [===>..........................] - ETA: 7:37 - loss: 4.9767 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/98 [===>..........................] - ETA: 7:32 - loss: 4.9620 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/98 [===>..........................] - ETA: 7:29 - loss: 4.9468 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/98 [====>.........................] - ETA: 7:26 - loss: 4.9342 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/98 [====>.........................] - ETA: 7:26 - loss: 4.9152 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "19/98 [====>.........................] - ETA: 7:24 - loss: 4.8964 - accuracy: 0.0181 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/98 [=====>........................] - ETA: 7:19 - loss: 4.9050 - accuracy: 0.0188 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/98 [=====>........................] - ETA: 7:14 - loss: 4.8993 - accuracy: 0.0193 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/98 [=====>........................] - ETA: 7:08 - loss: 4.8828 - accuracy: 0.0213 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "23/98 [======>.......................] - ETA: 7:00 - loss: 4.8595 - accuracy: 0.0231 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/98 [======>.......................] - ETA: 6:54 - loss: 4.8587 - accuracy: 0.0221 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/98 [======>.......................] - ETA: 6:48 - loss: 4.8478 - accuracy: 0.0213 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "26/98 [======>.......................] - ETA: 6:40 - loss: 4.8494 - accuracy: 0.0204 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "27/98 [=======>......................] - ETA: 6:34 - loss: 4.8501 - accuracy: 0.0197 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "28/98 [=======>......................] - ETA: 6:28 - loss: 4.8418 - accuracy: 0.0190 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "29/98 [=======>......................] - ETA: 6:21 - loss: 4.8310 - accuracy: 0.0183 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "30/98 [========>.....................] - ETA: 6:21 - loss: 4.8303 - accuracy: 0.0177 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "31/98 [========>.....................] - ETA: 6:19 - loss: 4.8550 - accuracy: 0.0171 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "32/98 [========>.....................] - ETA: 6:12 - loss: 4.8394 - accuracy: 0.0176 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "33/98 [=========>....................] - ETA: 6:06 - loss: 4.8439 - accuracy: 0.0170 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "34/98 [=========>....................] - ETA: 5:59 - loss: 4.8558 - accuracy: 0.0165 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "35/98 [=========>....................] - ETA: 5:52 - loss: 4.8549 - accuracy: 0.0170 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "36/98 [==========>...................] - ETA: 5:45 - loss: 4.8548 - accuracy: 0.0174 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "37/98 [==========>...................] - ETA: 5:39 - loss: 4.8597 - accuracy: 0.0169 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "38/98 [==========>...................] - ETA: 5:36 - loss: 4.8503 - accuracy: 0.0164 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "39/98 [==========>...................] - ETA: 5:30 - loss: 4.8467 - accuracy: 0.0168 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "40/98 [===========>..................] - ETA: 5:23 - loss: 4.8495 - accuracy: 0.0172 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "41/98 [===========>..................] - ETA: 5:17 - loss: 4.8455 - accuracy: 0.0168 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "42/98 [===========>..................] - ETA: 5:11 - loss: 4.8401 - accuracy: 0.0164 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "43/98 [============>.................] - ETA: 5:08 - loss: 4.8335 - accuracy: 0.0160 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "44/98 [============>.................] - ETA: 5:02 - loss: 4.8320 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "45/98 [============>.................] - ETA: 4:58 - loss: 4.8229 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "46/98 [=============>................] - ETA: 4:52 - loss: 4.8232 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "47/98 [=============>................] - ETA: 4:46 - loss: 4.8201 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "48/98 [=============>................] - ETA: 4:39 - loss: 4.8192 - accuracy: 0.0150 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "49/98 [==============>...............] - ETA: 4:33 - loss: 4.8153 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "50/98 [==============>...............] - ETA: 4:27 - loss: 4.8120 - accuracy: 0.0150 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "51/98 [==============>...............] - ETA: 4:21 - loss: 4.8097 - accuracy: 0.0147 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "52/98 [==============>...............] - ETA: 4:15 - loss: 4.8064 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "53/98 [===============>..............] - ETA: 4:09 - loss: 4.8011 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "54/98 [===============>..............] - ETA: 4:03 - loss: 4.7978 - accuracy: 0.0156 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "55/98 [===============>..............] - ETA: 3:57 - loss: 4.7968 - accuracy: 0.0153 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "56/98 [================>.............] - ETA: 3:51 - loss: 4.7922 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "57/98 [================>.............] - ETA: 3:45 - loss: 4.7883 - accuracy: 0.0154 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "58/98 [================>.............] - ETA: 3:42 - loss: 4.7870 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "59/98 [=================>............] - ETA: 3:36 - loss: 4.7822 - accuracy: 0.0154 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "60/98 [=================>............] - ETA: 3:30 - loss: 4.7820 - accuracy: 0.0151 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "61/98 [=================>............] - ETA: 3:24 - loss: 4.7826 - accuracy: 0.0149 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "62/98 [=================>............] - ETA: 3:18 - loss: 4.7771 - accuracy: 0.0146 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "63/98 [==================>...........] - ETA: 3:15 - loss: 4.7759 - accuracy: 0.0144 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "64/98 [==================>...........] - ETA: 3:09 - loss: 4.7716 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "65/98 [==================>...........] - ETA: 3:03 - loss: 4.7676 - accuracy: 0.0144 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "66/98 [===================>..........] - ETA: 2:57 - loss: 4.7656 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "67/98 [===================>..........] - ETA: 2:53 - loss: 4.7611 - accuracy: 0.0140 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "68/98 [===================>..........] - ETA: 2:47 - loss: 4.7591 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "69/98 [====================>.........] - ETA: 2:41 - loss: 4.7571 - accuracy: 0.0140 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "70/98 [====================>.........] - ETA: 2:35 - loss: 4.7546 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "71/98 [====================>.........] - ETA: 2:30 - loss: 4.7554 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "72/98 [=====================>........] - ETA: 2:24 - loss: 4.7540 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "73/98 [=====================>........] - ETA: 2:18 - loss: 4.7505 - accuracy: 0.0133 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "74/98 [=====================>........] - ETA: 2:13 - loss: 4.7468 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "75/98 [=====================>........] - ETA: 2:08 - loss: 4.7456 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "76/98 [======================>.......] - ETA: 2:03 - loss: 4.7455 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "77/98 [======================>.......] - ETA: 1:57 - loss: 4.7441 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "78/98 [======================>.......] - ETA: 1:52 - loss: 4.7409 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "79/98 [=======================>......] - ETA: 1:46 - loss: 4.7383 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "80/98 [=======================>......] - ETA: 1:40 - loss: 4.7341 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "81/98 [=======================>......] - ETA: 1:35 - loss: 4.7361 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "82/98 [========================>.....] - ETA: 1:29 - loss: 4.7337 - accuracy: 0.0133 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "83/98 [========================>.....] - ETA: 1:24 - loss: 4.7359 - accuracy: 0.0132 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "84/98 [========================>.....] - ETA: 1:18 - loss: 4.7330 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "85/98 [=========================>....] - ETA: 1:12 - loss: 4.7324 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "86/98 [=========================>....] - ETA: 1:07 - loss: 4.7301 - accuracy: 0.0134 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "87/98 [=========================>....] - ETA: 1:01 - loss: 4.7294 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "88/98 [=========================>....] - ETA: 55s - loss: 4.7289 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "89/98 [==========================>...] - ETA: 50s - loss: 4.7266 - accuracy: 0.0140 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "90/98 [==========================>...] - ETA: 44s - loss: 4.7259 - accuracy: 0.0139 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "91/98 [==========================>...] - ETA: 39s - loss: 4.7254 - accuracy: 0.0137 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "92/98 [===========================>..] - ETA: 33s - loss: 4.7242 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "93/98 [===========================>..] - ETA: 28s - loss: 4.7217 - accuracy: 0.0138 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "94/98 [===========================>..] - ETA: 22s - loss: 4.7181 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "95/98 [============================>.] - ETA: 16s - loss: 4.7178 - accuracy: 0.0135 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "96/98 [============================>.] - ETA: 11s - loss: 4.7176 - accuracy: 0.0140 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "97/98 [============================>.] - ETA: 5s - loss: 4.7164 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "98/98 [==============================] - ETA: 0s - loss: 4.7175 - accuracy: 0.0141 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "98/98 [==============================] - 581s 6s/step - loss: 4.7175 - accuracy: 0.0141 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 - val_loss: 4.5268 - val_accuracy: 0.0128 - val_recall_m: 0.0000e+00 - val_precision_m: 0.0000e+00\n",
            "\n",
            " 1/25 [>.............................] - ETA: 28s - loss: 4.0410 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/25 [=>............................] - ETA: 30s - loss: 4.1124 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/25 [==>...........................] - ETA: 29s - loss: 4.3665 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 4/25 [===>..........................] - ETA: 29s - loss: 4.4197 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 5/25 [=====>........................] - ETA: 27s - loss: 4.4453 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 6/25 [======>.......................] - ETA: 25s - loss: 4.4275 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 7/25 [=======>......................] - ETA: 24s - loss: 4.3932 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 8/25 [========>.....................] - ETA: 23s - loss: 4.4428 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 9/25 [=========>....................] - ETA: 21s - loss: 4.4619 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "10/25 [===========>..................] - ETA: 20s - loss: 4.4468 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "11/25 [============>.................] - ETA: 18s - loss: 4.4483 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "12/25 [=============>................] - ETA: 17s - loss: 4.4787 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "13/25 [==============>...............] - ETA: 16s - loss: 4.4664 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "14/25 [===============>..............] - ETA: 14s - loss: 4.4710 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "15/25 [=================>............] - ETA: 13s - loss: 4.4934 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "16/25 [==================>...........] - ETA: 11s - loss: 4.5105 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "17/25 [===================>..........] - ETA: 10s - loss: 4.5098 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "18/25 [====================>.........] - ETA: 9s - loss: 4.5082 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00 \n",
            "19/25 [=====================>........] - ETA: 8s - loss: 4.5175 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "20/25 [=======================>......] - ETA: 6s - loss: 4.5295 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "21/25 [========================>.....] - ETA: 5s - loss: 4.5395 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "22/25 [=========================>....] - ETA: 3s - loss: 4.5197 - accuracy: 0.0142 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00    \n",
            "23/25 [==========================>...] - ETA: 2s - loss: 4.5183 - accuracy: 0.0136 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "24/25 [===========================>..] - ETA: 1s - loss: 4.5224 - accuracy: 0.0130 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - ETA: 0s - loss: 4.5268 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "25/25 [==============================] - 32s 1s/step - loss: 4.5268 - accuracy: 0.0128 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                              \n",
            "validation recall: 0.0                                                                 \n",
            "validation precision: 0.0                                                              \n",
            " 1/98 [..............................] - ETA: 7:45 - loss: 46.0430 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 2/98 [..............................] - ETA: 9:24 - loss: 326429835264.0000 - accuracy: 0.0000e+00 - recall_m: 0.0000e+00 - precision_m: 0.0000e+00\n",
            " 3/98 [..............................] - ETA: 9:26 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan                            \n",
            " 4/98 [>.............................] - ETA: 9:38 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 5/98 [>.............................] - ETA: 9:08 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 6/98 [>.............................] - ETA: 9:01 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 7/98 [=>............................] - ETA: 9:10 - loss: nan - accuracy: 0.0000e+00 - recall_m: nan - precision_m: nan\n",
            " 8/98 [=>............................] - ETA: 8:36 - loss: nan - accuracy: 0.0039 - recall_m: nan - precision_m: nan    \n",
            " 9/98 [=>............................] - ETA: 8:13 - loss: nan - accuracy: 0.0035 - recall_m: nan - precision_m: nan\n",
            "10/98 [==>...........................] - ETA: 8:18 - loss: nan - accuracy: 0.0063 - recall_m: nan - precision_m: nan\n",
            "11/98 [==>...........................] - ETA: 8:22 - loss: nan - accuracy: 0.0057 - recall_m: nan - precision_m: nan\n",
            "12/98 [==>...........................] - ETA: 8:10 - loss: nan - accuracy: 0.0052 - recall_m: nan - precision_m: nan\n",
            "13/98 [==>...........................] - ETA: 8:12 - loss: nan - accuracy: 0.0048 - recall_m: nan - precision_m: nan\n",
            "14/98 [===>..........................] - ETA: 8:10 - loss: nan - accuracy: 0.0045 - recall_m: nan - precision_m: nan\n",
            "15/98 [===>..........................] - ETA: 7:54 - loss: nan - accuracy: 0.0042 - recall_m: nan - precision_m: nan\n",
            "16/98 [===>..........................] - ETA: 7:49 - loss: nan - accuracy: 0.0078 - recall_m: nan - precision_m: nan\n",
            "17/98 [====>.........................] - ETA: 7:46 - loss: nan - accuracy: 0.0092 - recall_m: nan - precision_m: nan\n",
            "18/98 [====>.........................] - ETA: 7:37 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "19/98 [====>.........................] - ETA: 7:33 - loss: nan - accuracy: 0.0099 - recall_m: nan - precision_m: nan\n",
            "20/98 [=====>........................] - ETA: 7:19 - loss: nan - accuracy: 0.0094 - recall_m: nan - precision_m: nan\n",
            "21/98 [=====>........................] - ETA: 7:09 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "22/98 [=====>........................] - ETA: 7:10 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "23/98 [======>.......................] - ETA: 7:08 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "24/98 [======>.......................] - ETA: 7:03 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "25/98 [======>.......................] - ETA: 6:56 - loss: nan - accuracy: 0.0100 - recall_m: nan - precision_m: nan\n",
            "26/98 [======>.......................] - ETA: 6:45 - loss: nan - accuracy: 0.0096 - recall_m: nan - precision_m: nan\n",
            "27/98 [=======>......................] - ETA: 6:35 - loss: nan - accuracy: 0.0093 - recall_m: nan - precision_m: nan\n",
            "28/98 [=======>......................] - ETA: 6:25 - loss: nan - accuracy: 0.0089 - recall_m: nan - precision_m: nan\n",
            "29/98 [=======>......................] - ETA: 6:16 - loss: nan - accuracy: 0.0086 - recall_m: nan - precision_m: nan\n",
            "30/98 [========>.....................] - ETA: 6:07 - loss: nan - accuracy: 0.0094 - recall_m: nan - precision_m: nan\n",
            "31/98 [========>.....................] - ETA: 5:59 - loss: nan - accuracy: 0.0091 - recall_m: nan - precision_m: nan\n",
            "32/98 [========>.....................] - ETA: 5:58 - loss: nan - accuracy: 0.0098 - recall_m: nan - precision_m: nan\n",
            "33/98 [=========>....................] - ETA: 5:55 - loss: nan - accuracy: 0.0104 - recall_m: nan - precision_m: nan\n",
            "34/98 [=========>....................] - ETA: 5:48 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "35/98 [=========>....................] - ETA: 5:44 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "36/98 [==========>...................] - ETA: 5:36 - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "37/98 [==========>...................] - ETA: 5:28 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "38/98 [==========>...................] - ETA: 5:25 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "39/98 [==========>...................] - ETA: 5:22 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "40/98 [===========>..................] - ETA: 5:14 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "41/98 [===========>..................] - ETA: 5:07 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "42/98 [===========>..................] - ETA: 5:00 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "43/98 [============>.................] - ETA: 4:52 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "44/98 [============>.................] - ETA: 4:45 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "45/98 [============>.................] - ETA: 4:43 - loss: nan - accuracy: 0.0111 - recall_m: nan - precision_m: nan\n",
            "46/98 [=============>................] - ETA: 4:40 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "47/98 [=============>................] - ETA: 4:33 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "48/98 [=============>................] - ETA: 4:26 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "49/98 [==============>...............] - ETA: 4:20 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "50/98 [==============>...............] - ETA: 4:16 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "51/98 [==============>...............] - ETA: 4:09 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "52/98 [==============>...............] - ETA: 4:03 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "53/98 [===============>..............] - ETA: 3:56 - loss: nan - accuracy: 0.0118 - recall_m: nan - precision_m: nan\n",
            "54/98 [===============>..............] - ETA: 3:50 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "55/98 [===============>..............] - ETA: 3:46 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "56/98 [================>.............] - ETA: 3:40 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "57/98 [================>.............] - ETA: 3:37 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "58/98 [================>.............] - ETA: 3:30 - loss: nan - accuracy: 0.0108 - recall_m: nan - precision_m: nan\n",
            "59/98 [=================>............] - ETA: 3:24 - loss: nan - accuracy: 0.0111 - recall_m: nan - precision_m: nan\n",
            "60/98 [=================>............] - ETA: 3:20 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "61/98 [=================>............] - ETA: 3:16 - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "62/98 [=================>............] - ETA: 3:12 - loss: nan - accuracy: 0.0111 - recall_m: nan - precision_m: nan\n",
            "63/98 [==================>...........] - ETA: 3:06 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "64/98 [==================>...........] - ETA: 3:00 - loss: nan - accuracy: 0.0107 - recall_m: nan - precision_m: nan\n",
            "65/98 [==================>...........] - ETA: 2:55 - loss: nan - accuracy: 0.0106 - recall_m: nan - precision_m: nan\n",
            "66/98 [===================>..........] - ETA: 2:49 - loss: nan - accuracy: 0.0109 - recall_m: nan - precision_m: nan\n",
            "67/98 [===================>..........] - ETA: 2:45 - loss: nan - accuracy: 0.0112 - recall_m: nan - precision_m: nan\n",
            "68/98 [===================>..........] - ETA: 2:39 - loss: nan - accuracy: 0.0110 - recall_m: nan - precision_m: nan\n",
            "69/98 [====================>.........] - ETA: 2:33 - loss: nan - accuracy: 0.0113 - recall_m: nan - precision_m: nan\n",
            "70/98 [====================>.........] - ETA: 2:27 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "71/98 [====================>.........] - ETA: 2:21 - loss: nan - accuracy: 0.0114 - recall_m: nan - precision_m: nan\n",
            "72/98 [=====================>........] - ETA: 2:17 - loss: nan - accuracy: 0.0117 - recall_m: nan - precision_m: nan\n",
            "73/98 [=====================>........] - ETA: 2:11 - loss: nan - accuracy: 0.0116 - recall_m: nan - precision_m: nan\n",
            "74/98 [=====================>........] - ETA: 2:05 - loss: nan - accuracy: 0.0118 - recall_m: nan - precision_m: nan\n",
            "75/98 [=====================>........] - ETA: 2:00 - loss: nan - accuracy: 0.0121 - recall_m: nan - precision_m: nan\n",
            "76/98 [======================>.......] - ETA: 1:54 - loss: nan - accuracy: 0.0119 - recall_m: nan - precision_m: nan\n",
            "77/98 [======================>.......] - ETA: 1:49 - loss: nan - accuracy: 0.0122 - recall_m: nan - precision_m: nan\n",
            "78/98 [======================>.......] - ETA: 1:44 - loss: nan - accuracy: 0.0120 - recall_m: nan - precision_m: nan\n",
            "79/98 [=======================>......] - ETA: 1:39 - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "80/98 [=======================>......] - ETA: 1:34 - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "81/98 [=======================>......] - ETA: 1:29 - loss: nan - accuracy: 0.0131 - recall_m: nan - precision_m: nan\n",
            "82/98 [========================>.....] - ETA: 1:24 - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "83/98 [========================>.....] - ETA: 1:18 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "84/98 [========================>.....] - ETA: 1:13 - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "85/98 [=========================>....] - ETA: 1:08 - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "86/98 [=========================>....] - ETA: 1:03 - loss: nan - accuracy: 0.0134 - recall_m: nan - precision_m: nan\n",
            "87/98 [=========================>....] - ETA: 58s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan \n",
            "88/98 [=========================>....] - ETA: 52s - loss: nan - accuracy: 0.0135 - recall_m: nan - precision_m: nan\n",
            "89/98 [==========================>...] - ETA: 47s - loss: nan - accuracy: 0.0133 - recall_m: nan - precision_m: nan\n",
            "90/98 [==========================>...] - ETA: 42s - loss: nan - accuracy: 0.0132 - recall_m: nan - precision_m: nan\n",
            "91/98 [==========================>...] - ETA: 36s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "92/98 [===========================>..] - ETA: 31s - loss: nan - accuracy: 0.0129 - recall_m: nan - precision_m: nan\n",
            "93/98 [===========================>..] - ETA: 26s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "94/98 [===========================>..] - ETA: 21s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan\n",
            "95/98 [============================>.] - ETA: 15s - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "96/98 [============================>.] - ETA: 10s - loss: nan - accuracy: 0.0127 - recall_m: nan - precision_m: nan\n",
            "97/98 [============================>.] - ETA: 5s - loss: nan - accuracy: 0.0126 - recall_m: nan - precision_m: nan \n",
            "98/98 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan\n",
            "98/98 [==============================] - 543s 6s/step - loss: nan - accuracy: 0.0125 - recall_m: nan - precision_m: nan - val_loss: nan - val_accuracy: 0.0128 - val_recall_m: nan - val_precision_m: nan\n",
            "\n",
            " 1/25 [>.............................] - ETA: 23s - loss: nan - accuracy: 0.3125 - recall_m: nan - precision_m: nan\n",
            " 2/25 [=>............................] - ETA: 21s - loss: nan - accuracy: 0.1562 - recall_m: nan - precision_m: nan\n",
            " 3/25 [==>...........................] - ETA: 20s - loss: nan - accuracy: 0.1042 - recall_m: nan - precision_m: nan\n",
            " 4/25 [===>..........................] - ETA: 19s - loss: nan - accuracy: 0.0781 - recall_m: nan - precision_m: nan\n",
            " 5/25 [=====>........................] - ETA: 18s - loss: nan - accuracy: 0.0625 - recall_m: nan - precision_m: nan\n",
            " 6/25 [======>.......................] - ETA: 18s - loss: nan - accuracy: 0.0521 - recall_m: nan - precision_m: nan\n",
            " 7/25 [=======>......................] - ETA: 17s - loss: nan - accuracy: 0.0446 - recall_m: nan - precision_m: nan\n",
            " 8/25 [========>.....................] - ETA: 16s - loss: nan - accuracy: 0.0391 - recall_m: nan - precision_m: nan\n",
            " 9/25 [=========>....................] - ETA: 15s - loss: nan - accuracy: 0.0347 - recall_m: nan - precision_m: nan\n",
            "10/25 [===========>..................] - ETA: 14s - loss: nan - accuracy: 0.0312 - recall_m: nan - precision_m: nan\n",
            "11/25 [============>.................] - ETA: 13s - loss: nan - accuracy: 0.0284 - recall_m: nan - precision_m: nan\n",
            "12/25 [=============>................] - ETA: 12s - loss: nan - accuracy: 0.0260 - recall_m: nan - precision_m: nan\n",
            "13/25 [==============>...............] - ETA: 12s - loss: nan - accuracy: 0.0240 - recall_m: nan - precision_m: nan\n",
            "14/25 [===============>..............] - ETA: 11s - loss: nan - accuracy: 0.0223 - recall_m: nan - precision_m: nan\n",
            "15/25 [=================>............] - ETA: 11s - loss: nan - accuracy: 0.0208 - recall_m: nan - precision_m: nan\n",
            "16/25 [==================>...........] - ETA: 11s - loss: nan - accuracy: 0.0195 - recall_m: nan - precision_m: nan\n",
            "17/25 [===================>..........] - ETA: 10s - loss: nan - accuracy: 0.0184 - recall_m: nan - precision_m: nan\n",
            "18/25 [====================>.........] - ETA: 8s - loss: nan - accuracy: 0.0174 - recall_m: nan - precision_m: nan \n",
            "19/25 [=====================>........] - ETA: 7s - loss: nan - accuracy: 0.0164 - recall_m: nan - precision_m: nan\n",
            "20/25 [=======================>......] - ETA: 6s - loss: nan - accuracy: 0.0156 - recall_m: nan - precision_m: nan\n",
            "21/25 [========================>.....] - ETA: 5s - loss: nan - accuracy: 0.0149 - recall_m: nan - precision_m: nan\n",
            "22/25 [=========================>....] - ETA: 4s - loss: nan - accuracy: 0.0142 - recall_m: nan - precision_m: nan\n",
            "23/25 [==========================>...] - ETA: 2s - loss: nan - accuracy: 0.0136 - recall_m: nan - precision_m: nan\n",
            "24/25 [===========================>..] - ETA: 1s - loss: nan - accuracy: 0.0130 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - ETA: 0s - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "25/25 [==============================] - 32s 1s/step - loss: nan - accuracy: 0.0128 - recall_m: nan - precision_m: nan\n",
            "\n",
            "validation Accuracy: 0.012820512987673283                                               \n",
            "validation recall: nan                                                                  \n",
            "validation precision: nan                                                               \n",
            "  6%|â–‹         | 13/200 [1:36:13<30:17:33, 583.17s/trial, best loss: 4.5267558097839355]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "job exception: Graph execution error:\n",
            "\n",
            "Detected at node 'RMSprop/RMSprop/update_4/Square' defined at (most recent call last):\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
            "      return _run_code(code, main_globals, None,\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\runpy.py\", line 86, in _run_code\n",
            "      exec(code, run_globals)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
            "      app.launch_new_instance()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
            "      app.start()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
            "      self.io_loop.start()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
            "      self.asyncio_loop.run_forever()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
            "      self._run_once()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
            "      handle._run()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\events.py\", line 80, in _run\n",
            "      self._context.run(self._callback, *self._args)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
            "      await self.process_one()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
            "      await dispatch(*args)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
            "      await result\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
            "      await super().execute_request(stream, ident, parent)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
            "      reply_content = await reply_content\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
            "      res = shell.run_cell(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
            "      return super().run_cell(*args, **kwargs)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
            "      result = self._run_cell(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
            "      result = runner(coro)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
            "      coro.send(None)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
            "      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
            "      if await self.run_code(code, result, async_=asy):\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
            "      exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "    File \"C:\\Users\\Panther\\AppData\\Local\\Temp\\ipykernel_20716\\778908521.py\", line 1, in <module>\n",
            "      best_params = fmin(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 540, in fmin\n",
            "      return trials.fmin(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py\", line 671, in fmin\n",
            "      return fmin(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 586, in fmin\n",
            "      rval.exhaust()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 364, in exhaust\n",
            "      self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 300, in run\n",
            "      self.serial_evaluate()\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 178, in serial_evaluate\n",
            "      result = self.domain.evaluate(spec, ctrl)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py\", line 892, in evaluate\n",
            "      rval = self.fn(pyll_rval)\n",
            "    File \"C:\\Users\\Panther\\AppData\\Local\\Temp\\ipykernel_20716\\846914089.py\", line 60, in create_model\n",
            "      model.fit(train_images, train_name_enc,\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n",
            "      return fn(*args, **kwargs)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n",
            "      tmp_logs = self.train_function(iterator)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n",
            "      return step_function(self, iterator)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n",
            "      outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n",
            "      outputs = model.train_step(data)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n",
            "      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n",
            "      return self.apply_gradients(grads_and_vars, name=name)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 738, in apply_gradients\n",
            "      return tf.__internal__.distribute.interim.maybe_merge_call(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 797, in _distributed_apply\n",
            "      update_op = distribution.extended.update(\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 776, in apply_grad_to_update_var\n",
            "      update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n",
            "    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py\", line 226, in _resource_apply_dense\n",
            "      ] * tf.square(grad)\n",
            "Node: 'RMSprop/RMSprop/update_4/Square'\n",
            "OOM when allocating tensor with shape[384000,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n",
            "\t [[{{node RMSprop/RMSprop/update_4/Square}}]]\n",
            "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n",
            " [Op:__inference_train_function_32140]\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  6%|â–‹         | 13/200 [1:37:11<23:18:00, 448.56s/trial, best loss: 4.5267558097839355]\n"
          ]
        },
        {
          "ename": "ResourceExhaustedError",
          "evalue": "Graph execution error:\n\nDetected at node 'RMSprop/RMSprop/update_4/Square' defined at (most recent call last):\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Panther\\AppData\\Local\\Temp\\ipykernel_20716\\778908521.py\", line 1, in <module>\n      best_params = fmin(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 540, in fmin\n      return trials.fmin(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py\", line 671, in fmin\n      return fmin(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 586, in fmin\n      rval.exhaust()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 364, in exhaust\n      self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 300, in run\n      self.serial_evaluate()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 178, in serial_evaluate\n      result = self.domain.evaluate(spec, ctrl)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py\", line 892, in evaluate\n      rval = self.fn(pyll_rval)\n    File \"C:\\Users\\Panther\\AppData\\Local\\Temp\\ipykernel_20716\\846914089.py\", line 60, in create_model\n      model.fit(train_images, train_name_enc,\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 738, in apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 797, in _distributed_apply\n      update_op = distribution.extended.update(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 776, in apply_grad_to_update_var\n      update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py\", line 226, in _resource_apply_dense\n      ] * tf.square(grad)\nNode: 'RMSprop/RMSprop/update_4/Square'\nOOM when allocating tensor with shape[384000,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node RMSprop/RMSprop/update_4/Square}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_32140]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#bayesian optimization\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#number of models to try\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py:540\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    537\u001b[0m     fn \u001b[38;5;241m=\u001b[39m __objective_fmin_wrapper(fn)\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_trials_fmin \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfmin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(trials_save_file):\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py:671\u001b[0m, in \u001b[0;36mTrials.fmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# -- Stop-gap implementation!\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m#    fmin should have been a Trials method in the first place\u001b[39;00m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m#    but for now it's still sitting in another file.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fmin\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_trials_fmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# -- prevent recursion\u001b[39;49;00m\n\u001b[0;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpass_expr_memo_ctrl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatch_eval_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_argmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_argmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progressbar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progressbar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrials_save_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrials_save_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
            "Cell \u001b[1;32mIn[37], line 60\u001b[0m, in \u001b[0;36mcreate_model\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics \u001b[38;5;241m=\u001b[39m metrics)\n\u001b[0;32m     54\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     55\u001b[0m                                                   mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m#to match the fmin function from hyperopt\u001b[39;00m\n\u001b[0;32m     56\u001b[0m                                                                 \u001b[38;5;66;03m#we will search for the model with the least loss\u001b[39;00m\n\u001b[0;32m     57\u001b[0m                                                   verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     58\u001b[0m                                                   patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_name_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_name_enc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m loss, accuracy, recall, precision \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(validation_images, validation_name_enc)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'RMSprop/RMSprop/update_4/Square' defined at (most recent call last):\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n      app.start()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n      res = shell.run_cell(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n      result = self._run_cell(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n      result = runner(coro)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n      coro.send(None)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Panther\\AppData\\Local\\Temp\\ipykernel_20716\\778908521.py\", line 1, in <module>\n      best_params = fmin(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 540, in fmin\n      return trials.fmin(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py\", line 671, in fmin\n      return fmin(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 586, in fmin\n      rval.exhaust()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 364, in exhaust\n      self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 300, in run\n      self.serial_evaluate()\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\fmin.py\", line 178, in serial_evaluate\n      result = self.domain.evaluate(spec, ctrl)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\hyperopt\\base.py\", line 892, in evaluate\n      rval = self.fn(pyll_rval)\n    File \"C:\\Users\\Panther\\AppData\\Local\\Temp\\ipykernel_20716\\846914089.py\", line 60, in create_model\n      model.fit(train_images, train_name_enc,\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 579, in minimize\n      return self.apply_gradients(grads_and_vars, name=name)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 738, in apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 797, in _distributed_apply\n      update_op = distribution.extended.update(\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 776, in apply_grad_to_update_var\n      update_op = self._resource_apply_dense(grad, var, **apply_kwargs)\n    File \"c:\\Users\\Panther\\anaconda3\\envs\\dl_env\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\rmsprop.py\", line 226, in _resource_apply_dense\n      ] * tf.square(grad)\nNode: 'RMSprop/RMSprop/update_4/Square'\nOOM when allocating tensor with shape[384000,2048] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node RMSprop/RMSprop/update_4/Square}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_32140]"
          ]
        }
      ],
      "source": [
        "best_params = fmin(\n",
        "    fn = create_model,\n",
        "    space = params_space,\n",
        "    algo = tpe.suggest, #bayesian optimization\n",
        "    max_evals= 200, #number of models to try\n",
        "    trials= trial\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}