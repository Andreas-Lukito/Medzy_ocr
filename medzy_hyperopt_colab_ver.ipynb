{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andreas-Lukito/Medzy_ocr/blob/main/medzy_hyperopt_colab_ver.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc0HHa_PGtJZ"
      },
      "source": [
        "# Medzy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u31q74K4GtJb"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqiL40_6GtJb"
      },
      "source": [
        "This project aims to develop a machine learning model capable of interpreting doctorsâ€™ handwriting on prescriptions. By accurately detecting and translating challenging handwriting, the model will empower patients to read their prescriptions independently, making it easier for them to purchase their medications without confusion if they run out of medecine or to check if the cleric gave the correct medicine.\n",
        "\n",
        "This model will use Tensor flows' keras convolutional neural network as reference to this <a href = \"https://www.tensorflow.org/tutorials/images/cnn\">documentation</a>. The model will also be trained using this <a href=\"https://www.kaggle.com/datasets/mamun1113/doctors-handwritten-prescription-bd-dataset\">dataset</a> from kaggle."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohxPss_mGtJc"
      },
      "source": [
        "## Importing needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperopt\n",
        "!pip install pynvml"
      ],
      "metadata": {
        "id": "e12rEA6lGykr",
        "outputId": "42c7cea5-5e91-4db3-f39d-8f566f3c52a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.11/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.11/dist-packages (from hyperopt) (3.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from hyperopt) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt) (0.10.9.7)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (11.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vsj5gt4NGtJd"
      },
      "outputs": [],
      "source": [
        "# basic python libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageEnhance, ImageFilter\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# data preprocessing libraries\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# tensor flow libraries\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# Fine tuning libraries\n",
        "import hyperopt\n",
        "from hyperopt import hp, fmin, tpe, Trials, space_eval, STATUS_OK\n",
        "\n",
        "# library for gpu utilization\n",
        "import pynvml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIXSh1PiGtJe"
      },
      "source": [
        "## GPU check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NKWjbKhiGtJe",
        "outputId": "7a02c717-a21d-41c4-d446-af2a3f151e45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is using the GPU\n",
            "GPU Name: b'NVIDIA L4'\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Check if TensorFlow is using the GPU\n",
        "if tf.test.is_gpu_available():\n",
        "    print(\"TensorFlow is using the GPU\")\n",
        "\n",
        "    # Initialize the pynvml library\n",
        "    pynvml.nvmlInit()\n",
        "\n",
        "    # Get the number of GPU devices\n",
        "    num_gpus = pynvml.nvmlDeviceGetCount()\n",
        "\n",
        "    # Iterate over GPU devices\n",
        "    for i in range(num_gpus):\n",
        "        # Get the device identifier\n",
        "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
        "        # Get the full GPU name\n",
        "        gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
        "        print(\"GPU Name:\", gpu_name)\n",
        "\n",
        "    # Shutdown the pynvml library\n",
        "    pynvml.nvmlShutdown()\n",
        "else:\n",
        "    print(\"TensorFlow is not using the GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TUp8IrvTGtJf",
        "outputId": "a5ce82f5-e705-438f-c2d2-ee4c1ecf94ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gts3HoBTGtJg"
      },
      "source": [
        "## Importing the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "PgIT78aaHnZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDQj0FStGtJg"
      },
      "source": [
        "### Train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiOlj8mLGtJg"
      },
      "source": [
        "#### Train Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LjoGCE_5GtJg",
        "outputId": "3d9c8459-1d6d-4733-85f1-4eafad3eba9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './Dataset/archive/dataset/Training/training_labels.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c72291129aab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Dataset/archive/dataset/Training\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"training_labels.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Dataset/archive/dataset/Training/training_labels.csv'"
          ]
        }
      ],
      "source": [
        "train_path = \"/content/drive/MyDrive/project_medzy/dataset/Training\"\n",
        "train_labels = pd.read_csv(os.path.join(train_path,\"training_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN2gisjrGtJh"
      },
      "outputs": [],
      "source": [
        "train_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqhKnRLGtJh"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96zeKeBZGtJh"
      },
      "outputs": [],
      "source": [
        "medicine_enc = LabelEncoder()\n",
        "train_name_enc = to_categorical(medicine_enc.fit_transform(train_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# train_labels[\"MEDICINE_NAME_ENC\"] = train_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3GuZEFJGtJh"
      },
      "outputs": [],
      "source": [
        "len(train_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9OxfOO0GtJh"
      },
      "source": [
        "after encoding there are 78 unique values/medicines since we are using label encoder, we will put them all in to a seperate column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7rFEOQaGtJh"
      },
      "source": [
        "#### Train Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvOKyAnRGtJi"
      },
      "outputs": [],
      "source": [
        "#the image width and height to pass to the model\n",
        "img_width = 420\n",
        "img_height = np.round(img_width/3, 0).astype(\"int\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73P5qIzjGtJi"
      },
      "outputs": [],
      "source": [
        "train_images = []\n",
        "train_files = glob.glob(\"/content/drive/MyDrive/project_medzy/dataset/Training/training_words/*.png\")\n",
        "for picture in train_files:\n",
        "    image = cv2.resize(cv2.imread(picture), (img_width, img_height))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "\n",
        "    #apply adaptive treshold\n",
        "    image = cv2.adaptiveThreshold(image,\n",
        "                                         255, # the max value\n",
        "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                         cv2.THRESH_BINARY, #the treshold we are using\n",
        "                                         41, #how many pixels to look at\n",
        "                                         10 #noise reduction\n",
        "                                         )\n",
        "\n",
        "    #sharpening the image\n",
        "    # Create the sharpening kernel\n",
        "    kernel = np.array([[-1, -1, 1],\n",
        "                        [-1,  8, -1],\n",
        "                        [-1, -2, -1]])\n",
        "\n",
        "    #increase the contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(7,7))\n",
        "    image = clahe.apply(image)\n",
        "\n",
        "    #blur the image so that the lines are more defined\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "\n",
        "    # Sharpen the image\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "    train_images.append(image)\n",
        "\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "train_images = np.array(train_images)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(train_images)\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YhHFS-nGtJi"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", train_images[0].shape)\n",
        "print(\"Labels shape:\", train_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6Hu_KwEGtJi"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYXSUF7yGtJj"
      },
      "outputs": [],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpEyS13DGtJk"
      },
      "outputs": [],
      "source": [
        "plt.imshow(train_images[20], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VAkpk9gGtJk"
      },
      "outputs": [],
      "source": [
        "print(\"Example label:\", train_labels.iloc[3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e0FkNlSGtJl"
      },
      "source": [
        "### Validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxrNpj-TGtJl"
      },
      "source": [
        "#### validation Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asLNtGGnGtJl"
      },
      "outputs": [],
      "source": [
        "validation_path = \"/content/drive/MyDrive/project_medzy/dataset/Validation\"\n",
        "validation_labels = pd.read_csv(os.path.join(validation_path,\"validation_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A59YkxHbGtJl"
      },
      "outputs": [],
      "source": [
        "validation_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EspgN4ZhGtJm"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEGcgKSrGtJm"
      },
      "outputs": [],
      "source": [
        "validation_name_enc = to_categorical(medicine_enc.transform(validation_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# validation_labels[\"MEDECINE_NAME_ENC\"] = validation_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-HL34pvGtJm"
      },
      "outputs": [],
      "source": [
        "type(validation_name_enc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_1xCiLsGtJn"
      },
      "outputs": [],
      "source": [
        "len(validation_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLzfJuW9GtJn"
      },
      "source": [
        "#### Validation Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT5j06V1GtJn"
      },
      "outputs": [],
      "source": [
        "validation_images = []\n",
        "validation_files = glob.glob(\"/content/drive/MyDrive/project_medzy/dataset/Validation/validation_words/*.png\")\n",
        "for picture in validation_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    #apply adaptive treshold\n",
        "    image = cv2.adaptiveThreshold(image,\n",
        "                                         255, # the max value\n",
        "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                         cv2.THRESH_BINARY, #the treshold we are using\n",
        "                                         41, #how many pixels to look at\n",
        "                                         10 #noise reduction\n",
        "                                         )\n",
        "\n",
        "    #sharpening the image\n",
        "    # Create the sharpening kernel\n",
        "    kernel = np.array([[-1, -1, 1],\n",
        "                        [-1,  8, -1],\n",
        "                        [-1, -2, -1]])\n",
        "\n",
        "    #increase the contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(7,7))\n",
        "    image = clahe.apply(image)\n",
        "\n",
        "    #blur the image so that the lines are more defined\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "\n",
        "    # Sharpen the image\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "\n",
        "    validation_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "validation_images = np.array(validation_images)\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((validation_images, validation_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(validation_images)\n",
        "validation_dataset = validation_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUWTjKQ5GtJn"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", validation_images.shape)\n",
        "print(\"Labels shape:\", validation_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRoGOtCTGtJo"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhb5q2_KGtJo"
      },
      "outputs": [],
      "source": [
        "validation_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLDzjiTjGtJo"
      },
      "outputs": [],
      "source": [
        "plt.imshow(validation_images[0], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lwc4lycfGtJv"
      },
      "outputs": [],
      "source": [
        "print(\"Example label:\", validation_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfXPIJN2GtJv"
      },
      "source": [
        "### Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBdes_TOGtJv"
      },
      "source": [
        "#### Test Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Okmf7MGVGtJv"
      },
      "outputs": [],
      "source": [
        "test_path = \"/content/drive/MyDrive/project_medzy/dataset/Testing\"\n",
        "test_labels = pd.read_csv(os.path.join(test_path,\"testing_labels.csv\"), delimiter = \",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U63Q5QFKGtJw"
      },
      "outputs": [],
      "source": [
        "test_labels.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvqF5rqGtJw"
      },
      "source": [
        "##### Encode the medecine name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR6GPdVOGtJw"
      },
      "outputs": [],
      "source": [
        "test_name_enc = to_categorical(medicine_enc.transform(test_labels[\"MEDICINE_NAME\"]), num_classes=78)\n",
        "# test_labels[\"train_medecine_name_enc\"] = test_name_enc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi-SnPVhGtJw"
      },
      "outputs": [],
      "source": [
        "len(test_labels[\"MEDICINE_NAME\"].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeJH3OkMGtJw"
      },
      "source": [
        "#### Testing Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikoo83DHGtJx"
      },
      "outputs": [],
      "source": [
        "test_images = []\n",
        "test_files = glob.glob(\"/content/drive/MyDrive/project_medzy/dataset/Testing/testing_words/*.png\")\n",
        "for picture in test_files:\n",
        "    image = cv2.resize(cv2.imread(picture, cv2.IMREAD_GRAYSCALE), (img_width, img_height))\n",
        "\n",
        "    #since cv2 sometimes return a \"none\" type we will append the data after validating it if it is a not \"none\" type\n",
        "    if image is None:\n",
        "        print(f\"Err importing picture {picture}\")\n",
        "        continue\n",
        "    #apply adaptive treshold\n",
        "    image = cv2.adaptiveThreshold(image,\n",
        "                                         255, # the max value\n",
        "                                         cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                         cv2.THRESH_BINARY, #the treshold we are using\n",
        "                                         41, #how many pixels to look at\n",
        "                                         10 #noise reduction\n",
        "                                         )\n",
        "\n",
        "    #sharpening the image\n",
        "    # Create the sharpening kernel\n",
        "    kernel = np.array([[-1, -1, 1],\n",
        "                        [-1,  8, -1],\n",
        "                        [-1, -2, -1]])\n",
        "\n",
        "    #increase the contrast\n",
        "    clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(7,7))\n",
        "    image = clahe.apply(image)\n",
        "\n",
        "    #blur the image so that the lines are more defined\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "    image = cv2.GaussianBlur(image, (7,7), 20)\n",
        "\n",
        "    # Sharpen the image\n",
        "    image = cv2.filter2D(image, -1, kernel)\n",
        "\n",
        "    test_images.append(image)\n",
        "    # image = np.asarray(image) # for numpy 1.23\n",
        "\n",
        "    # To show the images\n",
        "    # plt.imshow(image, cmap = \"gray\")\n",
        "    # plt.show()\n",
        "\n",
        "test_images = np.array(test_images)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "\n",
        "# Shuffling the data\n",
        "BUFFER_SIZE = len(test_images)\n",
        "test_dataset = test_dataset.shuffle(BUFFER_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Og3oHqMiGtJx"
      },
      "outputs": [],
      "source": [
        "print(\"Dataset shape:\", test_images.shape)\n",
        "print(\"Labels shape:\", test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRICNfFMGtJx"
      },
      "source": [
        "##### Check if it is correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSLQZ0C_GtJy"
      },
      "outputs": [],
      "source": [
        "test_images[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRb5WGV8GtJy"
      },
      "outputs": [],
      "source": [
        "plt.imshow(test_images[0], cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VoBw2moGtJy"
      },
      "outputs": [],
      "source": [
        "print(\"Example label:\", test_labels.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5eQd0nEGtJy"
      },
      "source": [
        "## Building the artificial neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGDLHhDLGtJz"
      },
      "source": [
        "#### Make a model create function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnyhLojVGtJz"
      },
      "source": [
        "##### Parameters for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSymO6AFGtJz"
      },
      "outputs": [],
      "source": [
        "#number of classes to determine how many neurons are in the output layer\n",
        "num_classes = len(train_labels[\"MEDICINE_NAME\"].unique())\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXx0BIZbGtJz"
      },
      "outputs": [],
      "source": [
        "#the image size to determine the shape for the convolutional neural network to scan\n",
        "train_images[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKdWqFwzGtJ0"
      },
      "source": [
        "#### Custom metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aBDZLA9GtJ0"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return keras.backend.cast(recall, \"float16\")\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return keras.backend.cast(precision, \"float16\")\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    f1 = 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "    return keras.backend.cast(f1, \"float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hM89xQcmGtJ0"
      },
      "source": [
        "#### Create a model builder for gridsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOaTV6HvGtJ0"
      },
      "source": [
        "the even filter shapes aren't recommended because it lacks the ability to devide the previous layer pixels arould the output pixel <a hre = \"https://medium.com/analytics-vidhya/how-to-choose-the-size-of-the-convolution-filter-or-kernel-size-for-cnn-86a55a1e2d15\">(Pandey, 2020)</a>.\n",
        "\n",
        "<a href = \"https://medium.com/@nerdjock/convolutional-neural-network-lesson-9-activation-functions-in-cnns-57def9c6e759\">Machine Learning in Plain English (2023)</a> The most common activation functions are \"relu\" and \"leaky relu\" therefore we would pass it in the grid search.\n",
        "\n",
        "Max pooling excells in image classification, due to how max pooling captures the most prominent features and reduce the variance of the input <a href = \"https://www.linkedin.com/advice/1/how-do-you-choose-appropriate-pooling-method-2uvmc#adaptive-pooling\">(Awad et. al, n.d.)</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsN-QAxeGtJ1"
      },
      "outputs": [],
      "source": [
        "# Model Parameters to Grid Search\n",
        "params_space = {\n",
        "    \"filter_choice\" : hp.choice(\"filter_choice\", [32, 64, 128]),\n",
        "    \"kernel_size\" : hp.choice(\"kernel_size\", [3, 5, 7]),\n",
        "    \"n_neurons\" : hp.choice(\"n_neurons\", [256, 512, 1024]),\n",
        "    \"learning_rate\": hp.uniform(\"learning_rate\",0.001,1),\n",
        "    \"activation\" : hp.choice(\"activation\",['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
        "                   'elu', 'exponential', 'softmax']),\n",
        "    \"n_conv_layers\": hp.choice(\"n_conv_layers\", [1, 3]),\n",
        "    \"n_pool_layers\": hp.choice(\"n_pool_layers\", [1, 2, 3]),\n",
        "    \"dense_optimizer\": hp.choice('optimizer', ['adam', 'sgd', 'rmsprop']),\n",
        "    \"dropout_rate\": hp.uniform(\"dropout_rate\", 0.1, 0.5)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DXEF9E4GtJ1"
      },
      "outputs": [],
      "source": [
        "def create_model(params):\n",
        "    input_shape = (img_height, img_width, 1)\n",
        "    model = Sequential()\n",
        "    metrics = ['accuracy', recall_m, precision_m]\n",
        "\n",
        "    # Input layer\n",
        "    model.add(layers.Conv2D(\n",
        "        filters = params[\"filter_choice\"],\n",
        "        kernel_size = params[\"kernel_size\"],\n",
        "        activation= params[\"activation\"],\n",
        "        input_shape=input_shape\n",
        "    ))\n",
        "    model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # Convolutional layers\n",
        "    for i in range(params[\"n_conv_layers\"]):\n",
        "        model.add(layers.Conv2D(\n",
        "            filters= params[\"filter_choice\"],\n",
        "            kernel_size = params[\"kernel_size\"],\n",
        "            activation= params[\"activation\"]\n",
        "        ))\n",
        "        if i < params[\"n_pool_layers\"]:\n",
        "            model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "    # drop out layers to prevent overfitting\n",
        "    model.add(layers.Dropout(params['dropout_rate']))\n",
        "\n",
        "    # Flatten layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Intermediate dense layer\n",
        "    model.add(layers.Dense(\n",
        "        params[\"n_neurons\"],\n",
        "        activation= params[\"activation\"]\n",
        "    ))\n",
        "\n",
        "    # another drop out layers to prevent overfitting\n",
        "    model.add(layers.Dropout(params['dropout_rate']))\n",
        "\n",
        "    # Output layer (78 classes)\n",
        "    model.add(layers.Dense(78, activation=\"softmax\"))\n",
        "\n",
        "    # Optimizer selection\n",
        "    optimizer_name = params[\"dense_optimizer\"]\n",
        "    if optimizer_name == 'adam': #dynamic learning rate\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate = params[\"learning_rate\"])\n",
        "    elif optimizer_name == 'sgd': #static learning rate\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate = params[\"learning_rate\"])\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.RMSprop(learning_rate = params[\"learning_rate\"])\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics = metrics)\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                      mode = 'min', #to match the fmin function from hyperopt\n",
        "                                                                    #we will search for the model with the least loss\n",
        "                                                      verbose=1,\n",
        "                                                      patience=3)\n",
        "\n",
        "    model.fit(train_images, train_name_enc,\n",
        "              validation_data=(validation_images, validation_name_enc),\n",
        "              callbacks=[early_stopping])\n",
        "\n",
        "    loss, accuracy, recall, precision = model.evaluate(validation_images, validation_name_enc)\n",
        "    print(f\"validation Accuracy: {accuracy}\")\n",
        "    print(f\"validation recall: {recall}\")\n",
        "    print(f\"validation precision: {precision}\")\n",
        "    return {\n",
        "        'loss' : loss,\n",
        "        'status' :STATUS_OK,\n",
        "        'model' : model,\n",
        "        'params' : params\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEqAcWtRGtJ1"
      },
      "source": [
        "In this project, a hyperopt gridsearch will be implemented as reference to <a href = \"https://medium.com/@icaro_vazquez/neural-network-hyperparameter-optimization-with-hyperopt-f3e0cb4346c8\">(Icaro, 2024)</a>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imGHOy3WGtJ1"
      },
      "outputs": [],
      "source": [
        "# initialize trials to keep the history of every trial\n",
        "trial =  Trials()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWhAK90eGtJ2"
      },
      "source": [
        "#### Start the grid search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CpNxzD3GtJ2"
      },
      "outputs": [],
      "source": [
        "best_params = fmin(\n",
        "    fn = create_model,\n",
        "    space = params_space,\n",
        "    algo = tpe.suggest, #bayesian optimization\n",
        "    max_evals= 200, #number of models to try\n",
        "    trials= trial\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}